{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "import gensim.downloader as api\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Importing conlleval for evaluation\n",
    "from conlleval import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    processed_data = []\n",
    "    \n",
    "    for item in data:\n",
    "        sentence = item['sentence']\n",
    "        aspect_terms = item['aspect_terms']\n",
    "        \n",
    "        # Use NLTK's tokenizer for better accuracy\n",
    "        tokens = word_tokenize(sentence)\n",
    "        \n",
    "        # Initialize all labels as 'O'\n",
    "        labels = ['O'] * len(tokens)\n",
    "        \n",
    "        # Extract aspect terms\n",
    "        terms = []\n",
    "        \n",
    "        for aspect in aspect_terms:\n",
    "            term = aspect['term']\n",
    "            terms.append(term)\n",
    "            \n",
    "            # Get the start and end positions\n",
    "            start = int(aspect['from'])\n",
    "            end = int(aspect['to'])\n",
    "            \n",
    "            # Find the tokens that correspond to this aspect term\n",
    "            term_indices = []\n",
    "            char_index = 0\n",
    "            for i, token in enumerate(tokens):\n",
    "                token_start = char_index\n",
    "                token_end = token_start + len(token)\n",
    "                \n",
    "                # Check if this token overlaps with the aspect term\n",
    "                if token_end > start and token_start < end:\n",
    "                    term_indices.append(i)\n",
    "                \n",
    "                char_index = token_end + 1  # +1 for the space\n",
    "            \n",
    "            # Apply BIO tagging\n",
    "            if term_indices:\n",
    "                labels[term_indices[0]] = 'B'  # Beginning of aspect term\n",
    "                for idx in term_indices[1:]:\n",
    "                    labels[idx] = 'I'  # Inside of aspect term\n",
    "        \n",
    "        processed_item = {\n",
    "            'sentence': sentence,\n",
    "            'tokens': tokens,\n",
    "            'labels': labels,\n",
    "            'aspect_terms': terms\n",
    "        }\n",
    "        \n",
    "        processed_data.append(processed_item)\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(processed_data, f, indent=2)\n",
    "    \n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AspectTermDataset(Dataset):\n",
    "    def __init__(self, data, embedding_model, label_to_idx):\n",
    "        self.data = data\n",
    "        self.embedding_model = embedding_model\n",
    "        self.embedding_dim = embedding_model.vector_size\n",
    "        self.label_to_idx = label_to_idx\n",
    "        self.unk_vector = np.zeros(self.embedding_dim)  # Zero vector for unknown words\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        tokens = item['tokens']\n",
    "        labels = item['labels']\n",
    "        \n",
    "        # Get word embeddings directly from the model\n",
    "        token_embeddings = []\n",
    "        for token in tokens:\n",
    "            token_lower = token.lower()\n",
    "            if token_lower in self.embedding_model:\n",
    "                token_embeddings.append(self.embedding_model[token_lower])\n",
    "            else:\n",
    "                token_embeddings.append(self.unk_vector)\n",
    "        \n",
    "        token_embeddings = np.array(token_embeddings)\n",
    "        label_indices = [self.label_to_idx[label] for label in labels]\n",
    "        \n",
    "        return {\n",
    "            'embeddings': torch.tensor(token_embeddings, dtype=torch.float),\n",
    "            'labels': torch.tensor(label_indices, dtype=torch.long),\n",
    "            'lengths': len(tokens)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate function for batching\n",
    "def collate_fn(batch):\n",
    "    # Sort the batch by length in descending order\n",
    "    batch = sorted(batch, key=lambda x: x['lengths'], reverse=True)\n",
    "    \n",
    "    # Get the length of each sequence\n",
    "    lengths = [item['lengths'] for item in batch]\n",
    "    \n",
    "    # Get the maximum length in the batch\n",
    "    max_length = max(lengths)\n",
    "    embedding_dim = batch[0]['embeddings'].shape[1]\n",
    "    \n",
    "    # Pad the sequences\n",
    "    embeddings = torch.zeros(len(batch), max_length, embedding_dim, dtype=torch.float)\n",
    "    labels = torch.zeros(len(batch), max_length, dtype=torch.long)\n",
    "    \n",
    "    for i, item in enumerate(batch):\n",
    "        embeddings[i, :item['lengths']] = item['embeddings']\n",
    "        labels[i, :item['lengths']] = item['labels']\n",
    "    \n",
    "    return {\n",
    "        'embeddings': embeddings,\n",
    "        'labels': labels,\n",
    "        'lengths': torch.tensor(lengths, dtype=torch.long)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified RNN Model (non-bidirectional)\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        # No embedding layer as we're using embeddings directly\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True, bidirectional=False)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, embeddings, lengths):\n",
    "        # Pack the sequences\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embeddings, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        packed_output, hidden = self.rnn(packed_embedded)\n",
    "        \n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        output = self.dropout(output)\n",
    "        logits = self.fc(output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Modified GRU Model (non-bidirectional)\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim):\n",
    "        super(GRUModel, self).__init__()\n",
    "        \n",
    "        # No embedding layer as we're using embeddings directly\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True, bidirectional=False)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, embeddings, lengths):\n",
    "        # Pack the sequences\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embeddings, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        packed_output, hidden = self.gru(packed_embedded)\n",
    "        \n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        output = self.dropout(output)\n",
    "        logits = self.fc(output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load GloVe embeddings\n",
    "# def load_glove_embeddings(path, word_to_idx, embedding_dim=300):\n",
    "#     embeddings = np.zeros((len(word_to_idx), embedding_dim))\n",
    "    \n",
    "#     with open(path, 'r', encoding='utf-8') as f:\n",
    "#         for line in f:\n",
    "#             values = line.split()\n",
    "#             word = values[0]\n",
    "#             if word in word_to_idx:\n",
    "#                 vector = np.asarray(values[1:], dtype='float32')\n",
    "#                 embeddings[word_to_idx[word]] = vector\n",
    "    \n",
    "#     return torch.FloatTensor(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load FastText embeddings\n",
    "# def load_fasttext_embeddings(path, word_to_idx, embedding_dim=300):\n",
    "#     model = KeyedVectors.load_word2vec_format(path, binary=False)\n",
    "#     embeddings = np.zeros((len(word_to_idx), embedding_dim))\n",
    "    \n",
    "#     for word, idx in word_to_idx.items():\n",
    "#         if word in model:\n",
    "#             embeddings[idx] = model[word]\n",
    "    \n",
    "#     return torch.FloatTensor(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            embeddings = batch['embeddings'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            lengths = batch['lengths']\n",
    "            \n",
    "            outputs = model(embeddings, lengths)\n",
    "            \n",
    "            # Reshape outputs and labels for loss calculation\n",
    "            outputs_flat = outputs.view(-1, outputs.shape[-1])\n",
    "            labels_flat = labels.view(-1)\n",
    "            \n",
    "            # Calculate loss (ignore padding)\n",
    "            mask = labels_flat != 0  # Assuming 0 is the padding index\n",
    "            loss = criterion(outputs_flat[mask], labels_flat[mask])\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Get predictions\n",
    "            _, predictions = torch.max(outputs, dim=2)\n",
    "            \n",
    "            # Collect predictions and labels (ignoring padding)\n",
    "            for i in range(len(lengths)):\n",
    "                length = lengths[i].item()\n",
    "                pred = predictions[i, :length].cpu().numpy()\n",
    "                lab = labels[i, :length].cpu().numpy()\n",
    "                \n",
    "                all_predictions.extend(pred)\n",
    "                all_labels.extend(lab)\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "    \n",
    "    return total_loss / len(data_loader), f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, train_loader, val_loader, optimizer, criterion, device, epochs, model_save_path):\n",
    "    best_f1 = 0.0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}'):\n",
    "            embeddings = batch['embeddings'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            lengths = batch['lengths']\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(embeddings, lengths)\n",
    "            \n",
    "            # Reshape outputs and labels for loss calculation\n",
    "            outputs = outputs.view(-1, outputs.shape[-1])\n",
    "            labels = labels.view(-1)\n",
    "            \n",
    "            # Calculate loss (ignore padding)\n",
    "            mask = labels != 0  # Assuming 0 is the padding index\n",
    "            loss = criterion(outputs[mask], labels[mask])\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        train_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        val_loss, val_f1 = evaluate_model(model, val_loader, criterion, device)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}')\n",
    "        \n",
    "        # Save the best model\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f'Model saved to {model_save_path}')\n",
    "    \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predictions to BIO format\n",
    "def convert_to_bio(idx_to_label, predictions, lengths):\n",
    "    bio_predictions = []\n",
    "    \n",
    "    for i, length in enumerate(lengths):\n",
    "        # Convert tensor values to Python integers before dictionary lookup\n",
    "        bio_predictions.append([idx_to_label[pred.item()] for pred in predictions[i, :length]])\n",
    "    \n",
    "    return bio_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1_conlleval(tokens, true_labels, pred_labels):\n",
    "    results = []\n",
    "    \n",
    "    for sample_tokens, sample_true, sample_pred in zip(tokens, true_labels, pred_labels):\n",
    "        for token, true, pred in zip(sample_tokens, sample_true, sample_pred):\n",
    "            results.append(f\"{token} {true} {pred}\")\n",
    "        results.append(\"\")  # Empty line between sentences\n",
    "    \n",
    "    # Remove debug print\n",
    "    eval_output = evaluate(results)\n",
    "    \n",
    "    # Extract chunk-level metrics\n",
    "    chunk_metrics = eval_output['overall']['chunks']['evals']\n",
    "    # Extract tag-level metrics\n",
    "    tag_metrics = eval_output['overall']['tags']['evals']\n",
    "    \n",
    "    return chunk_metrics, tag_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(train_losses, val_losses, title, save_path):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_data, embedding_model, label_to_idx, idx_to_label, device):\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_tokens = []\n",
    "    all_true_labels = []\n",
    "    \n",
    "    embedding_dim = embedding_model.vector_size\n",
    "    unk_vector = np.zeros(embedding_dim)  # Zero vector for unknown words\n",
    "    \n",
    "    # Process each item separately to maintain direct mapping to test_data\n",
    "    for item in test_data:\n",
    "        tokens = item['tokens']\n",
    "        labels = item['labels']\n",
    "        \n",
    "        # Get word embeddings directly\n",
    "        token_embeddings = []\n",
    "        for token in tokens:\n",
    "            token_lower = token.lower()\n",
    "            if token_lower in embedding_model:\n",
    "                token_embeddings.append(embedding_model[token_lower])\n",
    "            else:\n",
    "                token_embeddings.append(unk_vector)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        token_embeddings = np.array(token_embeddings)\n",
    "        token_tensor = torch.tensor([token_embeddings], dtype=torch.float).to(device)\n",
    "        length = torch.tensor([len(tokens)], dtype=torch.long)\n",
    "        \n",
    "        # Get predictions\n",
    "        outputs = model(token_tensor, length)\n",
    "        _, predictions = torch.max(outputs, dim=2)\n",
    "        \n",
    "        # Convert predictions to BIO format\n",
    "        label_indices = [label_to_idx[label] for label in labels]\n",
    "        bio_predictions = [idx_to_label[pred.item()] for pred in predictions[0, :len(tokens)]]\n",
    "        bio_true = labels  # Already in BIO format\n",
    "        \n",
    "        all_predictions.append(bio_predictions)\n",
    "        all_tokens.append(tokens)\n",
    "        all_true_labels.append(bio_true)\n",
    "    \n",
    "    # Calculate both chunk-level and tag-level metrics using conlleval\n",
    "    chunk_metrics, tag_metrics = calculate_f1_conlleval(all_tokens, all_true_labels, all_predictions)\n",
    "    \n",
    "    print(\"Chunk-level evaluation:\")\n",
    "    print(f\"  Precision: {chunk_metrics['prec']:.4f}\")\n",
    "    print(f\"  Recall:    {chunk_metrics['rec']:.4f}\")\n",
    "    print(f\"  F1 Score:  {chunk_metrics['f1']:.4f}\")\n",
    "    \n",
    "    print(\"\\nTag-level evaluation:\")\n",
    "    print(f\"  Precision: {tag_metrics['prec']:.4f}\")\n",
    "    print(f\"  Recall:    {tag_metrics['rec']:.4f}\")\n",
    "    print(f\"  F1 Score:  {tag_metrics['f1']:.4f}\")\n",
    "    \n",
    "    return chunk_metrics, tag_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Set device\n",
    "    device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Using device: {device}')\n",
    "    \n",
    "    # Preprocess data\n",
    "    print('Preprocessing data...')\n",
    "    train_data = preprocess_data('train.json', 'train_task_1.json')\n",
    "    val_data = preprocess_data('val.json', 'val_task_1.json')\n",
    "    \n",
    "    # Define label mapping\n",
    "    label_to_idx = {'<PAD>': 0, 'O': 1, 'B': 2, 'I': 3}\n",
    "    idx_to_label = {0: '<PAD>', 1: 'O', 2: 'B', 3: 'I'}\n",
    "    \n",
    "    # Save label mapping\n",
    "    with open(\"label_mapping.json\", \"w\") as f:\n",
    "        json.dump({\n",
    "            \"label_to_idx\": label_to_idx,\n",
    "            \"idx_to_label\": idx_to_label\n",
    "        }, f)\n",
    "    \n",
    "    # Load pretrained embeddings    \n",
    "    try:\n",
    "        print('Loading pretrained embeddings...')\n",
    "        \n",
    "        # Load models\n",
    "        print('Loading models...')\n",
    "        glove_model = api.load(\"glove-wiki-gigaword-300\")\n",
    "        print('GloVe model loaded')\n",
    "        fasttext_model = api.load(\"fasttext-wiki-news-subwords-300\")\n",
    "        print('FastText model loaded')\n",
    "        \n",
    "        embedding_dim = 300\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading from gensim: {e}\")\n",
    "        print(\"Exiting as we need pre-trained embeddings.\")\n",
    "        return\n",
    "    \n",
    "    # Get the embedding dimension\n",
    "    embedding_dim = glove_model.vector_size\n",
    "    hidden_dim = 512\n",
    "    output_dim = len(label_to_idx)\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    print('Creating datasets and dataloaders...')\n",
    "    train_dataset_glove = AspectTermDataset(train_data, glove_model, label_to_idx)\n",
    "    val_dataset_glove = AspectTermDataset(val_data, glove_model, label_to_idx)\n",
    "    train_dataset_fasttext = AspectTermDataset(train_data, fasttext_model, label_to_idx)\n",
    "    val_dataset_fasttext = AspectTermDataset(val_data, fasttext_model, label_to_idx)\n",
    "    \n",
    "    batch_size = 16\n",
    "    train_loader_glove = DataLoader(train_dataset_glove, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader_glove = DataLoader(val_dataset_glove, batch_size=batch_size, collate_fn=collate_fn)\n",
    "    train_loader_fasttext = DataLoader(train_dataset_fasttext, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader_fasttext = DataLoader(val_dataset_fasttext, batch_size=batch_size, collate_fn=collate_fn)\n",
    "    \n",
    "    # Create the directory for saving models if it doesn't exist\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    os.makedirs('plots', exist_ok=True)\n",
    "    \n",
    "    # Define models\n",
    "    models = {\n",
    "        'RNN_GloVe': {\n",
    "            'model': RNNModel(embedding_dim, hidden_dim, output_dim).to(device),\n",
    "            'train_loader': train_loader_glove,\n",
    "            'val_loader': val_loader_glove,\n",
    "            'embedding_model': glove_model\n",
    "        },\n",
    "        'RNN_FastText': {\n",
    "            'model': RNNModel(embedding_dim, hidden_dim, output_dim).to(device),\n",
    "            'train_loader': train_loader_fasttext,\n",
    "            'val_loader': val_loader_fasttext,\n",
    "            'embedding_model': fasttext_model\n",
    "        },\n",
    "        'GRU_GloVe': {\n",
    "            'model': GRUModel(embedding_dim, hidden_dim, output_dim).to(device),\n",
    "            'train_loader': train_loader_glove,\n",
    "            'val_loader': val_loader_glove,\n",
    "            'embedding_model': glove_model\n",
    "        },\n",
    "        'GRU_FastText': {\n",
    "            'model': GRUModel(embedding_dim, hidden_dim, output_dim).to(device),\n",
    "            'train_loader': train_loader_fasttext,\n",
    "            'val_loader': val_loader_fasttext,\n",
    "            'embedding_model': fasttext_model\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding index\n",
    "    epochs = 15\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Train and evaluate each model\n",
    "    for name, model_config in models.items():\n",
    "        print(f'\\nTraining {name}...')\n",
    "        model = model_config['model']\n",
    "        train_loader = model_config['train_loader']\n",
    "        val_loader = model_config['val_loader']\n",
    "        \n",
    "        optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "        model_save_path = f'models/{name}_best.pt'\n",
    "        \n",
    "        train_losses, val_losses = train(model, train_loader, val_loader, optimizer, criterion, device, epochs, model_save_path)\n",
    "        \n",
    "        # Plot losses\n",
    "        plot_losses(train_losses, val_losses, f'{name} Training and Validation Loss', f'plots/{name}_loss.png')\n",
    "        \n",
    "        # Load the best model\n",
    "        model.load_state_dict(torch.load(model_save_path))\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        chunk_metrics, tag_metrics = test_model(model, val_data, model_config['embedding_model'], label_to_idx, idx_to_label, device)\n",
    "        \n",
    "        results[name] = {\n",
    "            'chunk_f1': chunk_metrics['f1'],\n",
    "            'chunk_prec': chunk_metrics['prec'],\n",
    "            'chunk_rec': chunk_metrics['rec'],\n",
    "            'tag_f1': tag_metrics['f1'],\n",
    "            'tag_prec': tag_metrics['prec'],\n",
    "            'tag_rec': tag_metrics['rec']\n",
    "        }\n",
    "        \n",
    "        print(f'{name} validation results:')\n",
    "        print(f\"  Chunk-level -> Precision: {chunk_metrics['prec']:.4f}, Recall: {chunk_metrics['rec']:.4f}, F1: {chunk_metrics['f1']:.4f}\")\n",
    "        print(f\"  Tag-level   -> Precision: {tag_metrics['prec']:.4f}, Recall: {tag_metrics['rec']:.4f}, F1: {tag_metrics['f1']:.4f}\")\n",
    "    \n",
    "    # Print results summary\n",
    "    print('\\nResults Summary:')\n",
    "    for name, result in results.items():\n",
    "        print(f\"{name}: Chunk F1 = {result['chunk_f1']:.4f}, Tag F1 = {result['tag_f1']:.4f}\")\n",
    "    \n",
    "    # Find the best model based on chunk-level F1\n",
    "    best_model_name = max(results, key=lambda x: results[x]['chunk_f1'])\n",
    "    print(f'\\nBest model: {best_model_name} with Chunk F1 = {results[best_model_name][\"chunk_f1\"]:.4f} and Tag F1 = {results[best_model_name][\"tag_f1\"]:.4f}')\n",
    "    \n",
    "    # Save best model info\n",
    "    with open('best_model_info.json', 'w') as f:\n",
    "        json.dump({\n",
    "            'model_name': best_model_name,\n",
    "            'chunk_f1': results[best_model_name]['chunk_f1'],\n",
    "            'tag_f1': results[best_model_name]['tag_f1']\n",
    "        }, f)\n",
    "    \n",
    "    # Save embedding model information\n",
    "    with open('embedding_model_info.json', 'w') as f:\n",
    "        json.dump({\n",
    "            'best_model': best_model_name,\n",
    "            'glove_model_name': \"glove-wiki-gigaword-300\",\n",
    "            'fasttext_model_name': \"fasttext-wiki-news-subwords-300\"\n",
    "        }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_test(test_file, model_path, model_type, embedding_model_name):\n",
    "    # Load label mapping\n",
    "    with open(\"label_mapping.json\", \"r\") as f:\n",
    "        label_data = json.load(f)\n",
    "    label_to_idx = label_data[\"label_to_idx\"]\n",
    "    # Convert idx_to_label keys to integers\n",
    "    idx_to_label = {int(k): v for k, v in label_data[\"idx_to_label\"].items()}\n",
    "    \n",
    "    # Process the test file\n",
    "    test_data = preprocess_data(test_file, \"test_task_1.json\")\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Load embedding model\n",
    "    try:\n",
    "        print(f'Loading {embedding_model_name} embeddings...')\n",
    "        embedding_model = api.load(embedding_model_name)\n",
    "        print(f'{embedding_model_name} loaded')\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading embedding model: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Get the embedding dimension\n",
    "    embedding_dim = embedding_dim = embedding_model.vector_size\n",
    "    hidden_dim = 512\n",
    "    output_dim = len(label_to_idx)\n",
    "    \n",
    "    # Initialize model\n",
    "    if model_type.startswith('RNN'):\n",
    "        model = RNNModel(embedding_dim, hidden_dim, output_dim).to(device)\n",
    "    elif model_type.startswith('GRU'):\n",
    "        model = GRUModel(embedding_dim, hidden_dim, output_dim).to(device)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "    \n",
    "    # Load model weights\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    \n",
    "    # Test the model\n",
    "    chunk_metrics, tag_metrics = test_model(model, test_data, embedding_model, label_to_idx, idx_to_label, device)\n",
    "    \n",
    "    print(\"Test Results:\")\n",
    "    print(\"Chunk-level:\")\n",
    "    print(f\"  Precision: {chunk_metrics['prec']:.4f}\")\n",
    "    print(f\"  Recall:    {chunk_metrics['rec']:.4f}\")\n",
    "    print(f\"  F1 Score:  {chunk_metrics['f1']:.4f}\")\n",
    "    print(\"Tag-level:\")\n",
    "    print(f\"  Precision: {tag_metrics['prec']:.4f}\")\n",
    "    print(f\"  Recall:    {tag_metrics['rec']:.4f}\")\n",
    "    print(f\"  F1 Score:  {tag_metrics['f1']:.4f}\")\n",
    "    \n",
    "    return chunk_metrics, tag_metrics\n",
    "\n",
    "def test_with_best_model(test_file):\n",
    "    # Load best model info\n",
    "    with open('best_model_info.json', 'r') as f:\n",
    "        best_model_info = json.load(f)\n",
    "    \n",
    "    # Load embedding model info\n",
    "    with open('embedding_model_info.json', 'r') as f:\n",
    "        embedding_info = json.load(f)\n",
    "    \n",
    "    best_model_name = best_model_info['model_name']\n",
    "    model_path = f'models/{best_model_name}_best.pt'\n",
    "    \n",
    "    # Determine which embedding model to use\n",
    "    if 'GloVe' in best_model_name:\n",
    "        embedding_model_name = embedding_info['glove_model_name']\n",
    "    else:\n",
    "        embedding_model_name = embedding_info['fasttext_model_name']\n",
    "    \n",
    "    # Extract model type (RNN or GRU)\n",
    "    model_type = best_model_name.split('_')[0]\n",
    "    \n",
    "    print(f\"Testing with best model: {best_model_name}\")\n",
    "    print(f\"Using embedding model: {embedding_model_name}\")\n",
    "    print(f\"Model type: {model_type}\")\n",
    "    \n",
    "    # Test the model\n",
    "    chunk_metrics, tag_metrics = load_and_test(test_file, model_path, model_type, embedding_model_name)\n",
    "    \n",
    "    return chunk_metrics, tag_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n",
      "Preprocessing data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained embeddings...\n",
      "Loading models...\n",
      "GloVe model loaded\n",
      "FastText model loaded\n",
      "Creating datasets and dataloaders...\n",
      "\n",
      "Training RNN_GloVe...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 153/153 [00:00<00:00, 211.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Train Loss: 0.2821, Val Loss: 0.2307, Val F1: 0.9222\n",
      "Model saved to models/RNN_GloVe_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15: 100%|██████████| 153/153 [00:00<00:00, 217.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15, Train Loss: 0.2303, Val Loss: 0.1999, Val F1: 0.9276\n",
      "Model saved to models/RNN_GloVe_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15: 100%|██████████| 153/153 [00:00<00:00, 214.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15, Train Loss: 0.2152, Val Loss: 0.2073, Val F1: 0.9175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15: 100%|██████████| 153/153 [00:00<00:00, 212.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15, Train Loss: 0.2117, Val Loss: 0.1981, Val F1: 0.9285\n",
      "Model saved to models/RNN_GloVe_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15: 100%|██████████| 153/153 [00:00<00:00, 209.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15, Train Loss: 0.2096, Val Loss: 0.2058, Val F1: 0.9267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15: 100%|██████████| 153/153 [00:00<00:00, 216.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15, Train Loss: 0.1994, Val Loss: 0.2021, Val F1: 0.9294\n",
      "Model saved to models/RNN_GloVe_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15: 100%|██████████| 153/153 [00:00<00:00, 223.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15, Train Loss: 0.1931, Val Loss: 0.1934, Val F1: 0.9284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15: 100%|██████████| 153/153 [00:00<00:00, 218.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15, Train Loss: 0.1899, Val Loss: 0.1919, Val F1: 0.9273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15: 100%|██████████| 153/153 [00:00<00:00, 219.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15, Train Loss: 0.1754, Val Loss: 0.1958, Val F1: 0.9324\n",
      "Model saved to models/RNN_GloVe_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15: 100%|██████████| 153/153 [00:00<00:00, 218.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15, Train Loss: 0.1656, Val Loss: 0.1999, Val F1: 0.9279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15: 100%|██████████| 153/153 [00:00<00:00, 219.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15, Train Loss: 0.1560, Val Loss: 0.1891, Val F1: 0.9281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15: 100%|██████████| 153/153 [00:00<00:00, 219.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15, Train Loss: 0.1501, Val Loss: 0.1946, Val F1: 0.9314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15: 100%|██████████| 153/153 [00:00<00:00, 219.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/15, Train Loss: 0.1795, Val Loss: 0.2086, Val F1: 0.9273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15: 100%|██████████| 153/153 [00:00<00:00, 219.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/15, Train Loss: 0.1395, Val Loss: 0.1817, Val F1: 0.9319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15: 100%|██████████| 153/153 [00:00<00:00, 216.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15, Train Loss: 0.1255, Val Loss: 0.2109, Val F1: 0.9286\n",
      "Chunk-level evaluation:\n",
      "  Precision: 0.6124\n",
      "  Recall:    0.5892\n",
      "  F1 Score:  0.6006\n",
      "\n",
      "Tag-level evaluation:\n",
      "  Precision: 0.9378\n",
      "  Recall:    0.9378\n",
      "  F1 Score:  0.9378\n",
      "RNN_GloVe validation results:\n",
      "  Chunk-level -> Precision: 0.6124, Recall: 0.5892, F1: 0.6006\n",
      "  Tag-level   -> Precision: 0.9378, Recall: 0.9378, F1: 0.9378\n",
      "\n",
      "Training RNN_FastText...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 153/153 [00:00<00:00, 218.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Train Loss: 0.3327, Val Loss: 0.2379, Val F1: 0.9182\n",
      "Model saved to models/RNN_FastText_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15: 100%|██████████| 153/153 [00:00<00:00, 218.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15, Train Loss: 0.2348, Val Loss: 0.2027, Val F1: 0.9273\n",
      "Model saved to models/RNN_FastText_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15: 100%|██████████| 153/153 [00:00<00:00, 215.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15, Train Loss: 0.2183, Val Loss: 0.2233, Val F1: 0.9240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15: 100%|██████████| 153/153 [00:00<00:00, 217.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15, Train Loss: 0.2157, Val Loss: 0.2115, Val F1: 0.9269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15: 100%|██████████| 153/153 [00:00<00:00, 217.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15, Train Loss: 0.2045, Val Loss: 0.1857, Val F1: 0.9315\n",
      "Model saved to models/RNN_FastText_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15: 100%|██████████| 153/153 [00:00<00:00, 217.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15, Train Loss: 0.2021, Val Loss: 0.1907, Val F1: 0.9300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15: 100%|██████████| 153/153 [00:00<00:00, 217.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15, Train Loss: 0.1999, Val Loss: 0.1958, Val F1: 0.9333\n",
      "Model saved to models/RNN_FastText_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15: 100%|██████████| 153/153 [00:00<00:00, 226.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15, Train Loss: 0.1991, Val Loss: 0.1921, Val F1: 0.9311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15: 100%|██████████| 153/153 [00:00<00:00, 235.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15, Train Loss: 0.1960, Val Loss: 0.1833, Val F1: 0.9316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15: 100%|██████████| 153/153 [00:00<00:00, 231.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15, Train Loss: 0.1900, Val Loss: 0.1826, Val F1: 0.9340\n",
      "Model saved to models/RNN_FastText_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15: 100%|██████████| 153/153 [00:00<00:00, 232.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15, Train Loss: 0.2029, Val Loss: 0.2152, Val F1: 0.9239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15: 100%|██████████| 153/153 [00:00<00:00, 232.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15, Train Loss: 0.3646, Val Loss: 0.2332, Val F1: 0.9201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15: 100%|██████████| 153/153 [00:00<00:00, 233.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/15, Train Loss: 0.2707, Val Loss: 0.2271, Val F1: 0.9134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15: 100%|██████████| 153/153 [00:00<00:00, 235.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/15, Train Loss: 0.2435, Val Loss: 0.1934, Val F1: 0.9314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15: 100%|██████████| 153/153 [00:00<00:00, 232.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15, Train Loss: 0.2395, Val Loss: 0.1949, Val F1: 0.9268\n",
      "Chunk-level evaluation:\n",
      "  Precision: 0.6229\n",
      "  Recall:    0.5892\n",
      "  F1 Score:  0.6056\n",
      "\n",
      "Tag-level evaluation:\n",
      "  Precision: 0.9384\n",
      "  Recall:    0.9384\n",
      "  F1 Score:  0.9384\n",
      "RNN_FastText validation results:\n",
      "  Chunk-level -> Precision: 0.6229, Recall: 0.5892, F1: 0.6056\n",
      "  Tag-level   -> Precision: 0.9384, Recall: 0.9384, F1: 0.9384\n",
      "\n",
      "Training GRU_GloVe...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 153/153 [00:00<00:00, 231.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Train Loss: 0.2771, Val Loss: 0.1954, Val F1: 0.9290\n",
      "Model saved to models/GRU_GloVe_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15: 100%|██████████| 153/153 [00:00<00:00, 223.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15, Train Loss: 0.2079, Val Loss: 0.1749, Val F1: 0.9322\n",
      "Model saved to models/GRU_GloVe_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15: 100%|██████████| 153/153 [00:00<00:00, 227.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15, Train Loss: 0.1847, Val Loss: 0.1723, Val F1: 0.9363\n",
      "Model saved to models/GRU_GloVe_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15: 100%|██████████| 153/153 [00:00<00:00, 228.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15, Train Loss: 0.1627, Val Loss: 0.1630, Val F1: 0.9408\n",
      "Model saved to models/GRU_GloVe_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15: 100%|██████████| 153/153 [00:00<00:00, 230.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15, Train Loss: 0.1447, Val Loss: 0.1590, Val F1: 0.9411\n",
      "Model saved to models/GRU_GloVe_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15: 100%|██████████| 153/153 [00:00<00:00, 227.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15, Train Loss: 0.1227, Val Loss: 0.1655, Val F1: 0.9366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15: 100%|██████████| 153/153 [00:00<00:00, 229.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15, Train Loss: 0.1064, Val Loss: 0.1753, Val F1: 0.9399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15: 100%|██████████| 153/153 [00:00<00:00, 229.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15, Train Loss: 0.0874, Val Loss: 0.1753, Val F1: 0.9384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15: 100%|██████████| 153/153 [00:00<00:00, 227.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15, Train Loss: 0.0679, Val Loss: 0.2101, Val F1: 0.9375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15: 100%|██████████| 153/153 [00:00<00:00, 223.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15, Train Loss: 0.0544, Val Loss: 0.2067, Val F1: 0.9353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15: 100%|██████████| 153/153 [00:00<00:00, 224.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15, Train Loss: 0.0412, Val Loss: 0.2322, Val F1: 0.9378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15: 100%|██████████| 153/153 [00:00<00:00, 226.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15, Train Loss: 0.0316, Val Loss: 0.2416, Val F1: 0.9359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15: 100%|██████████| 153/153 [00:00<00:00, 231.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/15, Train Loss: 0.0254, Val Loss: 0.2695, Val F1: 0.9265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15: 100%|██████████| 153/153 [00:00<00:00, 229.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/15, Train Loss: 0.0274, Val Loss: 0.2840, Val F1: 0.9326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15: 100%|██████████| 153/153 [00:00<00:00, 231.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15, Train Loss: 0.0162, Val Loss: 0.3037, Val F1: 0.9339\n",
      "Chunk-level evaluation:\n",
      "  Precision: 0.6488\n",
      "  Recall:    0.5892\n",
      "  F1 Score:  0.6176\n",
      "\n",
      "Tag-level evaluation:\n",
      "  Precision: 0.9452\n",
      "  Recall:    0.9452\n",
      "  F1 Score:  0.9452\n",
      "GRU_GloVe validation results:\n",
      "  Chunk-level -> Precision: 0.6488, Recall: 0.5892, F1: 0.6176\n",
      "  Tag-level   -> Precision: 0.9452, Recall: 0.9452, F1: 0.9452\n",
      "\n",
      "Training GRU_FastText...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 153/153 [00:00<00:00, 209.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Train Loss: 0.3516, Val Loss: 0.2099, Val F1: 0.9248\n",
      "Model saved to models/GRU_FastText_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15: 100%|██████████| 153/153 [00:00<00:00, 207.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15, Train Loss: 0.2185, Val Loss: 0.1866, Val F1: 0.9316\n",
      "Model saved to models/GRU_FastText_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15: 100%|██████████| 153/153 [00:00<00:00, 224.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15, Train Loss: 0.2036, Val Loss: 0.1756, Val F1: 0.9323\n",
      "Model saved to models/GRU_FastText_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15: 100%|██████████| 153/153 [00:00<00:00, 224.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15, Train Loss: 0.1883, Val Loss: 0.1642, Val F1: 0.9401\n",
      "Model saved to models/GRU_FastText_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15: 100%|██████████| 153/153 [00:00<00:00, 227.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15, Train Loss: 0.1765, Val Loss: 0.1670, Val F1: 0.9390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15: 100%|██████████| 153/153 [00:00<00:00, 221.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15, Train Loss: 0.1648, Val Loss: 0.1694, Val F1: 0.9377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15: 100%|██████████| 153/153 [00:00<00:00, 214.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15, Train Loss: 0.1584, Val Loss: 0.1569, Val F1: 0.9418\n",
      "Model saved to models/GRU_FastText_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15: 100%|██████████| 153/153 [00:00<00:00, 216.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15, Train Loss: 0.1489, Val Loss: 0.1615, Val F1: 0.9408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15: 100%|██████████| 153/153 [00:00<00:00, 221.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15, Train Loss: 0.1416, Val Loss: 0.1602, Val F1: 0.9399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15: 100%|██████████| 153/153 [00:00<00:00, 216.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15, Train Loss: 0.1337, Val Loss: 0.1691, Val F1: 0.9418\n",
      "Model saved to models/GRU_FastText_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15: 100%|██████████| 153/153 [00:00<00:00, 217.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15, Train Loss: 0.1265, Val Loss: 0.1586, Val F1: 0.9417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15: 100%|██████████| 153/153 [00:00<00:00, 226.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15, Train Loss: 0.1179, Val Loss: 0.1639, Val F1: 0.9437\n",
      "Model saved to models/GRU_FastText_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15: 100%|██████████| 153/153 [00:00<00:00, 217.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/15, Train Loss: 0.1074, Val Loss: 0.1665, Val F1: 0.9405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15: 100%|██████████| 153/153 [00:00<00:00, 211.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/15, Train Loss: 0.0986, Val Loss: 0.1713, Val F1: 0.9403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15: 100%|██████████| 153/153 [00:00<00:00, 186.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15, Train Loss: 0.0906, Val Loss: 0.1775, Val F1: 0.9424\n",
      "Chunk-level evaluation:\n",
      "  Precision: 0.6410\n",
      "  Recall:    0.6514\n",
      "  F1 Score:  0.6461\n",
      "\n",
      "Tag-level evaluation:\n",
      "  Precision: 0.9458\n",
      "  Recall:    0.9458\n",
      "  F1 Score:  0.9458\n",
      "GRU_FastText validation results:\n",
      "  Chunk-level -> Precision: 0.6410, Recall: 0.6514, F1: 0.6461\n",
      "  Tag-level   -> Precision: 0.9458, Recall: 0.9458, F1: 0.9458\n",
      "\n",
      "Results Summary:\n",
      "RNN_GloVe: Chunk F1 = 0.6006, Tag F1 = 0.9378\n",
      "RNN_FastText: Chunk F1 = 0.6056, Tag F1 = 0.9384\n",
      "GRU_GloVe: Chunk F1 = 0.6176, Tag F1 = 0.9452\n",
      "GRU_FastText: Chunk F1 = 0.6461, Tag F1 = 0.9458\n",
      "\n",
      "Best model: GRU_FastText with Chunk F1 = 0.6461 and Tag F1 = 0.9458\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Create directory for plots\n",
    "    os.makedirs('plots', exist_ok=True)\n",
    "    \n",
    "    # Run main function\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with best model: GRU_FastText\n",
      "Using embedding model: fasttext-wiki-news-subwords-300\n",
      "Model type: GRU\n",
      "Loading fasttext-wiki-news-subwords-300 embeddings...\n",
      "fasttext-wiki-news-subwords-300 loaded\n",
      "Chunk-level evaluation:\n",
      "  Precision: 0.6410\n",
      "  Recall:    0.6514\n",
      "  F1 Score:  0.6461\n",
      "\n",
      "Tag-level evaluation:\n",
      "  Precision: 0.9458\n",
      "  Recall:    0.9458\n",
      "  F1 Score:  0.9458\n",
      "Test Results:\n",
      "Chunk-level:\n",
      "  Precision: 0.6410\n",
      "  Recall:    0.6514\n",
      "  F1 Score:  0.6461\n",
      "Tag-level:\n",
      "  Precision: 0.9458\n",
      "  Recall:    0.9458\n",
      "  F1 Score:  0.9458\n"
     ]
    }
   ],
   "source": [
    "# test function:\n",
    "chunk_metrics, tag_metrics = test_with_best_model('val.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
