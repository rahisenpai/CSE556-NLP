{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from gensim.models import KeyedVectors\n",
    "import re\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Importing conlleval for evaluation\n",
    "from conlleval import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    processed_data = []\n",
    "    \n",
    "    for item in data:\n",
    "        sentence = item['sentence']\n",
    "        aspect_terms = item['aspect_terms']\n",
    "        \n",
    "        # Tokenize the sentence\n",
    "        tokens = sentence.split()\n",
    "        \n",
    "        # Initialize all labels as 'O'\n",
    "        labels = ['O'] * len(tokens)\n",
    "        \n",
    "        # Extract aspect terms\n",
    "        terms = []\n",
    "        \n",
    "        for aspect in aspect_terms:\n",
    "            term = aspect['term']\n",
    "            terms.append(term)\n",
    "            \n",
    "            # Get the start and end positions\n",
    "            start = int(aspect['from'])\n",
    "            end = int(aspect['to'])\n",
    "            \n",
    "            # Find the tokens that correspond to this aspect term\n",
    "            term_tokens = []\n",
    "            term_indices = []\n",
    "            \n",
    "            char_index = 0\n",
    "            for i, token in enumerate(tokens):\n",
    "                token_start = char_index\n",
    "                token_end = token_start + len(token)\n",
    "                \n",
    "                # Check if this token overlaps with the aspect term\n",
    "                if token_end > start and token_start < end:\n",
    "                    term_tokens.append(token)\n",
    "                    term_indices.append(i)\n",
    "                \n",
    "                char_index = token_end + 1  # +1 for the space\n",
    "            \n",
    "            # Apply BIO tagging\n",
    "            if term_indices:\n",
    "                labels[term_indices[0]] = 'B'  # Beginning of aspect term\n",
    "                for idx in term_indices[1:]:\n",
    "                    labels[idx] = 'I'  # Inside of aspect term\n",
    "        \n",
    "        processed_item = {\n",
    "            'sentence': sentence,\n",
    "            'tokens': tokens,\n",
    "            'labels': labels,\n",
    "            'aspect_terms': terms\n",
    "        }\n",
    "        \n",
    "        processed_data.append(processed_item)\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(processed_data, f, indent=2)\n",
    "    \n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class AspectTermDataset(Dataset):\n",
    "    def __init__(self, data, word_to_idx, label_to_idx):\n",
    "        self.data = data\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.label_to_idx = label_to_idx\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        tokens = item['tokens']\n",
    "        labels = item['labels']\n",
    "        \n",
    "        # Convert tokens to indices\n",
    "        token_indices = [self.word_to_idx.get(token.lower(), self.word_to_idx['<UNK>']) for token in tokens]\n",
    "        \n",
    "        # Convert labels to indices\n",
    "        label_indices = [self.label_to_idx[label] for label in labels]\n",
    "        \n",
    "        return {\n",
    "            'tokens': torch.tensor(token_indices, dtype=torch.long),\n",
    "            'labels': torch.tensor(label_indices, dtype=torch.long),\n",
    "            'lengths': len(tokens)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate function for batching\n",
    "def collate_fn(batch):\n",
    "    # Sort the batch by length in descending order\n",
    "    batch = sorted(batch, key=lambda x: x['lengths'], reverse=True)\n",
    "    \n",
    "    # Get the length of each sequence\n",
    "    lengths = [item['lengths'] for item in batch]\n",
    "    \n",
    "    # Get the maximum length in the batch\n",
    "    max_length = max(lengths)\n",
    "    \n",
    "    # Pad the sequences\n",
    "    tokens = torch.zeros(len(batch), max_length, dtype=torch.long)\n",
    "    labels = torch.zeros(len(batch), max_length, dtype=torch.long)\n",
    "    \n",
    "    for i, item in enumerate(batch):\n",
    "        tokens[i, :item['lengths']] = item['tokens']\n",
    "        labels[i, :item['lengths']] = item['labels']\n",
    "    \n",
    "    return {\n",
    "        'tokens': tokens,\n",
    "        'labels': labels,\n",
    "        'lengths': torch.tensor(lengths, dtype=torch.long)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classes\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, pretrained_embeddings=None):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "    def forward(self, tokens, lengths):\n",
    "        embedded = self.embedding(tokens)\n",
    "        \n",
    "        # Pack the sequences\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, lengths.cpu(), batch_first=True)\n",
    "        \n",
    "        packed_output, hidden = self.rnn(packed_embedded)\n",
    "        \n",
    "        # Unpack the sequences\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        # Pass through the fully connected layer\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, pretrained_embeddings=None):\n",
    "        super(GRUModel, self).__init__()\n",
    "        \n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "    def forward(self, tokens, lengths):\n",
    "        embedded = self.embedding(tokens)\n",
    "        \n",
    "        # Pack the sequences\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, lengths.cpu(), batch_first=True)\n",
    "        \n",
    "        packed_output, hidden = self.gru(packed_embedded)\n",
    "        \n",
    "        # Unpack the sequences\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        # Pass through the fully connected layer\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe embeddings\n",
    "def load_glove_embeddings(path, word_to_idx, embedding_dim=300):\n",
    "    embeddings = np.zeros((len(word_to_idx), embedding_dim))\n",
    "    \n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            if word in word_to_idx:\n",
    "                vector = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings[word_to_idx[word]] = vector\n",
    "    \n",
    "    return torch.FloatTensor(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FastText embeddings\n",
    "def load_fasttext_embeddings(path, word_to_idx, embedding_dim=300):\n",
    "    model = KeyedVectors.load_word2vec_format(path, binary=False)\n",
    "    embeddings = np.zeros((len(word_to_idx), embedding_dim))\n",
    "    \n",
    "    for word, idx in word_to_idx.items():\n",
    "        if word in model:\n",
    "            embeddings[idx] = model[word]\n",
    "    \n",
    "    return torch.FloatTensor(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            tokens = batch['tokens'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            lengths = batch['lengths']\n",
    "            \n",
    "            outputs = model(tokens, lengths)\n",
    "            \n",
    "            # Reshape outputs and labels for loss calculation\n",
    "            outputs_flat = outputs.view(-1, outputs.shape[-1])\n",
    "            labels_flat = labels.view(-1)\n",
    "            \n",
    "            # Calculate loss (ignore padding)\n",
    "            mask = labels_flat != 0  # Assuming 0 is the padding index\n",
    "            loss = criterion(outputs_flat[mask], labels_flat[mask])\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Get predictions\n",
    "            _, predictions = torch.max(outputs, dim=2)\n",
    "            \n",
    "            # Collect predictions and labels (ignoring padding)\n",
    "            for i in range(len(lengths)):\n",
    "                length = lengths[i].item()\n",
    "                pred = predictions[i, :length].cpu().numpy()\n",
    "                lab = labels[i, :length].cpu().numpy()\n",
    "                \n",
    "                all_predictions.extend(pred)\n",
    "                all_labels.extend(lab)\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "    \n",
    "    return total_loss / len(data_loader), f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, train_loader, val_loader, optimizer, criterion, device, epochs, model_save_path):\n",
    "    best_f1 = 0.0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}'):\n",
    "            tokens = batch['tokens'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            lengths = batch['lengths']\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(tokens, lengths)\n",
    "            \n",
    "            # Reshape outputs and labels for loss calculation\n",
    "            outputs = outputs.view(-1, outputs.shape[-1])\n",
    "            labels = labels.view(-1)\n",
    "            \n",
    "            # Calculate loss (ignore padding)\n",
    "            mask = labels != 0  # Assuming 0 is the padding index\n",
    "            loss = criterion(outputs[mask], labels[mask])\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        train_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        val_loss, val_f1 = evaluate_model(model, val_loader, criterion, device)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}')\n",
    "        \n",
    "        # Save the best model\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f'Model saved to {model_save_path}')\n",
    "    \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predictions to BIO format\n",
    "def convert_to_bio(idx_to_label, predictions, lengths):\n",
    "    bio_predictions = []\n",
    "    \n",
    "    for i, length in enumerate(lengths):\n",
    "        bio_predictions.append([idx_to_label[pred] for pred in predictions[i, :length]])\n",
    "    \n",
    "    return bio_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1 score using conlleval\n",
    "def calculate_f1_conlleval(tokens, true_labels, pred_labels):\n",
    "    results = []\n",
    "    \n",
    "    for sample_tokens, sample_true, sample_pred in zip(tokens, true_labels, pred_labels):\n",
    "        for token, true, pred in zip(sample_tokens, sample_true, sample_pred):\n",
    "            results.append(f\"{token} {true} {pred}\")\n",
    "        results.append(\"\")  # Empty line between sentences\n",
    "    \n",
    "    prec, rec, f1 = evaluate(results)\n",
    "    return prec, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot training and validation losses\n",
    "def plot_losses(train_losses, val_losses, title, save_path):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing function\n",
    "def test_model(model, test_data, word_to_idx, label_to_idx, idx_to_label, device):\n",
    "    model.eval()\n",
    "    \n",
    "    test_dataset = AspectTermDataset(test_data, word_to_idx, label_to_idx)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, collate_fn=collate_fn)\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_tokens = []\n",
    "    all_true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            tokens = batch['tokens'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            lengths = batch['lengths']\n",
    "            \n",
    "            outputs = model(tokens, lengths)\n",
    "            \n",
    "            # Get predictions\n",
    "            _, predictions = torch.max(outputs, dim=2)\n",
    "            \n",
    "            # Convert predictions to BIO format\n",
    "            bio_predictions = convert_to_bio(idx_to_label, predictions.cpu(), lengths)\n",
    "            \n",
    "            # Get true labels\n",
    "            bio_true = convert_to_bio(idx_to_label, labels.cpu(), lengths)\n",
    "            \n",
    "            # Get tokens\n",
    "            batch_tokens = []\n",
    "            for i, length in enumerate(lengths):\n",
    "                batch_tokens.append([test_data[i]['tokens'][j] for j in range(length)])\n",
    "            \n",
    "            all_predictions.extend(bio_predictions)\n",
    "            all_tokens.extend(batch_tokens)\n",
    "            all_true_labels.extend(bio_true)\n",
    "    \n",
    "    # Calculate F1 score using conlleval\n",
    "    prec, rec, f1 = calculate_f1_conlleval(all_tokens, all_true_labels, all_predictions)\n",
    "    \n",
    "    return prec, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Using device: {device}')\n",
    "    \n",
    "    # Preprocess data\n",
    "    print('Preprocessing data...')\n",
    "    train_data = preprocess_data('train.json', 'train_task_1.json')\n",
    "    val_data = preprocess_data('val.json', 'val_task_1.json')\n",
    "    \n",
    "    # Build vocabulary\n",
    "    print('Building vocabulary...')\n",
    "    word_to_idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "    label_to_idx = {'<PAD>': 0, 'O': 1, 'B': 2, 'I': 3}\n",
    "    idx_to_label = {0: '<PAD>', 1: 'O', 2: 'B', 3: 'I'}\n",
    "    \n",
    "    for item in train_data:\n",
    "        for token in item['tokens']:\n",
    "            if token.lower() not in word_to_idx:\n",
    "                word_to_idx[token.lower()] = len(word_to_idx)\n",
    "    \n",
    "    vocab_size = len(word_to_idx)\n",
    "    output_dim = len(label_to_idx)\n",
    "    \n",
    "    # print(\"Vocabulary: \", word_to_idx)\n",
    "    \n",
    "    print(f'Vocabulary size: {vocab_size}')\n",
    "    print(f'Number of labels: {output_dim}')\n",
    "    \n",
    "    # Load pretrained embeddings    \n",
    "    try:\n",
    "        print('Loading pretrained embeddings...')\n",
    "        embedding_dim = 300\n",
    "        glove_embeddings = np.zeros((len(word_to_idx), embedding_dim))\n",
    "        fasttext_embeddings = np.zeros((len(word_to_idx), embedding_dim))\n",
    "        \n",
    "        # Load models\n",
    "        print('Loading models...')\n",
    "        glove_model = api.load(\"glove-wiki-gigaword-300\")\n",
    "        print('Glove model loaded')\n",
    "        fasttext_model = api.load(\"fasttext-wiki-news-subwords-300\")\n",
    "        print('FastText model loaded')\n",
    "        \n",
    "        # Fill embedding matrices\n",
    "        print('Filling embedding matrices...')\n",
    "        for word, idx in word_to_idx.items():\n",
    "            if word in glove_model:\n",
    "                glove_embeddings[idx] = glove_model[word]\n",
    "            if word in fasttext_model:\n",
    "                fasttext_embeddings[idx] = fasttext_model[word]\n",
    "        \n",
    "        # Convert to torch tensors\n",
    "        print('Converting to torch tensors...')\n",
    "        glove_embeddings = torch.FloatTensor(glove_embeddings)\n",
    "        fasttext_embeddings = torch.FloatTensor(fasttext_embeddings)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading from gensim: {e}\")\n",
    "        print(\"Using random initializations for demonstration.\")\n",
    "        embedding_dim = 300\n",
    "        glove_embeddings = torch.FloatTensor(np.random.normal(0, 0.01, (vocab_size, embedding_dim)))\n",
    "        fasttext_embeddings = torch.FloatTensor(np.random.normal(0, 0.01, (vocab_size, embedding_dim)))\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    print('Creating datasets and dataloaders...')\n",
    "    train_dataset = AspectTermDataset(train_data, word_to_idx, label_to_idx)\n",
    "    val_dataset = AspectTermDataset(val_data, word_to_idx, label_to_idx)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, collate_fn=collate_fn)\n",
    "    \n",
    "    # Define models, loss, and optimizer\n",
    "    embedding_dim = 300\n",
    "    hidden_dim = 200\n",
    "    \n",
    "    # Create the directory for saving models if it doesn't exist\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    \n",
    "    models = {\n",
    "        'RNN_GloVe': RNNModel(vocab_size, embedding_dim, hidden_dim, output_dim, glove_embeddings).to(device),\n",
    "        'RNN_FastText': RNNModel(vocab_size, embedding_dim, hidden_dim, output_dim, fasttext_embeddings).to(device),\n",
    "        'GRU_GloVe': GRUModel(vocab_size, embedding_dim, hidden_dim, output_dim, glove_embeddings).to(device),\n",
    "        'GRU_FastText': GRUModel(vocab_size, embedding_dim, hidden_dim, output_dim, fasttext_embeddings).to(device)\n",
    "    }\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding index\n",
    "    epochs = 10\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Train and evaluate each model\n",
    "    for name, model in models.items():\n",
    "        print(f'\\nTraining {name}...')\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        model_save_path = f'models/{name}_best.pt'\n",
    "        \n",
    "        train_losses, val_losses = train(model, train_loader, val_loader, optimizer, criterion, device, epochs, model_save_path)\n",
    "        \n",
    "        # Plot losses\n",
    "        plot_losses(train_losses, val_losses, f'{name} Training and Validation Loss', f'plots/{name}_loss.png')\n",
    "        \n",
    "        # Load the best model\n",
    "        model.load_state_dict(torch.load(model_save_path))\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        _, _, val_f1 = test_model(model, val_data, word_to_idx, label_to_idx, idx_to_label, device)\n",
    "        \n",
    "        results[name] = {\n",
    "            'val_f1': val_f1\n",
    "        }\n",
    "        \n",
    "        print(f'{name} validation F1: {val_f1}')\n",
    "    \n",
    "    # Print results summary\n",
    "    print('\\nResults Summary:')\n",
    "    for name, result in results.items():\n",
    "        print(f'{name}: F1 = {result[\"val_f1\"]:.4f}')\n",
    "    \n",
    "    # Find the best model\n",
    "    best_model_name = max(results, key=lambda x: results[x]['val_f1'])\n",
    "    print(f'\\nBest model: {best_model_name} with F1 = {results[best_model_name][\"val_f1\"]:.4f}')\n",
    "    \n",
    "    # Save best model info\n",
    "    with open('best_model_info.json', 'w') as f:\n",
    "        json.dump({\n",
    "            'model_name': best_model_name,\n",
    "            'f1_score': results[best_model_name]['val_f1']\n",
    "        }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function for inference\n",
    "def load_and_test(test_file, model_path, model_type, word_to_idx_path, label_to_idx_path):\n",
    "    # Load model configuration\n",
    "    with open(word_to_idx_path, 'r') as f:\n",
    "        word_to_idx = json.load(f)\n",
    "    \n",
    "    with open(label_to_idx_path, 'r') as f:\n",
    "        label_to_idx = json.load(f)\n",
    "    \n",
    "    idx_to_label = {int(idx): label for label, idx in label_to_idx.items()}\n",
    "    \n",
    "    # Load test data\n",
    "    test_data = preprocess_data(test_file, f'test_task1.json')\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Initialize model\n",
    "    vocab_size = len(word_to_idx)\n",
    "    output_dim = len(label_to_idx)\n",
    "    embedding_dim = 300\n",
    "    hidden_dim = 200\n",
    "    \n",
    "    if model_type.startswith('RNN'):\n",
    "        model = RNNModel(vocab_size, embedding_dim, hidden_dim, output_dim).to(device)\n",
    "    elif model_type.startswith('GRU'):\n",
    "        model = GRUModel(vocab_size, embedding_dim, hidden_dim, output_dim).to(device)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "    \n",
    "    # Load model weights\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    \n",
    "    # Test the model\n",
    "    prec, rec, f1 = test_model(model, test_data, word_to_idx, label_to_idx, idx_to_label, device)\n",
    "    \n",
    "    print(f'Test Results for {model_type}:')\n",
    "    print(f'Precision: {prec:.4f}')\n",
    "    print(f'Recall: {rec:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "    \n",
    "    return {\n",
    "        'precision': prec,\n",
    "        'recall': rec,\n",
    "        'f1_score': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Preprocessing data...\n",
      "Building vocabulary...\n",
      "Vocabulary size: 5763\n",
      "Number of labels: 4\n",
      "Loading pretrained embeddings...\n",
      "Loading models...\n",
      "Glove model loaded\n",
      "FastText model loaded\n",
      "Filling embedding matrices...\n",
      "Converting to torch tensors...\n",
      "Creating datasets and dataloaders...\n",
      "\n",
      "Training RNN_GloVe...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 77/77 [00:32<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.2862, Val Loss: 0.1613, Val F1: 0.9399\n",
      "Model saved to models/RNN_GloVe_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 77/77 [00:23<00:00,  3.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Train Loss: 0.1419, Val Loss: 0.1334, Val F1: 0.9478\n",
      "Model saved to models/RNN_GloVe_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 77/77 [00:16<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Train Loss: 0.0938, Val Loss: 0.1329, Val F1: 0.9474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 77/77 [00:21<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Train Loss: 0.0650, Val Loss: 0.1321, Val F1: 0.9553\n",
      "Model saved to models/RNN_GloVe_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 77/77 [00:22<00:00,  3.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Train Loss: 0.0372, Val Loss: 0.1482, Val F1: 0.9544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 77/77 [00:17<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Train Loss: 0.0210, Val Loss: 0.1681, Val F1: 0.9501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 77/77 [00:17<00:00,  4.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Train Loss: 0.0112, Val Loss: 0.1720, Val F1: 0.9496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 77/77 [00:19<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Train Loss: 0.0085, Val Loss: 0.1928, Val F1: 0.9516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 77/77 [00:16<00:00,  4.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Train Loss: 0.0033, Val Loss: 0.1998, Val F1: 0.9515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 77/77 [00:16<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Train Loss: 0.0015, Val Loss: 0.2187, Val F1: 0.9499\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "tensor(1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplots\u001b[39m\u001b[38;5;124m'\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Run main function\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[37], line 106\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    103\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(model_save_path))\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# Evaluate on validation set\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m _, _, val_f1 \u001b[38;5;241m=\u001b[39m \u001b[43mtest_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_to_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_to_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx_to_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m results[name] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_f1\u001b[39m\u001b[38;5;124m'\u001b[39m: val_f1\n\u001b[0;32m    110\u001b[0m }\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m validation F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_f1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[25], line 24\u001b[0m, in \u001b[0;36mtest_model\u001b[1;34m(model, test_data, word_to_idx, label_to_idx, idx_to_label, device)\u001b[0m\n\u001b[0;32m     21\u001b[0m _, predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Convert predictions to BIO format\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m bio_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_bio\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx_to_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Get true labels\u001b[39;00m\n\u001b[0;32m     27\u001b[0m bio_true \u001b[38;5;241m=\u001b[39m convert_to_bio(idx_to_label, labels\u001b[38;5;241m.\u001b[39mcpu(), lengths)\n",
      "Cell \u001b[1;32mIn[22], line 6\u001b[0m, in \u001b[0;36mconvert_to_bio\u001b[1;34m(idx_to_label, predictions, lengths)\u001b[0m\n\u001b[0;32m      3\u001b[0m bio_predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, length \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(lengths):\n\u001b[1;32m----> 6\u001b[0m     bio_predictions\u001b[38;5;241m.\u001b[39mappend(\u001b[43m[\u001b[49m\u001b[43midx_to_label\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bio_predictions\n",
      "Cell \u001b[1;32mIn[22], line 6\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      3\u001b[0m bio_predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, length \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(lengths):\n\u001b[1;32m----> 6\u001b[0m     bio_predictions\u001b[38;5;241m.\u001b[39mappend([\u001b[43midx_to_label\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m predictions[i, :length]])\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bio_predictions\n",
      "\u001b[1;31mKeyError\u001b[0m: tensor(1)"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Create directory for plots\n",
    "    os.makedirs('plots', exist_ok=True)\n",
    "    \n",
    "    # Run main function\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of test function:\n",
    "results = load_and_test('val.json', 'models/GRU_GloVe_best.pt', 'GRU_GloVe', 'word_to_idx.json', 'label_to_idx.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
