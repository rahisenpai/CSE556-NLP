{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load BERT Tokenizer & SQuADv2 and preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhimanshu22216\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizerFast, BertForQuestionAnswering, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchcrf import CRF\n",
    "\n",
    "\n",
    "torch.manual_seed(seed=42)\n",
    "wandb.login()\n",
    "\n",
    "dataset = load_dataset('rajpurkar/squad_v2')\n",
    "dataset['train'] = dataset['train'].shuffle(seed=42).select(range(5000)) #select 32k samples for finetuning\n",
    "tokenizer = BertTokenizerFast.from_pretrained('spanbert/spanbert-large-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(examples):\n",
    "    questions = [q.strip() for q in examples['question']] #remove leading and trailing whitespaces\n",
    "    inputs = tokenizer(questions, examples['context'], max_length=512,\n",
    "                truncation='only_second', stride=128, return_overflowing_tokens=True,\n",
    "                return_offsets_mapping=True, padding='max_length')\n",
    "\n",
    "    offset_mapping = inputs.pop('offset_mapping') #mapping between tokens and original text positions\n",
    "    sample_map = inputs.pop('overflow_to_sample_mapping') #mapping of tokenized examples with original examples\n",
    "\n",
    "    #map answers to the tokenized context\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i] #index in dataset\n",
    "        answer = examples['answers'][sample_idx] #true answer for the question (ground truth)\n",
    "        start_char = answer['answer_start'][0] if answer['answer_start'] else 0\n",
    "        end_char = start_char + len(answer['text'][0]) if answer['text'] else 0\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        #find the start and end of the context\n",
    "        idx = 0\n",
    "        while idx < len(sequence_ids) and sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while idx < len(sequence_ids) and sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        #if the answer is not fully inside the context, label it (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "\n",
    "        #else find the token indices that correspond to the start and end of the answer\n",
    "        else:\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs['start_positions'] = torch.tensor(start_positions)\n",
    "    inputs['end_positions'] = torch.tensor(end_positions)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def exact_match_score(predictions, references):\n",
    "    assert len(predictions) == len(references), \"Lists must have the same length\"\n",
    "    matches = sum(p == r for p, r in zip(predictions, references))\n",
    "    return matches / len(references) * 100 # Convert to percentage\n",
    "\n",
    "\n",
    "def compute_em(preds):\n",
    "    label = preds.label_ids\n",
    "    pred = np.argmax(preds.predictions, axis=-1)\n",
    "    predictions, references = [], []\n",
    "    for i in range(len(label[0])):\n",
    "        predictions.append(str(pred[0][i]) + ',' + str(pred[1][i]))\n",
    "        references.append(str(label[0][i]) + ',' + str(label[1][i]))\n",
    "    return exact_match_score(predictions, references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_dataset, batched=True, remove_columns=dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning spanbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanbert = BertForQuestionAnswering.from_pretrained('SpanBERT/spanbert-large-cased')\n",
    "\n",
    "wandb.init(entity='cv-himanshu', project='finetuning-spanberts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir='./results', eval_strategy='epoch',\n",
    "                learning_rate=1e-5, num_train_epochs=6, weight_decay=0.01,\n",
    "                per_device_train_batch_size=64, per_device_eval_batch_size=64,\n",
    "                report_to='wandb', logging_dir='./logs', logging_steps=50)\n",
    "\n",
    "trainer = Trainer(model=spanbert, args=training_args, processing_class=tokenizer,\n",
    "    train_dataset=tokenized_dataset['train'], eval_dataset=tokenized_dataset['validation'])\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = trainer.predict(tokenized_dataset['validation'])\n",
    "\n",
    "spanbert_em = compute_em(preds)\n",
    "print(spanbert_em)\n",
    "print(f\"SpanBERT Exact Match Score: {spanbert_em:.2f} %\")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning SpanBERT_CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple, Union\n",
    "from transformers.modeling_outputs import QuestionAnsweringModelOutput\n",
    "from transformers import BertPreTrainedModel, BertModel\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "class SpanBERTCRF(BertForQuestionAnswering):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.crf = CRF(num_tags=config.num_labels, batch_first=True) #will learn transitions between start and end positions\n",
    "\n",
    "    def forward(\n",
    "       self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        start_positions: Optional[torch.Tensor] = None,\n",
    "        end_positions: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], QuestionAnsweringModelOutput]:\n",
    "        r\"\"\"\n",
    "        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n",
    "            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n",
    "            are not taken into account for computing the loss.\n",
    "        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n",
    "            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n",
    "            are not taken into account for computing the loss.\n",
    "        \"\"\"\n",
    "        \n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # Get BERT embeddings\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]  # Shape: (batch_size, sequence_length, hidden_size)\n",
    "\n",
    "        # Compute logits\n",
    "        logits = self.qa_outputs(sequence_output)  # Shape: (batch_size, sequence_length, num_labels)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1).contiguous()  # Shape: (batch_size, seq_length)\n",
    "        end_logits = end_logits.squeeze(-1).contiguous()  # Shape: (batch_size, seq_length)\n",
    "\n",
    "        total_loss = None\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            # If we are on multi-GPU, split add a dimension\n",
    "            if len(start_positions.size()) > 1:\n",
    "                start_positions = start_positions.squeeze(-1)\n",
    "            if len(end_positions.size()) > 1:\n",
    "                end_positions = end_positions.squeeze(-1)\n",
    "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
    "            ignored_index = start_logits.size(1)\n",
    "            start_positions = start_positions.clamp(0, ignored_index)\n",
    "            end_positions = end_positions.clamp(0, ignored_index)\n",
    "\n",
    "            batch_size, seq_length = start_logits.shape\n",
    "\n",
    "            # Create label tensors for CRF\n",
    "            start_labels = torch.zeros((batch_size, seq_length), dtype=torch.long, device=input_ids.device)\n",
    "            end_labels = torch.zeros((batch_size, seq_length), dtype=torch.long, device=input_ids.device)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                start, end = start_positions[i].item(), end_positions[i].item()\n",
    "                if 0 <= start < seq_length:\n",
    "                    start_labels[i, start] = 1  # Mark the correct start position\n",
    "                if 0 <= end < seq_length:\n",
    "                    end_labels[i, end] = 1  # Mark the correct end position\n",
    "\n",
    "            # Stack start and end logits\n",
    "            stacked_logits = torch.stack([start_logits, end_logits], dim=-1)  # (batch_size, seq_length, 2)\n",
    "\n",
    "            # Compute CRF loss separately for start and end logits\n",
    "            start_loss = -self.crf(stacked_logits, start_labels, mask=attention_mask.bool(), reduction=\"mean\")\n",
    "            end_loss = -self.crf(stacked_logits, end_labels, mask=attention_mask.bool(), reduction=\"mean\")\n",
    "            total_loss = (start_loss + end_loss) / 2\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (start_logits, end_logits) + outputs[2:]\n",
    "            return ((total_loss,) + output) if total_loss is not None else output\n",
    "\n",
    "        return QuestionAnsweringModelOutput(\n",
    "            loss=total_loss,\n",
    "            start_logits=start_logits,\n",
    "            end_logits=end_logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SpanBERTCRF were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['crf.end_transitions', 'crf.start_transitions', 'crf.transitions', 'qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/gpu1/himanshu/workspace/wandb/run-20250316_135413-3b5k4yss</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cv-himanshu/finetuning-spanberts/runs/3b5k4yss' target=\"_blank\">glowing-shadow-45</a></strong> to <a href='https://wandb.ai/cv-himanshu/finetuning-spanberts' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cv-himanshu/finetuning-spanberts' target=\"_blank\">https://wandb.ai/cv-himanshu/finetuning-spanberts</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cv-himanshu/finetuning-spanberts/runs/3b5k4yss' target=\"_blank\">https://wandb.ai/cv-himanshu/finetuning-spanberts/runs/3b5k4yss</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/cv-himanshu/finetuning-spanberts/runs/3b5k4yss?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fc029fb8ec0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SpanBERTCRF.from_pretrained('SpanBERT/spanbert-large-cased')\n",
    "\n",
    "wandb.init(entity='cv-himanshu', project='finetuning-spanberts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "/home/gpu1/anaconda3/envs/newenv/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='80' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [80/80 06:22, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.986413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>13.753800</td>\n",
       "      <td>4.797614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpu1/anaconda3/envs/newenv/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=80, training_loss=10.642256927490234, metrics={'train_runtime': 386.1617, 'train_samples_per_second': 25.984, 'train_steps_per_second': 0.207, 'total_flos': 9318646205607936.0, 'train_loss': 10.642256927490234, 'epoch': 2.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(output_dir='./results', eval_strategy='epoch',\n",
    "                learning_rate=1e-5, num_train_epochs=2, weight_decay=0.01,\n",
    "                per_device_train_batch_size=64, per_device_eval_batch_size=64,\n",
    "                report_to='wandb', logging_dir='./logs', logging_steps=50,\n",
    "                label_names=['start_positions', 'end_positions'],)\n",
    "\n",
    "trainer = Trainer(model=model, args=training_args, processing_class=tokenizer, \n",
    "    train_dataset=tokenized_dataset['train'], eval_dataset=tokenized_dataset['validation'])\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "SpanBERT Exact Match Score: 0.00 %\n"
     ]
    }
   ],
   "source": [
    "preds = trainer.predict(tokenized_dataset['validation'])\n",
    "\n",
    "spanbert_em = compute_em(preds)\n",
    "print(spanbert_em)\n",
    "print(f\"SpanBERT Exact Match Score: {spanbert_em:.2f} %\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
