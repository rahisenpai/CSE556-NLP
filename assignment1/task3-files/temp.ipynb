{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\nlp\\assignment1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\nlp\\nlpenv\\Lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from task1 import WordPieceTokenizer\n",
    "from task2 import Word2VecModel\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Neural LM Dataset class\n",
    "class NeuralLMDataset(Dataset):\n",
    "    def __init__(self, tokenizer, word2vec, context_size=3):\n",
    "        self.pad_token = '[PAD]'\n",
    "        self.tokenizer = tokenizer\n",
    "        self.word2vec = word2vec\n",
    "        self.context_size = context_size\n",
    "        self.data = self.preprocess_data()\n",
    "\n",
    "    def tokenize_txt_file(self, input_file: str, output_file: str) -> None:\n",
    "        \"\"\"Tokenize sentences from input TXT file and save results as JSON\"\"\"\n",
    "        # Reading the input text file\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        results = {}\n",
    "        for idx, line in enumerate(lines):\n",
    "            sentence = line.strip()  # Remove leading/trailing whitespaces or newlines\n",
    "            tokens = self.tokenizer.tokenize(sentence)  # Tokenize the sentence using your tokenization method\n",
    "            results[str(idx)] = tokens  # Store tokens with index as the key (starting from 0)\n",
    "        # Saving the results to an output JSON file\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        # self.tokenizer.construct_vocabulary(\"corpus.txt\", vocab_size=100)\n",
    "        # self.tokenize_txt_file(\"corpus.txt\", \"tokenized_corpus.json\")\n",
    "\n",
    "        corpus = None\n",
    "        with open('task2-files/tokenized_corpus.json', 'r') as f:\n",
    "            # Load the JSON data\n",
    "            tokenized_corpus = json.load(f)\n",
    "            \n",
    "            # Convert the dictionary into a list of sentences (list of tokenized words)\n",
    "            corpus = [tokens for tokens in tokenized_corpus.values()]\n",
    "            \n",
    "        \n",
    "        self.tokenized_sentences = corpus\n",
    "        # updates the word to index mapping\n",
    "        self.word2idx = {word: idx for idx, word in enumerate(self.tokenizer.vocab)}\n",
    "        # updates the reverse index to word mapping\n",
    "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
    "\n",
    "        # indexed_sentences = [[self.word2vec.embeddings(torch.tensor(self.word2idx[token], dtype=torch.long)).detach().numpy() for token in sent] for sent in self.tokenized_sentences]\n",
    "        data = []\n",
    "\n",
    "        for sent in self.tokenized_sentences:\n",
    "            if len(sent) > self.context_size:\n",
    "                for i in range(len(sent) - self.context_size):\n",
    "                    # context = sent[i:i+self.context_size]\n",
    "                    context = [self.word2vec.embeddings(torch.tensor(self.word2idx[token], dtype=torch.long)).detach().numpy() for token in sent[i:i+self.context_size]]\n",
    "                    target = sent[i+self.context_size]\n",
    "                    if target in self.word2idx:\n",
    "                        target_idx = self.word2idx[target]\n",
    "                    else:\n",
    "                        target_idx = self.word2idx['[UNK]']\n",
    "                    data.append((context, target_idx))\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context, target = self.data[idx]\n",
    "        context_tensor = torch.tensor(np.concatenate(context).flatten(), dtype=torch.float32)\n",
    "        target_tensor = torch.tensor(target, dtype=torch.long)\n",
    "        return context_tensor, target_tensor\n",
    "\n",
    "# Define Neural Language Model Variations\n",
    "class NeuralLM1(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, vocab_size):\n",
    "        super(NeuralLM1, self).__init__()\n",
    "        self.architecture = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, vocab_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.architecture(x)\n",
    "\n",
    "class NeuralLM2(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, vocab_size):\n",
    "        super(NeuralLM2, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.projection = nn.Linear(input_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = self.projection(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        out = x + identity\n",
    "        return out\n",
    "\n",
    "class NeuralLM3(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, vocab_size):\n",
    "        super(NeuralLM3, self).__init__()\n",
    "        # self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x = self.embedding(x).view(x.shape[0], -1)\n",
    "        x = self.relu(self.batch_norm(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "# Training function with loss tracking\n",
    "def train(model, train_dataloader, val_dataloader, epochs=100, lr=0.01):\n",
    "    model.to(\"cpu\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    train_losses, val_losses = [], []\n",
    "    train_acc, val_acc = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        #training phase\n",
    "        model.train()\n",
    "        train_loss, correct, total = 0, 0, 0\n",
    "        for context, target in train_dataloader:\n",
    "            optimizer.zero_grad() #zero the gradients\n",
    "            #forward pass\n",
    "            output = model(context)\n",
    "            loss = criterion(output, target)\n",
    "            train_loss += loss.item()\n",
    "            #backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #compute accuracy\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            correct += (predicted == target).sum().item()\n",
    "            total += target.size(0)\n",
    "        #calculate average training loss\n",
    "        train_losses.append(train_loss / len(train_dataloader))\n",
    "        train_acc.append(100 * correct / total)\n",
    "\n",
    "        #validation phase\n",
    "        model.eval()\n",
    "        val_loss, correct, total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for context, target in val_dataloader:\n",
    "                output = model(context)\n",
    "                loss = criterion(output, target)\n",
    "                val_loss += loss.item()\n",
    "                #compute accuracy\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                correct += (predicted == target).sum().item()\n",
    "                total += target.size(0)\n",
    "        #calculate average validation loss\n",
    "        val_losses.append(val_loss / len(val_dataloader))\n",
    "        val_acc.append(100 * correct / total)\n",
    "\n",
    "        #print losses for each epoch\n",
    "        print(f\"----- Epoch {epoch + 1}/{epochs} -----\")\n",
    "        # print(f\"Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}\")\n",
    "        print(f\"Train Loss: {train_losses[-1]:.4f}, Train Accuracy: {train_acc[-1]:.2f}%\")\n",
    "        print(f\"Val Loss: {val_losses[-1]:.4f}, Val Accuracy: {val_acc[-1]:.2f}%\")\n",
    "    print(type(train_losses[-1]))\n",
    "    print(type(output))\n",
    "    print(type(target))\n",
    "    # return train_losses, train_acc, val_losses, val_acc #return losses\n",
    "\n",
    "# Plot loss function\n",
    "def plot_losses(train_losses, val_losses):\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss vs Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Accuracy and Perplexity computation\n",
    "def compute_accuracy(model, dataloader):\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for context, target in dataloader:\n",
    "            output = model(context)\n",
    "            predictions = torch.argmax(output, dim=1)\n",
    "            correct += (predictions == target).sum().item()\n",
    "            total += target.size(0)\n",
    "    return correct / total\n",
    "\n",
    "def compute_perplexity(loss):\n",
    "    return np.exp(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_class):\n",
    "    model_path = 'task2-files/final_model/final_model.pt'\n",
    "    checkpoint = torch.load(model_path)\n",
    "    \n",
    "    model = model_class(vocab_size=checkpoint['vocab_size'], embedding_dim=checkpoint['embedding_dim'])\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    val_loss = checkpoint['val_loss']\n",
    "    val_accuracy = checkpoint['val_accuracy']\n",
    "    \n",
    "    return model, val_loss, val_accuracy\n",
    "\n",
    "# Example usage:\n",
    "word2vec, val_loss, val_accuracy = load_model(Word2VecModel)\n",
    "tokenizer = pickle.load(open('task1-files/tokenizer.pkl', 'rb'))\n",
    "dataset = NeuralLMDataset(tokenizer, word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 2\n",
    "BATCH_SIZE = 1024\n",
    "NUM_EPOCHS = 1\n",
    "LEARNING_RATE = 0.02\n",
    "TRAIN_SPLIT = 0.8\n",
    "\n",
    "# Split dataset into training and validation\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset=dataset, lengths=[TRAIN_SPLIT, 1-TRAIN_SPLIT], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "print(type(compute_perplexity(0.12)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Epoch 1/1 -----\n",
      "Train Loss: 4.9545, Train Accuracy: 31.20%\n",
      "Val Loss: 4.5821, Val Accuracy: 32.49%\n",
      "<class 'float'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "model1 = NeuralLM1(input_dim=30, hidden_dim=256, vocab_size=8500)\n",
    "train(model1, train_loader, val_loader, epochs=NUM_EPOCHS, lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = NeuralLM2(input_dim=30, hidden_dim=256, vocab_size=8500)\n",
    "train(model2, train_loader, val_loader, epochs=NUM_EPOCHS, lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = NeuralLM3(input_dim=30, hidden_dim=256, vocab_size=8500)\n",
    "train(model3, train_loader, val_loader, epochs=NUM_EPOCHS, lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tokens(sentence: str, num_tokens: int, context_size: int, dataset: NeuralLMDataset, model: nn.Module):\n",
    "    model.eval() #evaluation mode for predicing tokens\n",
    "\n",
    "    tokens = dataset.tokenizer.tokenize(sentence) #tokenize the input sentence\n",
    "    if len(tokens) < context_size:\n",
    "        tokens = [dataset.pad_token] * (context_size - len(tokens)) + tokens #add padding tokens\n",
    "\n",
    "    sentence_embeds = [dataset.word2vec.embeddings(torch.tensor(dataset.word2idx[token], dtype=torch.long)).detach().numpy() for token in tokens] #create embeddings for tokenized sentence\n",
    "\n",
    "    predicted_tokens = []\n",
    "    for _ in range(num_tokens):\n",
    "        context = sentence_embeds[-context_size:]\n",
    "        context_tensor = torch.tensor(np.concatenate(context).flatten(), dtype=torch.float32)\n",
    "        output = model(context_tensor)\n",
    "        predicted_idx = torch.argmax(output).item()\n",
    "        predicted_token = dataset.idx2word[predicted_idx]\n",
    "        predicted_tokens.append(predicted_token)\n",
    "        sentence_embeds.append(dataset.word2vec.embeddings(torch.tensor(predicted_idx, dtype=torch.long)).detach().numpy())\n",
    "    \n",
    "    return predicted_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_tokens('', 3, 3, dataset, model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediciton_pipeline(input_file, num_tokens, context_size, dataset, model):\n",
    "    with open(input_file, 'r') as f:\n",
    "        sentences = f.readlines()\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        predicted_tokens = predict_tokens(sentence, num_tokens, context_size, dataset, model)\n",
    "        print(f\"Input: {sentence}\")\n",
    "        print(f\"Predicted Tokens: {' '.join(predicted_tokens)}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediciton_pipeline('task3-files/sample_test.txt', 3, 3, dataset, model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_words = []\n",
    "for _ in range(3):\n",
    "    with torch.no_grad():\n",
    "        context = sentence_embeds[-3:]\n",
    "        context = torch.tensor(np.concatenate(context).flatten(), dtype=torch.float32)#.unsqueeze(0)  # Add batch dim\n",
    "        output = model(context)  # Predict next word\n",
    "        next_word_idx = torch.argmax(output).item()  # Get most probable word\n",
    "        # print(next_word_idx)\n",
    "        next_word = dataset.idx2word.get(next_word_idx, \"<UNK>\")  # Convert index to word\n",
    "        # print(next_word)\n",
    "        predicted_words.append(next_word)\n",
    "\n",
    "        tokens.append(next_word)\n",
    "        sentence_embeds.append(dataset.word2vec.embeddings(torch.tensor(next_word_idx, dtype=torch.long)).detach().numpy())\n",
    "\n",
    "\n",
    "print(\" \".join(predicted_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_words(model, sentence, tokenizer, word2vec, vocab, num_words=3):\n",
    "    model.eval()\n",
    "    words = tokenizer.tokenize(sentence)\n",
    "    \n",
    "    # Convert to embeddings\n",
    "    context = [word2vec[word] for word in words if word in word2vec][-3:]  # Use last 3 tokens\n",
    "    context = torch.tensor(context, dtype=torch.float32).unsqueeze(0)  # Add batch dim\n",
    "\n",
    "    predicted_words = []\n",
    "    for _ in range(num_words):\n",
    "        with torch.no_grad():\n",
    "            output = model(context)  # Predict next word\n",
    "            next_word_idx = torch.argmax(output, dim=1).item()  # Get most probable word\n",
    "            next_word = vocab[next_word_idx]  # Convert index to word\n",
    "            predicted_words.append(next_word)\n",
    "\n",
    "            # Update context by adding new word embedding\n",
    "            if next_word in word2vec:\n",
    "                next_embedding = word2vec[next_word]\n",
    "                context = torch.cat([context[:, 1:, :], torch.tensor(next_embedding).unsqueeze(0).unsqueeze(1)], dim=1)\n",
    "\n",
    "    return \" \".join(predicted_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The weather is\"\n",
    "predicted = predict_next_words(model, sentence, tokenizer, word2vec, vocab)\n",
    "print(f\"Predicted words: {predicted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training\n",
    "dataset = NeuralLMDataset(corpus, text_processor.word2idx)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Train models\n",
    "embedding_dim = 10\n",
    "hidden_dim = 20\n",
    "\n",
    "models = [NeuralLM1, NeuralLM2, NeuralLM3]\n",
    "losses = {}\n",
    "accuracies = {}\n",
    "perplexities = {}\n",
    "\n",
    "for i, model_class in enumerate(models):\n",
    "    model = model_class(len(text_processor.vocab), embedding_dim, hidden_dim)\n",
    "    loss = train(model, dataloader)\n",
    "    losses[f'NeuralLM{i+1}'] = loss\n",
    "    acc = compute_accuracy(model, dataloader)\n",
    "    accuracies[f'NeuralLM{i+1}'] = acc\n",
    "    perplexities[f'NeuralLM{i+1}'] = compute_perplexity(loss[-1])\n",
    "    plot_losses(loss, f'NeuralLM{i+1} Training Loss')\n",
    "    torch.save(model.state_dict(), f\"neural_lm{i+1}.pth\")\n",
    "    print(f\"NeuralLM{i+1} - Accuracy: {acc}, Perplexity: {perplexities[f'NeuralLM{i+1}']}\")\n",
    "\n",
    "# Predict next 3 tokens from test.txt\n",
    "def predict_next_tokens(model, sentence, n=3):\n",
    "    words = sentence.split()\n",
    "    context = torch.tensor([text_processor.word2idx[word] for word in words[-2:]], dtype=torch.long)\n",
    "    predictions = []\n",
    "    for _ in range(n):\n",
    "        output = model(context.unsqueeze(0))\n",
    "        next_word_idx = torch.argmax(output, dim=1).item()\n",
    "        next_word = text_processor.idx2word[next_word_idx]\n",
    "        predictions.append(next_word)\n",
    "        context = torch.cat((context[1:], torch.tensor([next_word_idx])))\n",
    "    return predictions\n",
    "\n",
    "print(\"Model checkpoints saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
