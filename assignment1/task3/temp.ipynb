{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\nlp\\assignment1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\nlp\\nlpenv\\Lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from Task1.task1 import WordPieceTokenizer\n",
    "from Task2.task2 import Word2VecModel\n",
    "\n",
    "\n",
    "# Neural LM Dataset class\n",
    "class NeuralLMDataset(Dataset):\n",
    "    def __init__(self, tokenizer, word2vec, context_size=3):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.word2vec = word2vec\n",
    "        self.context_size = context_size\n",
    "        self.data = self.preprocess_data()\n",
    "        # self.pad_token = '[PAD]'\n",
    "        # self.context_size = context_size\n",
    "        # self.word2idx = None\n",
    "        # self.corpus = None\n",
    "        # self.data = []\n",
    "        # for sentence in corpus:\n",
    "        #     words = sentence.split()\n",
    "        #     for i in range(context_size, len(words) - 1):\n",
    "        #         context = words[i - context_size:i]\n",
    "        #         target = words[i]\n",
    "        #         self.data.append((context, target))\n",
    "        # self.word2idx = word2idx\n",
    "\n",
    "    def tokenize_txt_file(self, input_file: str, output_file: str) -> None:\n",
    "        \"\"\"Tokenize sentences from input TXT file and save results as JSON\"\"\"\n",
    "        # Reading the input text file\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        results = {}\n",
    "        for idx, line in enumerate(lines):\n",
    "            sentence = line.strip()  # Remove leading/trailing whitespaces or newlines\n",
    "            tokens = self.tokenizer.tokenize(sentence)  # Tokenize the sentence using your tokenization method\n",
    "            results[str(idx)] = tokens  # Store tokens with index as the key (starting from 0)\n",
    "        # Saving the results to an output JSON file\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        self.tokenizer.construct_vocabulary(\"corpus.txt\", vocab_size=100)\n",
    "        self.tokenize_txt_file(\"corpus.txt\", \"tokenized_corpus.json\")\n",
    "\n",
    "        corpus = None\n",
    "        with open('tokenized_corpus.json', 'r', encoding='utf-8') as f:\n",
    "            # Load the JSON data\n",
    "            tokenized_corpus = json.load(f)\n",
    "            \n",
    "            # Convert the dictionary into a list of sentences (list of tokenized words)\n",
    "            corpus = [tokens for tokens in tokenized_corpus.values()]\n",
    "            \n",
    "        \n",
    "        self.tokenized_sentences = corpus\n",
    "        # updates the word to index mapping\n",
    "        self.word2idx = {word: idx for idx, word in enumerate(self.tokenizer.vocab)}\n",
    "        # updates the reverse index to word mapping\n",
    "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
    "\n",
    "        # indexed_sentences = [[self.word2vec.embeddings(torch.tensor(self.word2idx[token], dtype=torch.long)).detach().numpy() for token in sent] for sent in self.tokenized_sentences]\n",
    "        data = []\n",
    "\n",
    "        for sent in self.tokenized_sentences:\n",
    "            if len(sent) > self.context_size:\n",
    "                for i in range(len(sent) - self.context_size):\n",
    "                    # context = sent[i:i+self.context_size]\n",
    "                    context = [self.word2vec.embeddings(torch.tensor(self.word2idx[token], dtype=torch.long)).detach().numpy() for token in sent[i:i+self.context_size]]\n",
    "                    target = sent[i+self.context_size]\n",
    "                    if target in self.word2idx:\n",
    "                        target_idx = self.word2idx[target]\n",
    "                    else:\n",
    "                        target_idx = self.word2idx['[UNK]']\n",
    "                    data.append((context, target_idx))\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context, target = self.data[idx]\n",
    "        context_tensor = torch.tensor(np.concatenate(context).flatten(), dtype=torch.float32)\n",
    "        target_tensor = torch.tensor(target, dtype=torch.long)\n",
    "        return context_tensor, target_tensor\n",
    "\n",
    "# Define Neural Language Model Variations\n",
    "class NeuralLM1(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, vocab_size):\n",
    "        super(NeuralLM1, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.out = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x = self.embedding(x).view(x.shape[0], -1)\n",
    "        x = self.relu(self.fc(x))\n",
    "        return self.out(x)\n",
    "\n",
    "class NeuralLM2(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(NeuralLM2, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc1 = nn.Linear(embedding_dim * 2, hidden_dim)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).view(x.shape[0], -1)\n",
    "        x = self.tanh(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "class NeuralLM3(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(NeuralLM3, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc1 = nn.Linear(embedding_dim * 2, hidden_dim)\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).view(x.shape[0], -1)\n",
    "        x = self.relu(self.batch_norm(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "# Training function with loss tracking\n",
    "def train(model, train_dataloader, val_dataloader, epochs=100, lr=0.01):\n",
    "    model.to(\"cpu\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    train_losses, val_losses = [], []\n",
    "    train_acc, val_acc = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        #training phase\n",
    "        model.train()\n",
    "        train_loss, correct, total = 0, 0, 0\n",
    "        for context, target in train_dataloader:\n",
    "            optimizer.zero_grad() #zero the gradients\n",
    "            #forward pass\n",
    "            output = model(context)\n",
    "            loss = criterion(output, target)\n",
    "            train_loss += loss.item()\n",
    "            #backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #compute accuracy\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            correct += (predicted == target).sum().item()\n",
    "            total += target.size(0)\n",
    "        #calculate average training loss\n",
    "        train_losses.append(train_loss / len(train_dataloader))\n",
    "        train_acc.append(100 * correct / total)\n",
    "\n",
    "        #validation phase\n",
    "        model.eval()\n",
    "        val_loss, correct, total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for context, target in val_dataloader:\n",
    "                output = model(context)\n",
    "                loss = criterion(output, target)\n",
    "                val_loss += loss.item()\n",
    "                #compute accuracy\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                correct += (predicted == target).sum().item()\n",
    "                total += target.size(0)\n",
    "        #calculate average validation loss\n",
    "        val_losses.append(val_loss / len(val_dataloader))\n",
    "        val_acc.append(100 * correct / total)\n",
    "\n",
    "        #print losses for each epoch\n",
    "        print(f\"----- Epoch {epoch + 1}/{epochs} -----\")\n",
    "        # print(f\"Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}\")\n",
    "        print(f\"Train Loss: {train_losses[-1]:.4f}, Train Accuracy: {train_acc[-1]:.2f}%\")\n",
    "        print(f\"Val Loss: {val_losses[-1]:.4f}, Val Accuracy: {val_acc[-1]:.2f}%\")\n",
    "\n",
    "    return train_losses, train_acc, val_losses, val_acc #return losses\n",
    "\n",
    "# Plot loss function\n",
    "def plot_losses(train_losses, val_losses):\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss vs Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Accuracy and Perplexity computation\n",
    "def compute_accuracy(model, dataloader):\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for context, target in dataloader:\n",
    "            output = model(context)\n",
    "            predictions = torch.argmax(output, dim=1)\n",
    "            correct += (predictions == target).sum().item()\n",
    "            total += target.size(0)\n",
    "    return correct / total\n",
    "\n",
    "def compute_perplexity(loss):\n",
    "    return np.exp(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\nlp\\assignment1\\task3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\nlp\\nlpenv\\Lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd task3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(final_model_dir, model_class):\n",
    "    model_path = r'D:\\nlp\\assignment1\\Task2\\final_model\\final_model.pt'\n",
    "    checkpoint = torch.load(model_path)\n",
    "    \n",
    "    model = model_class(vocab_size=checkpoint['vocab_size'], embedding_dim=checkpoint['embedding_dim'])\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    val_loss = checkpoint['val_loss']\n",
    "    val_accuracy = checkpoint['val_accuracy']\n",
    "    \n",
    "    return model, val_loss, val_accuracy\n",
    "\n",
    "def load_vocabulary(final_model_dir):\n",
    "    vocab_path = r'D:\\nlp\\assignment1\\Task2\\final_model\\vocabulary.json'\n",
    "    with open(vocab_path, 'r') as f:\n",
    "        vocab_data = json.load(f)\n",
    "    return vocab_data['word2idx'], vocab_data['idx2word']\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "word2vec, val_loss, val_accuracy = load_model('final_model', Word2VecModel)\n",
    "word2idx, idx2word = load_vocabulary('final_model')\n",
    "tokenizer = WordPieceTokenizer()\n",
    "dataset = NeuralLMDataset(tokenizer, word2vec)\n",
    "\n",
    "WINDOW_SIZE = 2\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 0.005\n",
    "TRAIN_SPLIT = 0.8\n",
    "\n",
    "# Split dataset into training and validation\n",
    "train_size = int(TRAIN_SPLIT * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, val_size]\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "model = NeuralLM1(input_dim=300, hidden_dim=256, vocab_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Epoch 1/10 -----\n",
      "Train Loss: 2.4937, Train Accuracy: 33.28%\n",
      "Val Loss: 2.3312, Val Accuracy: 36.56%\n",
      "----- Epoch 2/10 -----\n",
      "Train Loss: 2.2759, Train Accuracy: 37.42%\n",
      "Val Loss: 2.2691, Val Accuracy: 37.53%\n",
      "----- Epoch 3/10 -----\n",
      "Train Loss: 2.2135, Train Accuracy: 38.58%\n",
      "Val Loss: 2.2483, Val Accuracy: 38.28%\n",
      "----- Epoch 4/10 -----\n",
      "Train Loss: 2.1832, Train Accuracy: 39.14%\n",
      "Val Loss: 2.2304, Val Accuracy: 39.24%\n",
      "----- Epoch 5/10 -----\n",
      "Train Loss: 2.1576, Train Accuracy: 39.72%\n",
      "Val Loss: 2.2021, Val Accuracy: 39.41%\n",
      "----- Epoch 6/10 -----\n",
      "Train Loss: 2.1390, Train Accuracy: 40.02%\n",
      "Val Loss: 2.2245, Val Accuracy: 39.16%\n",
      "----- Epoch 7/10 -----\n",
      "Train Loss: 2.1262, Train Accuracy: 40.47%\n",
      "Val Loss: 2.1976, Val Accuracy: 39.95%\n",
      "----- Epoch 8/10 -----\n",
      "Train Loss: 2.1170, Train Accuracy: 40.51%\n",
      "Val Loss: 2.1886, Val Accuracy: 39.86%\n",
      "----- Epoch 9/10 -----\n",
      "Train Loss: 2.1062, Train Accuracy: 40.68%\n",
      "Val Loss: 2.2010, Val Accuracy: 39.95%\n",
      "----- Epoch 10/10 -----\n",
      "Train Loss: 2.1032, Train Accuracy: 40.72%\n",
      "Val Loss: 2.1931, Val Accuracy: 40.07%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([2.493681924516,\n",
       "  2.2759403121610147,\n",
       "  2.2135160806036036,\n",
       "  2.183190859634106,\n",
       "  2.1575845014654105,\n",
       "  2.139018481292144,\n",
       "  2.1262141663577796,\n",
       "  2.117036268875443,\n",
       "  2.1061677941277077,\n",
       "  2.1031842189061396],\n",
       " [33.277099156487346,\n",
       "  37.42396135942039,\n",
       "  38.57897868468027,\n",
       "  39.13828707430611,\n",
       "  39.719995799936996,\n",
       "  40.02450036750551,\n",
       "  40.468307024605366,\n",
       "  40.5138077071156,\n",
       "  40.67971019565294,\n",
       "  40.718910783661755],\n",
       " [2.3311901339462824,\n",
       "  2.2691075780561993,\n",
       "  2.248342454433441,\n",
       "  2.2304088030542646,\n",
       "  2.202116648214204,\n",
       "  2.224470841884613,\n",
       "  2.197626953465598,\n",
       "  2.188559335895947,\n",
       "  2.2009861043521335,\n",
       "  2.1930630160229545],\n",
       " [36.56269250154001,\n",
       "  37.53150025200202,\n",
       "  38.27910623284986,\n",
       "  39.24231393851151,\n",
       "  39.41311530492244,\n",
       "  39.161113288906314,\n",
       "  39.953519628157025,\n",
       "  39.86111888895111,\n",
       "  39.945119560956485,\n",
       "  40.07112056896455])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(model, train_loader, val_loader, epochs=NUM_EPOCHS, lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The weather is\"\n",
    "model.eval()\n",
    "words = tokenizer.tokenize(sentence)\n",
    "context = [dataset.word2vec.embeddings(torch.tensor(dataset.word2idx[token], dtype=torch.long)).detach().numpy() for token in words][-3:]\n",
    "context = torch.tensor(np.concatenate(context).flatten(), dtype=torch.float32).unsqueeze(0)  # Add batch dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['th', '##e', 'w', '##e', '##a', '##t', '##h', '##e', '##r', 'i', '##s']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m             next_embedding \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mword2vec\u001b[38;5;241m.\u001b[39membeddings(torch\u001b[38;5;241m.\u001b[39mtensor(word2idx[next_word], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong))\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m     12\u001b[0m             next_embedding \u001b[38;5;241m=\u001b[39m next_embedding\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Reshape for batch\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m             context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43mcontext\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m, next_embedding], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(predicted_words))\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "predicted_words = []\n",
    "for _ in range(3):\n",
    "    with torch.no_grad():\n",
    "        output = model(context)  # Predict next word\n",
    "        next_word_idx = torch.argmax(output, dim=1).item()  # Get most probable word\n",
    "        next_word = dataset.idx2word.get(next_word_idx, \"<UNK>\")  # Convert index to word\n",
    "        predicted_words.append(next_word)\n",
    "\n",
    "        # Update context by adding new word embedding\n",
    "        if next_word in word2idx:\n",
    "            next_embedding = dataset.word2vec.embeddings(torch.tensor(word2idx[next_word], dtype=torch.long)).detach()\n",
    "            next_embedding = next_embedding.unsqueeze(0).unsqueeze(1)  # Reshape for batch\n",
    "            context = torch.cat([context[:, 1:, :], next_embedding], dim=1)\n",
    "\n",
    "print(\" \".join(predicted_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_words(model, sentence, tokenizer, word2vec, vocab, num_words=3):\n",
    "    model.eval()\n",
    "    words = tokenizer.tokenize(sentence)\n",
    "    \n",
    "    # Convert to embeddings\n",
    "    context = [word2vec[word] for word in words if word in word2vec][-3:]  # Use last 3 tokens\n",
    "    context = torch.tensor(context, dtype=torch.float32).unsqueeze(0)  # Add batch dim\n",
    "\n",
    "    predicted_words = []\n",
    "    for _ in range(num_words):\n",
    "        with torch.no_grad():\n",
    "            output = model(context)  # Predict next word\n",
    "            next_word_idx = torch.argmax(output, dim=1).item()  # Get most probable word\n",
    "            next_word = vocab[next_word_idx]  # Convert index to word\n",
    "            predicted_words.append(next_word)\n",
    "\n",
    "            # Update context by adding new word embedding\n",
    "            if next_word in word2vec:\n",
    "                next_embedding = word2vec[next_word]\n",
    "                context = torch.cat([context[:, 1:, :], torch.tensor(next_embedding).unsqueeze(0).unsqueeze(1)], dim=1)\n",
    "\n",
    "    return \" \".join(predicted_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The weather is\"\n",
    "predicted = predict_next_words(model, sentence, tokenizer, word2vec, vocab)\n",
    "print(f\"Predicted words: {predicted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training\n",
    "dataset = NeuralLMDataset(corpus, text_processor.word2idx)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Train models\n",
    "embedding_dim = 10\n",
    "hidden_dim = 20\n",
    "\n",
    "models = [NeuralLM1, NeuralLM2, NeuralLM3]\n",
    "losses = {}\n",
    "accuracies = {}\n",
    "perplexities = {}\n",
    "\n",
    "for i, model_class in enumerate(models):\n",
    "    model = model_class(len(text_processor.vocab), embedding_dim, hidden_dim)\n",
    "    loss = train(model, dataloader)\n",
    "    losses[f'NeuralLM{i+1}'] = loss\n",
    "    acc = compute_accuracy(model, dataloader)\n",
    "    accuracies[f'NeuralLM{i+1}'] = acc\n",
    "    perplexities[f'NeuralLM{i+1}'] = compute_perplexity(loss[-1])\n",
    "    plot_losses(loss, f'NeuralLM{i+1} Training Loss')\n",
    "    torch.save(model.state_dict(), f\"neural_lm{i+1}.pth\")\n",
    "    print(f\"NeuralLM{i+1} - Accuracy: {acc}, Perplexity: {perplexities[f'NeuralLM{i+1}']}\")\n",
    "\n",
    "# Predict next 3 tokens from test.txt\n",
    "def predict_next_tokens(model, sentence, n=3):\n",
    "    words = sentence.split()\n",
    "    context = torch.tensor([text_processor.word2idx[word] for word in words[-2:]], dtype=torch.long)\n",
    "    predictions = []\n",
    "    for _ in range(n):\n",
    "        output = model(context.unsqueeze(0))\n",
    "        next_word_idx = torch.argmax(output, dim=1).item()\n",
    "        next_word = text_processor.idx2word[next_word_idx]\n",
    "        predictions.append(next_word)\n",
    "        context = torch.cat((context[1:], torch.tensor([next_word_idx])))\n",
    "    return predictions\n",
    "\n",
    "print(\"Model checkpoints saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
