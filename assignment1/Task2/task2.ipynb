{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Implement Word2vec 25 Marks\n",
    "You are tasked with building a pipeline for training a Word2Vec model using the CBOW (Continuous Bag\n",
    "of Words) approach FROM SCRATCH in PyTorch. It consist of the following components:\n",
    "\n",
    "1. You are required to create a Python class named Word2VecDataset that will serve as a custom dataset\n",
    "for training the Word2Vec model. The implementaion should include the following components:\n",
    "\n",
    "    - The custom implementation should work with PyTorch’s DataLoader to efficiently load the train-\n",
    "ing data.. You can refer this guide [Tutorial] on creating custom dataset classes in PyTroch.\n",
    "\n",
    "    - preprocess data - In this method, you will be preprocessing the provided corpus and prepare\n",
    "the CBOW training data for training the Word2Vec model.\n",
    "\n",
    "    - During preprocessing, you must use the WordPieceTokenizer implemented in Task 1 to tokenize\n",
    "the input text corpus.\n",
    "\n",
    "2. You required to create a Python class named Word2VecModel which implement Word2Vec CBOW\n",
    "architecture from scratch using PyTorch. After training the the model, save the trained model’s\n",
    "checkpoint for later use.\n",
    "\n",
    "3. Develop a function named train to manage the entire training process of the Word2Vec model. This\n",
    "function should include all the training logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ISHITA\\OneDrive\\Desktop\\CSAM6\\NLP\\Assignments\\Assignment 1\\CSE556-NLP\\assignment1\\Task2\n",
      "c:\\Users\\ISHITA\\OneDrive\\Desktop\\CSAM6\\NLP\\Assignments\\Assignment 1\\CSE556-NLP\\assignment1\\Task1\n",
      "c:\\Users\\ISHITA\\OneDrive\\Desktop\\CSAM6\\NLP\\Assignments\\Assignment 1\\CSE556-NLP\\assignment1\\Task2\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(os.path.abspath('../Task 1'))\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "# changing directories to get the WordPieceTokenizer class from task1\n",
    "os.chdir('../Task1')\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "from task1 import WordPieceTokenizer\n",
    "\n",
    "tokenizer = WordPieceTokenizer()\n",
    "\n",
    "os.chdir('../Task2')\n",
    "\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2VecDataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2VecDataset(Dataset) => inherits from the Dataset class of Pytorch\n",
    "class Word2VecDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, window_size, vocabulary_size):\n",
    "        \n",
    "        \"\"\"\n",
    "        text : input corput as string\n",
    "        window size : defines how many words to take on either side as context\n",
    "        pad token : a special token used for padding in case of insufficient context\n",
    "        \"\"\"\n",
    "\n",
    "        self.pad_token = '[PAD]'\n",
    "        self.window_size = window_size\n",
    "        self.tokenizer = WordPieceTokenizer()\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "\n",
    "        self.text = None\n",
    "\n",
    "        # stores unique words in the corpus\n",
    "        self.vocabulary = None\n",
    "        # a dictionary mapping words to indices\n",
    "        self.word_to_idx = None\n",
    "        # a dictionary mapping indices to words\n",
    "        self.idx_to_word = None\n",
    "        # a list that will store training pairs\n",
    "        self.cbow_pairs = []\n",
    "        \n",
    "        self.preprocess_data()\n",
    "\n",
    "    # uses Word Piece Tokenizer from Task 1 to tokenize input corpus\n",
    "    def tokenize_txt_file(self, input_file, output_file):\n",
    "        \n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for idx, line in enumerate(lines):\n",
    "            sentence = line.strip() \n",
    "            tokens = self.tokenizer.tokenize(sentence)\n",
    "            results[str(idx)] = tokens \n",
    "\n",
    "        \n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "\n",
    "    \n",
    "\n",
    "    def update_vocabulary(self):\n",
    "\n",
    "        self.tokenizer.construct_vocabulary(\"corpus.txt\", vocab_size=self.vocabulary_size)\n",
    "\n",
    "        vocabulary = []\n",
    "\n",
    "        # Open the file in read mode\n",
    "        with open('vocabulary_35.txt', 'r') as file:\n",
    "            \n",
    "            for line in file:\n",
    "                word = line.strip()\n",
    "                if word:  # to avoid adding empty lines\n",
    "                    vocabulary.append(word)\n",
    "\n",
    "        self.vocabulary = vocabulary\n",
    "        print(\"got and updated vocab\")\n",
    "\n",
    "    def tokenize_corpus(self):\n",
    "        \n",
    "        self.tokenize_txt_file(\"corpus.txt\", \"tokenized_corpus.json\")\n",
    "\n",
    "        corpus = None\n",
    "\n",
    "        with open('tokenized_corpus.json', 'r', encoding='utf-8') as f:\n",
    "\n",
    "            # Load the JSON data\n",
    "            tokenized_corpus = json.load(f)\n",
    "            \n",
    "            # Convert the dictionary into a list of sentences (list of tokenized words)\n",
    "            corpus = [tokens for tokens in tokenized_corpus.values()]\n",
    "            \n",
    "        \n",
    "        self.text = corpus\n",
    "\n",
    "\n",
    "    def generate_cbow_pairs(self):\n",
    "\n",
    "        # loops over all the  in the tokenized corpus\n",
    "        for sentence in self.text:\n",
    "\n",
    "            for j in range(len(sentence)):\n",
    "                # Get context words within window\n",
    "                context_words = (sentence[max(0, j - self.window_size):j] + \n",
    "                                sentence[j + 1:min(len(sentence), j + self.window_size + 1)])\n",
    "                \n",
    "                if len(context_words) > 0:\n",
    "                    # Pad context if necessary\n",
    "                    while len(context_words) < self.window_size * 2:\n",
    "                        context_words.append(self.pad_token)\n",
    "                    \n",
    "                    # Convert context words and target words into numerical indices using self.word2idx\n",
    "                    context_indices = [self.word_to_idx.get(w, self.word_to_idx[self.pad_token]) for w in context_words]\n",
    "                    target_idx = self.word_to_idx.get(sentence[j], self.word_to_idx[self.pad_token])\n",
    "                    \n",
    "                    # Store the (context, target) pairs in self.cbow_pairs\n",
    "                    self.cbow_pairs.append((context_indices, target_idx))\n",
    "\n",
    "\n",
    "    # this function tokenizes text, creates the vocabulary and generates CBOW training pairs\n",
    "    def preprocess_data(self):\n",
    "\n",
    "        print(\"updating vocabulary\")     \n",
    "        self.update_vocabulary()\n",
    "\n",
    "        print(\"tokenizing corpus\")\n",
    "        self.tokenize_corpus()\n",
    "\n",
    "        print(\"updating mapping - word2idx and idx2word\")\n",
    "\n",
    "        # updates the word to index mapping\n",
    "        self.word_to_idx = {word: idx for idx, word in enumerate(self.vocabulary)}\n",
    "        # updates the reverse index to word mapping\n",
    "        self.idx_to_word = {idx: word for word, idx in self.word_to_idx.items()}\n",
    "        \n",
    "\n",
    "        print(\"generating cbow pairs\")\n",
    "        self.generate_cbow_pairs()\n",
    "    \n",
    "    # returns the total number of CBOW training pairs\n",
    "    def __len__(self):\n",
    "        return len(self.cbow_pairs)\n",
    "    \n",
    "    \n",
    "    # used to retrieve a single CBOW pair (context, target) pair\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # get the pair of ith index as mentioned parameter \n",
    "        context_indices, target_idx = self.cbow_pairs[idx]\n",
    "        \n",
    "        # Convert to tensors with explicit types\n",
    "        context_tensor = torch.tensor(context_indices, dtype=torch.long)\n",
    "        target_tensor = torch.tensor(target_idx, dtype=torch.long)\n",
    "        \n",
    "        return context_tensor, target_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2VecModel class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2VecModel(nn.Module) => Inherits from nn.Module class in Pytorch\n",
    "class Word2VecModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        \n",
    "        '''\n",
    "        vocab size : the size of the vocabulary\n",
    "        embedding dimension : the number of features that represent each word\n",
    "        '''\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # creates a special table that stores word representations for each word\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.output_layer = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    # function to initialize weight\n",
    "    def _initialize_weights(self):\n",
    "        \n",
    "        # range of weights\n",
    "        initrange = 0.5 / self.embedding_dim\n",
    "        \n",
    "        # initializing embedding layer\n",
    "        self.embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "        # initializing output layer\n",
    "        self.output_layer.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "        # initializing the bias of output layer\n",
    "        self.output_layer.bias.data.zero_()\n",
    "    \n",
    "\n",
    "    # function for forward pass of the model\n",
    "    def forward(self, x: torch.Tensor):\n",
    "\n",
    "        \"\"\"\n",
    "        x : input tensor, which represents the indices of the context words in the vocabulary. Its shape is [batch size, window size*2] -- [target word, [list of indexes of context words]]\n",
    "        \"\"\"\n",
    "\n",
    "        # will look up for embeddings for each word in context and output list of embeddings of each congext word\n",
    "        # size = [batch size, window size*2, embedding dimension]\n",
    "        embedded = self.embeddings(x)\n",
    "        \n",
    "        # calculates average of context words... horizontally - row by row\n",
    "        # size = [batch_size, embedding_dim]\n",
    "        context_embedding = torch.mean(embedded, dim=1)\n",
    "        \n",
    "        # passing the average context embedding to output layer\n",
    "        # output size vector = [batch_size, vocab_size]\n",
    "        output = self.output_layer(context_embedding)\n",
    "\n",
    "        # returning after application of softmax function\n",
    "        return F.log_softmax(output, dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for saving and loading checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, checkpoint_path, epoch, optimizer, loss, accuracy):\n",
    "\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "        'accuracy': accuracy,\n",
    "        'vocab_size': model.vocab_size,\n",
    "        'embedding_dim': model.embedding_dim,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "\n",
    "def load_checkpoint(model, checkpoint_path, device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    # Load model state\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    \n",
    "    return (\n",
    "        model,\n",
    "        checkpoint['optimizer_state_dict'],\n",
    "        checkpoint['epoch'],\n",
    "        checkpoint['loss']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    num_epochs: int,\n",
    "    learning_rate: float,\n",
    "    checkpoint_dir: str = 'checkpoints',\n",
    "    save_frequency: int = 5,\n",
    "    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "):\n",
    "    \n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    history = {\n",
    "        'epoch_losses': [],\n",
    "        'epoch_accuracies': [],\n",
    "        'batch_losses': [],\n",
    "        'batch_accuracies': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Compute decayed learning rate\n",
    "        # current_lr = learning_rate * (1 - epoch / num_epochs)\n",
    "        current_lr = learning_rate\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = current_lr\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        epoch_correct = 0\n",
    "        epoch_total = 0\n",
    "        num_batches = len(train_loader)\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        model.train()\n",
    "\n",
    "        for batch_idx, (context, target) in enumerate(progress_bar):\n",
    "            context, target = context.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model.forward(context)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            predicted = output.argmax(dim=1)\n",
    "            correct = (predicted == target).sum().item()\n",
    "            total = target.size(0)\n",
    "            \n",
    "            batch_accuracy = (correct / total) * 100\n",
    "            batch_loss = loss.item()\n",
    "            epoch_loss += batch_loss\n",
    "            epoch_correct += correct\n",
    "            epoch_total += total\n",
    "            \n",
    "            history['batch_losses'].append(batch_loss)\n",
    "            history['batch_accuracies'].append(batch_accuracy)\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                'batch_loss': f'{batch_loss:.4f}',\n",
    "                'batch_acc': f'{batch_accuracy:.2f}%',\n",
    "                'avg_loss': f'{epoch_loss/(batch_idx+1):.4f}',\n",
    "                'lr': f'{current_lr:.6f}'\n",
    "            })\n",
    "        \n",
    "        avg_epoch_loss = epoch_loss / num_batches\n",
    "        epoch_accuracy = (epoch_correct / epoch_total) * 100\n",
    "        history['epoch_losses'].append(avg_epoch_loss)\n",
    "        history['epoch_accuracies'].append(epoch_accuracy)\n",
    "        \n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Average Loss: {avg_epoch_loss:.4f}')\n",
    "        print(f'Training Accuracy: {epoch_accuracy:.2f}%')\n",
    "        \n",
    "        print(\"\\nValidating model...\")\n",
    "        val_loss, accuracy, cosine_similarity = validate_model(model, val_loader, device)\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "        print(f\"Validation Accuracy: {accuracy:.2f}%\")\n",
    "        print(f\"Cosine Similarity: {cosine_similarity}\")\n",
    "        \n",
    "        if (epoch + 1) % save_frequency == 0:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f'word2vec_checkpoint_epoch_{epoch+1}.pt')\n",
    "            save_checkpoint(model, checkpoint_path, epoch, optimizer, avg_epoch_loss, epoch_accuracy)\n",
    "            print(f'Checkpoint saved: {checkpoint_path}')\n",
    "    \n",
    "    final_checkpoint_path = os.path.join(checkpoint_dir, 'word2vec_final_model.pt')\n",
    "    save_checkpoint(model, final_checkpoint_path, num_epochs-1, optimizer, history['epoch_losses'][-1], history['epoch_accuracies'][-1])\n",
    "    print(f'Final model saved: {final_checkpoint_path}')\n",
    "    \n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validate_model(model, val_loader, device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    \n",
    "    model.eval()\n",
    "    criterion = nn.NLLLoss()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    total_cosine_sim = 0.0\n",
    "    num_batches = len(val_loader)\n",
    "    \n",
    "    # Cosine similarity function\n",
    "    cos_sim = nn.CosineSimilarity(dim=1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for context, target in val_loader:\n",
    "            context = context.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(context)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # Calculate loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predicted = output.argmax(dim=1)\n",
    "            total_correct += (predicted == target).sum().item()\n",
    "            total_samples += target.size(0)\n",
    "            \n",
    "            # Calculate cosine similarity\n",
    "            # Get embeddings for predicted and target words\n",
    "            predicted_embeddings = model.embeddings(predicted)\n",
    "            target_embeddings = model.embeddings(target)\n",
    "            \n",
    "            # Calculate cosine similarity between predicted and target embeddings\n",
    "            batch_cosine_sim = cos_sim(predicted_embeddings, target_embeddings).mean()\n",
    "            total_cosine_sim += batch_cosine_sim.item()\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    avg_loss = total_loss / num_batches\n",
    "    accuracy = (total_correct / total_samples) * 100\n",
    "    avg_cosine_sim = total_cosine_sim / num_batches\n",
    "    \n",
    "    return avg_loss, accuracy, avg_cosine_sim\n",
    "\n",
    "def evaluate_model(model, val_loader, device, dataset, BATCH_SIZE) :\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (context, target) in enumerate(val_loader):\n",
    "            context = context.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            # Get model prediction\n",
    "            output = model(context)\n",
    "            predicted_indices = output.argmax(dim=1)\n",
    "            \n",
    "            # Convert to words\n",
    "            for j in range(len(context)):\n",
    "                context_words = [dataset.idx2word[idx.item()] for idx in context[j]]\n",
    "                true_word = dataset.idx2word[target[j].item()]\n",
    "                predicted_word = dataset.idx2word[predicted_indices[j].item()]\n",
    "                \n",
    "                print(f\"\\nPair {i*BATCH_SIZE + j + 1}:\")\n",
    "                print(f\"Context: {context_words}\")\n",
    "                print(f\"True word: {true_word}\")\n",
    "                print(f\"Predicted: {predicted_word}\")\n",
    "                print(f\"Correct: {'✓' if true_word == predicted_word else '✗'}\")\n",
    "            \n",
    "            # Print only first 5 validation pairs for brevity\n",
    "            if i >= 4:\n",
    "                print(\"\\n... (showing first 20 pairs only)\")\n",
    "                break\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Plot both training loss and accuracy over epochs\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot loss\n",
    "    ax1.plot(history['epoch_losses'], label='Training Loss')\n",
    "    ax1.set_title('Training Loss Over Epochs')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax2.plot(history['epoch_accuracies'], label='Training Accuracy', color='green')\n",
    "    ax2.set_title('Training Accuracy Over Epochs')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(final_model_dir, model, val_loss, accuracy):\n",
    "    \n",
    "    model_path = os.path.join(final_model_dir, 'final_model.pt')\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'vocab_size': model.vocab_size,\n",
    "        'embedding_dim': model.embedding_dim,\n",
    "        'val_loss': val_loss,\n",
    "        'val_accuracy': accuracy\n",
    "    }, model_path)\n",
    "\n",
    "\n",
    "def save_vocabulary(final_model_dir, dataset):  \n",
    "\n",
    "    vocab_path = os.path.join(final_model_dir, 'vocabulary.json')\n",
    "    vocab_data = {\n",
    "        'word2idx': dataset.word2idx,\n",
    "        'idx2word': dataset.idx2word\n",
    "    }\n",
    "    with open(vocab_path, 'w') as f:\n",
    "        json.dump(vocab_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 4\n",
    "EMBEDDING_DIM = 10\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 15\n",
    "LEARNING_RATE = 0.02\n",
    "TRAIN_SPLIT = 0.8    \n",
    "VOCAB_SIZE = 8500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset...\n",
      "updating vocabulary\n",
      "got and updated vocab\n",
      "tokenizing corpus\n",
      "updating mapping - word2idx and idx2word\n",
      "generating cbow pairs\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "print(\"Creating dataset...\")\n",
    "dataset = Word2VecDataset(window_size=WINDOW_SIZE, vocabulary_size=VOCAB_SIZE)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cbow_pairs(dataset):\n",
    "        cbow_pairs = []\n",
    "        print(WINDOW_SIZE)\n",
    "        # loops over all the  in the tokenized corpus\n",
    "        for sentence in dataset.text:\n",
    "\n",
    "            for j in range(len(sentence)):\n",
    "                # Get context words within window\n",
    "                context_words = (sentence[max(0, j - WINDOW_SIZE):j] + \n",
    "                                sentence[j + 1:min(len(sentence), j + WINDOW_SIZE + 1)])\n",
    "                \n",
    "                if len(context_words) > 0:\n",
    "                    # Pad context if necessary\n",
    "                    while len(context_words) < WINDOW_SIZE * 2:\n",
    "                        context_words.append(dataset.pad_token)\n",
    "                    \n",
    "                    # Convert context words and target words into numerical indices using self.word2idx\n",
    "                    context_indices = [dataset.word_to_idx.get(w, dataset.word_to_idx[dataset.pad_token]) for w in context_words]\n",
    "                    target_idx = dataset.word_to_idx.get(sentence[j], dataset.word_to_idx[dataset.pad_token])\n",
    "                    \n",
    "                    # Store the (context, target) pairs in self.cbow_pairs\n",
    "                    cbow_pairs.append((context_indices, target_idx))\n",
    "\n",
    "        \n",
    "        return cbow_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[([7550, 3010], 5634), ([5634, 5395], 7550), ([7550, 643], 5395), ([5395, 1940], 643), ([643, 643], 1940)]\n",
      "87749\n"
     ]
    }
   ],
   "source": [
    "dataset.cbow_pairs = generate_cbow_pairs(dataset)\n",
    "\n",
    "print(dataset.cbow_pairs[:5])\n",
    "print(len(dataset.cbow_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and validation\n",
    "train_size = int(TRAIN_SPLIT * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary Size: 8500\n",
      "Total Pairs: 87749\n",
      "Training Pairs: 70199\n",
      "Validation Pairs: 17550\n",
      "\n",
      "Sample vocabulary items: ['##a', '##aachan', '##ab', '##abb', '##abbing']\n"
     ]
    }
   ],
   "source": [
    "# Print dataset information\n",
    "print(f\"\\nVocabulary Size: {len(dataset.vocabulary)}\")\n",
    "print(f\"Total Pairs: {len(dataset)}\")\n",
    "print(f\"Training Pairs: {len(train_dataset)}\")\n",
    "print(f\"Validation Pairs: {len(val_dataset)}\")\n",
    "print(\"\\nSample vocabulary items:\", list(dataset.vocabulary)[:5])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create model and set device\n",
    "print(\"\\nInitializing model...\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Word2VecModel(\n",
    "        vocab_size=len(dataset.vocabulary),\n",
    "        embedding_dim=EMBEDDING_DIM\n",
    "    ).to(device)\n",
    "    \n",
    "# Train model\n",
    "print(\"\\nStarting training...\")\n",
    "\n",
    "history = train_word2vec(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        checkpoint_dir='word2vec_checkpoints',\n",
    "        save_frequency=2,\n",
    "        device=device\n",
    ")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing model...\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 138/138 [00:08<00:00, 16.07it/s, batch_loss=3.8772, batch_acc=32.73%, avg_loss=5.3069, lr=0.020000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/15:\n",
      "Average Loss: 5.3069\n",
      "Training Accuracy: 27.79%\n",
      "\n",
      "Validating model...\n",
      "Validation Loss: 4.3826\n",
      "Validation Accuracy: 32.52%\n",
      "Cosine Similarity: 0.6795270885740008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15: 100%|██████████| 138/138 [00:08<00:00, 15.48it/s, batch_loss=4.1709, batch_acc=30.91%, avg_loss=4.1191, lr=0.018667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/15:\n",
      "Average Loss: 4.1191\n",
      "Training Accuracy: 33.97%\n",
      "\n",
      "Validating model...\n",
      "Validation Loss: 4.0934\n",
      "Validation Accuracy: 35.13%\n",
      "Cosine Similarity: 0.5674447655677796\n",
      "Checkpoint saved: word2vec_checkpoints\\word2vec_checkpoint_epoch_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15: 100%|██████████| 138/138 [00:10<00:00, 13.40it/s, batch_loss=3.3517, batch_acc=45.45%, avg_loss=3.8211, lr=0.017333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/15:\n",
      "Average Loss: 3.8211\n",
      "Training Accuracy: 36.41%\n",
      "\n",
      "Validating model...\n",
      "Validation Loss: 3.9656\n",
      "Validation Accuracy: 37.23%\n",
      "Cosine Similarity: 0.5604792901447841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15: 100%|██████████| 138/138 [00:10<00:00, 12.73it/s, batch_loss=3.6602, batch_acc=32.73%, avg_loss=3.6495, lr=0.016000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/15:\n",
      "Average Loss: 3.6495\n",
      "Training Accuracy: 38.04%\n",
      "\n",
      "Validating model...\n",
      "Validation Loss: 3.9351\n",
      "Validation Accuracy: 37.52%\n",
      "Cosine Similarity: 0.5456450496401105\n",
      "Checkpoint saved: word2vec_checkpoints\\word2vec_checkpoint_epoch_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15: 100%|██████████| 138/138 [00:09<00:00, 14.17it/s, batch_loss=3.1142, batch_acc=49.09%, avg_loss=3.5375, lr=0.014667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/15:\n",
      "Average Loss: 3.5375\n",
      "Training Accuracy: 39.03%\n",
      "\n",
      "Validating model...\n",
      "Validation Loss: 3.9364\n",
      "Validation Accuracy: 37.40%\n",
      "Cosine Similarity: 0.5323825240135193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15: 100%|██████████| 138/138 [00:09<00:00, 13.85it/s, batch_loss=2.7780, batch_acc=50.91%, avg_loss=3.4597, lr=0.013333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/15:\n",
      "Average Loss: 3.4597\n",
      "Training Accuracy: 39.60%\n",
      "\n",
      "Validating model...\n",
      "Validation Loss: 3.9553\n",
      "Validation Accuracy: 37.28%\n",
      "Cosine Similarity: 0.5286958175046104\n",
      "Checkpoint saved: word2vec_checkpoints\\word2vec_checkpoint_epoch_6.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15: 100%|██████████| 138/138 [00:10<00:00, 13.77it/s, batch_loss=3.6717, batch_acc=41.82%, avg_loss=3.4091, lr=0.012000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/15:\n",
      "Average Loss: 3.4091\n",
      "Training Accuracy: 40.03%\n",
      "\n",
      "Validating model...\n",
      "Validation Loss: 3.9692\n",
      "Validation Accuracy: 37.45%\n",
      "Cosine Similarity: 0.5371948787144252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15: 100%|██████████| 138/138 [00:09<00:00, 14.12it/s, batch_loss=4.0975, batch_acc=36.36%, avg_loss=3.3693, lr=0.010667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/15:\n",
      "Average Loss: 3.3693\n",
      "Training Accuracy: 40.35%\n",
      "\n",
      "Validating model...\n",
      "Validation Loss: 3.9861\n",
      "Validation Accuracy: 37.40%\n",
      "Cosine Similarity: 0.5192864239215851\n",
      "Checkpoint saved: word2vec_checkpoints\\word2vec_checkpoint_epoch_8.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15: 100%|██████████| 138/138 [00:09<00:00, 13.92it/s, batch_loss=3.4557, batch_acc=40.00%, avg_loss=3.3304, lr=0.009333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/15:\n",
      "Average Loss: 3.3304\n",
      "Training Accuracy: 40.57%\n",
      "\n",
      "Validating model...\n",
      "Validation Loss: 4.0055\n",
      "Validation Accuracy: 37.53%\n",
      "Cosine Similarity: 0.521880818264825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15: 100%|██████████| 138/138 [00:09<00:00, 14.11it/s, batch_loss=3.6378, batch_acc=32.73%, avg_loss=3.3040, lr=0.008000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/15:\n",
      "Average Loss: 3.3040\n",
      "Training Accuracy: 40.80%\n",
      "\n",
      "Validating model...\n",
      "Validation Loss: 4.0197\n",
      "Validation Accuracy: 37.38%\n",
      "Cosine Similarity: 0.5266713091305324\n",
      "Checkpoint saved: word2vec_checkpoints\\word2vec_checkpoint_epoch_10.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15: 100%|██████████| 138/138 [00:10<00:00, 13.35it/s, batch_loss=2.9832, batch_acc=45.45%, avg_loss=3.2752, lr=0.006667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/15:\n",
      "Average Loss: 3.2752\n",
      "Training Accuracy: 40.99%\n",
      "\n",
      "Validating model...\n",
      "Validation Loss: 4.0328\n",
      "Validation Accuracy: 37.50%\n",
      "Cosine Similarity: 0.526365966456277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15: 100%|██████████| 138/138 [00:09<00:00, 14.24it/s, batch_loss=3.3360, batch_acc=38.18%, avg_loss=3.2571, lr=0.005333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/15:\n",
      "Average Loss: 3.2571\n",
      "Training Accuracy: 41.10%\n",
      "\n",
      "Validating model...\n",
      "Validation Loss: 4.0440\n",
      "Validation Accuracy: 37.50%\n",
      "Cosine Similarity: 0.5253479821341378\n",
      "Checkpoint saved: word2vec_checkpoints\\word2vec_checkpoint_epoch_12.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15: 100%|██████████| 138/138 [00:10<00:00, 13.33it/s, batch_loss=3.3743, batch_acc=41.82%, avg_loss=3.2400, lr=0.004000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/15:\n",
      "Average Loss: 3.2400\n",
      "Training Accuracy: 41.30%\n",
      "\n",
      "Validating model...\n",
      "Validation Loss: 4.0508\n",
      "Validation Accuracy: 37.29%\n",
      "Cosine Similarity: 0.5185267610209329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15: 100%|██████████| 138/138 [00:11<00:00, 12.49it/s, batch_loss=2.4642, batch_acc=52.73%, avg_loss=3.2196, lr=0.002667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/15:\n",
      "Average Loss: 3.2196\n",
      "Training Accuracy: 41.37%\n",
      "\n",
      "Validating model...\n",
      "Validation Loss: 4.0582\n",
      "Validation Accuracy: 37.36%\n",
      "Cosine Similarity: 0.525026034457343\n",
      "Checkpoint saved: word2vec_checkpoints\\word2vec_checkpoint_epoch_14.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15: 100%|██████████| 138/138 [00:10<00:00, 12.88it/s, batch_loss=2.9287, batch_acc=49.09%, avg_loss=3.2100, lr=0.001333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/15:\n",
      "Average Loss: 3.2100\n",
      "Training Accuracy: 41.52%\n",
      "\n",
      "Validating model...\n",
      "Validation Loss: 4.0612\n",
      "Validation Accuracy: 37.36%\n",
      "Cosine Similarity: 0.5255386199269976\n",
      "Final model saved: word2vec_checkpoints\\word2vec_final_model.pt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create model and set device\n",
    "print(\"\\nInitializing model...\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Word2VecModel(\n",
    "        vocab_size=len(dataset.vocabulary),\n",
    "        embedding_dim=EMBEDDING_DIM\n",
    "    ).to(device)\n",
    "    \n",
    "# Train model\n",
    "print(\"\\nStarting training...\")\n",
    "\n",
    "history = train_word2vec(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        checkpoint_dir='word2vec_checkpoints',\n",
    "        save_frequency=2,\n",
    "        device=device\n",
    ")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate model and get accuracy\n",
    "print(\"\\nValidating model...\")\n",
    "val_loss, accuracy = validate_model(model, val_loader, device)\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"Validation Accuracy: {accuracy:.2f}%\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plot_training_history(history)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print validation pairs and predictions\n",
    "print(\"\\nValidation Pairs vs Predictions:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "evaluate_model(model, val_loader, device, dataset, BATCH_SIZE)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model and vocabulary\n",
    "print(\"\\nSaving final model and vocabulary...\")\n",
    "\n",
    "final_model_dir = 'final_model'\n",
    "os.makedirs(final_model_dir, exist_ok=True)\n",
    "    \n",
    "# Save vocabulary\n",
    "save_vocabulary(final_model_dir, dataset)\n",
    "    \n",
    "# Save final model state\n",
    "save_model(final_model_dir, model, val_loss, accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
