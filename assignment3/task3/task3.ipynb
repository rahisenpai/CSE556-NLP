{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "GRbDexIOoYuY"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /home/ritika22408/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import pickle\n",
        "import argparse\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration, ViTModel, ViTFeatureExtractor, logging\n",
        "from transformers.modeling_outputs import BaseModelOutput, Seq2SeqLMOutput\n",
        "\n",
        "# For evaluation metrics\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from rouge_score import rouge_scorer\n",
        "from bert_score import score as bert_score\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# random_seed = 42\n",
        "# torch.manual_seed(random_seed)\n",
        "# np.random.seed(random_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "eBBJCQpxou3d"
      },
      "outputs": [],
      "source": [
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "# Silence transformers warnings\n",
        "logging.set_verbosity_error()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "1Tv83vzZoygn"
      },
      "outputs": [],
      "source": [
        "class MultimodalSarcasmDataset(Dataset):\n",
        "    def __init__(self, df_path, desc_pickle, obj_pickle, image_dir, tokenizer, max_length=256, transform=None):\n",
        "        \"\"\"\n",
        "        df_path: path to TSV file with columns [pid, text, explanation, sarcasm_target]\n",
        "        desc_pickle: pickle file containing image descriptions (dictionary: pid -> description string)\n",
        "        obj_pickle: pickle file containing detected objects (dictionary: pid -> object string or list)\n",
        "        image_dir: directory with images named by pid (e.g. pid.jpg or png)\n",
        "        tokenizer: BART tokenizer\n",
        "        max_length: maximum token length for input sequence\n",
        "        transform: torchvision transforms for image preprocessing\n",
        "        \"\"\"\n",
        "        self.df = pd.read_csv(df_path, sep=\"\\t\")\n",
        "        with open(desc_pickle, \"rb\") as f:\n",
        "            self.desc_dict = pickle.load(f)\n",
        "        with open(obj_pickle, \"rb\") as f:\n",
        "            self.obj_dict = pickle.load(f)\n",
        "        self.image_dir = image_dir\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        # if no transform provided, define one for ViT (assumes 224x224 images)\n",
        "        if transform is None:\n",
        "            self.transform = transforms.Compose([\n",
        "                transforms.Resize((224, 224)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "            ])\n",
        "        else:\n",
        "            self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the sample row\n",
        "        row = self.df.iloc[idx]\n",
        "        pid = str(row[\"pid\"])\n",
        "        text = str(row[\"text\"])\n",
        "        explanation = str(row[\"explanation\"])\n",
        "        sarcasm_target = str(row[\"target_of_sarcasm\"])\n",
        "        # Get image description and detected objects from pickles\n",
        "        image_desc = self.desc_dict.get(pid, \"\")\n",
        "        detected_obj = self.obj_dict.get(pid, \"\")\n",
        "        # Ensure all components are strings and concatenate them\n",
        "        input_text = sarcasm_target + \" \" + text + \" \" + str(image_desc) + \" \" + str(detected_obj)\n",
        "\n",
        "        # Load image (assuming image file named <pid>.jpg; adjust extension if needed)\n",
        "        image_path = os.path.join(self.image_dir, pid + \".jpg\")\n",
        "        if not os.path.exists(image_path):\n",
        "            image_path = os.path.join(self.image_dir, pid + \".png\")\n",
        "        try:\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "        except Exception as e:\n",
        "            raise FileNotFoundError(f\"Image for pid {pid} not found: {e}\")\n",
        "        image = self.transform(image)\n",
        "\n",
        "        return {\n",
        "            \"input_text\": input_text,\n",
        "            \"target_text\": explanation,\n",
        "            \"image\": image\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "vMcP_5zroyY2"
      },
      "outputs": [],
      "source": [
        "# ==============================\n",
        "# Custom Collate Function\n",
        "# ==============================\n",
        "def collate_fn(batch, tokenizer, max_length=256, target_max_length=64):\n",
        "    input_texts = [item[\"input_text\"] for item in batch]\n",
        "    target_texts = [item[\"target_text\"] for item in batch]\n",
        "    images = torch.stack([item[\"image\"] for item in batch])\n",
        "\n",
        "    # Tokenize inputs and targets\n",
        "    inputs = tokenizer(input_texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
        "    targets = tokenizer(target_texts, padding=True, truncation=True, max_length=target_max_length, return_tensors=\"pt\")\n",
        "    # Replace pad tokens in targets with -100 for loss computation\n",
        "    targets_input_ids = targets.input_ids.masked_fill(targets.input_ids == tokenizer.pad_token_id, -100)\n",
        "\n",
        "    batch_dict = {\n",
        "        \"input_ids\": inputs.input_ids,\n",
        "        \"attention_mask\": inputs.attention_mask,\n",
        "        \"decoder_input_ids\": targets.input_ids,  # for teacher forcing\n",
        "        \"labels\": targets_input_ids,\n",
        "        \"pixel_values\": images\n",
        "    }\n",
        "    return batch_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "0afEJo3qoyRD"
      },
      "outputs": [],
      "source": [
        "# ==============================\n",
        "# Model Definition\n",
        "# ==============================\n",
        "class MultimodalSarcasmExplanationModel(nn.Module):\n",
        "    def __init__(self, bart_model_name=\"facebook/bart-base\", vit_model_name=\"google/vit-base-patch16-224\"):\n",
        "        super(MultimodalSarcasmExplanationModel, self).__init__()\n",
        "        self.bart = BartForConditionalGeneration.from_pretrained(bart_model_name)\n",
        "        self.vit = ViTModel.from_pretrained(vit_model_name)\n",
        "        # The fusion layer projects the ViT output to BART's d_model if needed.\n",
        "        self.fusion_layer = nn.Linear(self.vit.config.hidden_size, self.bart.config.d_model)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, decoder_input_ids, pixel_values, labels=None):\n",
        "        # Encode the text input with BART encoder\n",
        "        encoder_outputs = self.bart.model.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        text_features = encoder_outputs.last_hidden_state  # shape: (batch, seq_len, d_model)\n",
        "\n",
        "        # Extract image features using ViT\n",
        "        vit_outputs = self.vit(pixel_values=pixel_values)\n",
        "        image_feature = vit_outputs.last_hidden_state[:, 0, :]  # global image representation\n",
        "        image_feature_proj = self.fusion_layer(image_feature)   # project to d_model\n",
        "\n",
        "        # Shared Fusion: add the projected image feature to each token embedding of text features.\n",
        "        fused_features = text_features + image_feature_proj.unsqueeze(1)\n",
        "\n",
        "        # Prepare custom encoder output to pass to the decoder\n",
        "        fused_encoder_outputs = BaseModelOutput(last_hidden_state=fused_features)\n",
        "\n",
        "        # Decode using BART decoder with the fused encoder outputs.\n",
        "        outputs = self.bart.model.decoder(\n",
        "            input_ids=decoder_input_ids,\n",
        "            encoder_hidden_states=fused_encoder_outputs.last_hidden_state,\n",
        "            encoder_attention_mask=attention_mask,\n",
        "            return_dict=True\n",
        "        )\n",
        "        sequence_output = outputs.last_hidden_state\n",
        "        # Compute logits using the shared language modeling head of BART\n",
        "        lm_logits = self.bart.lm_head(sequence_output) + self.bart.final_logits_bias\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "            loss = loss_fct(lm_logits.view(-1, self.bart.config.vocab_size), labels.view(-1))\n",
        "\n",
        "        return Seq2SeqLMOutput(loss=loss, logits=lm_logits, encoder_last_hidden_state=fused_encoder_outputs.last_hidden_state)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "w-3eMwJwoyF-"
      },
      "outputs": [],
      "source": [
        "# ==============================\n",
        "# Training Function\n",
        "# ==============================\n",
        "def train(model, dataloader, optimizer, tokenizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        # Move batch to device\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        decoder_input_ids = batch[\"decoder_input_ids\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        pixel_values = batch[\"pixel_values\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask,\n",
        "                        decoder_input_ids=decoder_input_ids, pixel_values=pixel_values, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    return avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "M6FHqaFIpBGF"
      },
      "outputs": [],
      "source": [
        "# ==============================\n",
        "# Validation & Evaluation Functions\n",
        "# ==============================\n",
        "def generate_explanations(model, dataloader, tokenizer, device, max_length=64):\n",
        "    model.eval()\n",
        "    generated_texts = []\n",
        "    ground_truths = []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            pixel_values = batch[\"pixel_values\"].to(device)\n",
        "            # Encode text and fuse image features\n",
        "            encoder_outputs = model.bart.model.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            text_features = encoder_outputs.last_hidden_state\n",
        "\n",
        "            vit_outputs = model.vit(pixel_values=pixel_values)\n",
        "            image_feature = vit_outputs.last_hidden_state[:, 0, :]\n",
        "            image_feature_proj = model.fusion_layer(image_feature)\n",
        "\n",
        "            fused_features = text_features + image_feature_proj.unsqueeze(1)\n",
        "            fused_encoder_outputs = BaseModelOutput(last_hidden_state=fused_features)\n",
        "\n",
        "            generated_ids = model.bart.generate(\n",
        "                encoder_outputs=fused_encoder_outputs,\n",
        "                attention_mask=attention_mask,\n",
        "                max_length=max_length,\n",
        "                num_beams=4,                # using beam search with a beam width of 4\n",
        "                repetition_penalty=2.0,     # increases penalty for repeated tokens\n",
        "                no_repeat_ngram_size=3,     # prevents any 3-gram from repeating\n",
        "                do_sample=True,             # enables sampling (stochastic generation)\n",
        "                top_k=50,                   # limits the next-token choices to the top 50 tokens\n",
        "                top_p=0.9,                  # alternatively, you could use nucleus sampling\n",
        "                temperature=1.2,            # a temperature above 1 can add diversity\n",
        "                early_stopping=True\n",
        "            )\n",
        "\n",
        "            decoded_preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "            generated_texts.extend(decoded_preds)\n",
        "\n",
        "            # Replace -100 with the pad token id before decoding ground truth labels\n",
        "            labels = batch[\"labels\"].to(device).clone()\n",
        "            labels[labels == -100] = tokenizer.pad_token_id\n",
        "            targets = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "            ground_truths.extend(targets)\n",
        "    return generated_texts, ground_truths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "UQ5MPAOgpFSI"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(preds, targets):\n",
        "    rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    rouge1_scores, rouge2_scores, rougeL_scores = [], [], []\n",
        "    bleu_scores, meteor_scores = [], []\n",
        "    for pred, target in zip(preds, targets):\n",
        "        scores = rouge.score(target, pred)\n",
        "        rouge1_scores.append(scores['rouge1'].fmeasure)\n",
        "        rouge2_scores.append(scores['rouge2'].fmeasure)\n",
        "        rougeL_scores.append(scores['rougeL'].fmeasure)\n",
        "\n",
        "        smoothing = SmoothingFunction().method1\n",
        "        bleu = sentence_bleu([target.split()], pred.split(), smoothing_function=smoothing)\n",
        "        bleu_scores.append(bleu)\n",
        "\n",
        "        # Split both prediction and target into tokens for meteor_score\n",
        "        meteor = meteor_score([target.split()], pred.split())\n",
        "        meteor_scores.append(meteor)\n",
        "\n",
        "    P, R, F1 = bert_score(preds, targets, lang=\"en\", verbose=False)\n",
        "\n",
        "    metrics = {\n",
        "        \"ROUGE-1\": np.mean(rouge1_scores),\n",
        "        \"ROUGE-2\": np.mean(rouge2_scores),\n",
        "        \"ROUGE-L\": np.mean(rougeL_scores),\n",
        "        \"BLEU\": np.mean(bleu_scores),\n",
        "        \"METEOR\": np.mean(meteor_scores),\n",
        "        \"BERTScore_F1\": F1.mean().item()\n",
        "    }\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "YzFOaXr5pFIg"
      },
      "outputs": [],
      "source": [
        "# ==============================\n",
        "# Inference Function\n",
        "# ==============================\n",
        "def inference(model, test_df_path, desc_pickle, obj_pickle, image_dir, tokenizer, device, output_file=\"sarcasm_explanations.txt\"):\n",
        "    test_dataset = MultimodalSarcasmDataset(test_df_path, desc_pickle, obj_pickle, image_dir, tokenizer)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False,\n",
        "                             collate_fn=lambda batch: collate_fn(batch, tokenizer))\n",
        "    generated_texts, _ = generate_explanations(model, test_loader, tokenizer, device)\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        for line in generated_texts:\n",
        "            f.write(line + \"\\n\")\n",
        "    print(f\"Generated sarcasm explanations saved to {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "cp_kgJC1pE4r"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda:0\n",
            "\n",
            "Epoch 1/5\n",
            "Training Loss: 3.8580\n",
            "Validation Loss: 0.4208\n",
            "\n",
            "Sample Generated Explanations (Validation):\n",
            "GT: the author is pissed at <user> for not getting network in malad.\n",
            "Pred: <<< < < <>>>>,>,>,>.>.>:>.>.>. level level level levels levels levels level levelslevels levels levels Levels levels levels highs highs highs peaks peaks peaks peak peaks peaks spikes peaks peaks peaked peaks peaks plateau peaks peaks heights peaks peaks Peak peaks peaks averages peaks peaks lows peaks\n",
            "----------------------------------------\n",
            "GT: nothing worst than waiting for an hour on the tarmac for a gate to come open in snowy, windy chicago.\n",
            "Pred: gategategate gate gate gate gates gates gates gate gate Gate gate gate fence fence fence barrier barrier barrier wall barrier barrier barriers barriers barriers walls barriers barriers obstacles obstacles obstacles barriers barriers blocks barriers barriers defences barriers barriers blockers barriers barriers blocking barriers barriers channels barriers barriers obstacle barriers barriers sections barriers barriers rails barriers barriers defenses barriers barriers\n",
            "----------------------------------------\n",
            "GT: nobody likes getting one hour of their life sucked away.\n",
            "Pred: springspringspring spring spring spring Spring spring springSpring spring springpringpringpringspringspringpringpring spring springspring springspringpringspring spring springs spring spring burst burst burst bursting burst burst bursts burst burst leaks leaks leaks spills spills spills leaked leaked leaked leaking leaked leaked spilled spilled spilled leaked leaked leak leaks leaks leaked leaked\n",
            "----------------------------------------\n",
            "\n",
            "Evaluation Metrics on Validation Set:\n",
            "ROUGE-1: 0.0249\n",
            "ROUGE-2: 0.0010\n",
            "ROUGE-L: 0.0234\n",
            "BLEU: 0.0019\n",
            "METEOR: 0.0235\n",
            "BERTScore_F1: 0.7646\n",
            "\n",
            "Epoch 2/5\n",
            "Training Loss: 0.2112\n",
            "Validation Loss: 0.0127\n",
            "\n",
            "Sample Generated Explanations (Validation):\n",
            "GT: the author is pissed at <user> for not getting network in malad.\n",
            "Pred: <<< < < <<<>>>>,>,>,>.>.>.>:>:>:%:%:':%:%:%::\\%:%:\":%:%:!:%:%::%:%: :%:%:::%:%::'%:%:%,%:%:^%:%: (~%:%:64%:%:*:%:\n",
            "----------------------------------------\n",
            "GT: nothing worst than waiting for an hour on the tarmac for a gate to come open in snowy, windy chicago.\n",
            "Pred: theretherethereThere there there there There there there here here here Here here hereHere here here this this thisThis this this This this thisthis this this these these thesethesethesetheseThese these these These these these THESE these theseThese thesethese these these such such such Such such suchsuchsuchsuchSuch\n",
            "----------------------------------------\n",
            "GT: nobody likes getting one hour of their life sucked away.\n",
            "Pred: thisthisthisThis this this This this this this these these thesethesethesetheseThese these these These these these those these these such such suchsuchsuchsuchSuch such such so so so So So SoSoSoSo So SosososoSOSOSO SO SO SOSOSOsosoososo\n",
            "----------------------------------------\n",
            "\n",
            "Evaluation Metrics on Validation Set:\n",
            "ROUGE-1: 0.0245\n",
            "ROUGE-2: 0.0002\n",
            "ROUGE-L: 0.0240\n",
            "BLEU: 0.0024\n",
            "METEOR: 0.0191\n",
            "BERTScore_F1: 0.7558\n",
            "\n",
            "Epoch 3/5\n",
            "Training Loss: 0.0709\n",
            "Validation Loss: 0.0080\n",
            "\n",
            "Sample Generated Explanations (Validation):\n",
            "GT: the author is pissed at <user> for not getting network in malad.\n",
            "Pred: thisthisthisThisthisthis this this this these these thesethesethesetheseThese these these those these these such such such so so so that that thatthatthatthatThatThatThat ThatThatThatthatthat that that which which whichwhichwhichwhichwhowhowho who who who whom who who whoever who who\n",
            "----------------------------------------\n",
            "GT: nothing worst than waiting for an hour on the tarmac for a gate to come open in snowy, windy chicago.\n",
            "Pred: thisthisthisThisthisthis this this this these these thesethesethesetheseThese these these These these these are are are aren aren aren weren weren weren wasn wasn wasn didn didn didn hadn hadn hadn hasn hasn hasn haven haven haven Haven Haven HavenhavenhavenhavenavenavenavenVENVENVENavenavenvenaven\n",
            "----------------------------------------\n",
            "GT: nobody likes getting one hour of their life sucked away.\n",
            "Pred: thisthisthisThis this this this these these thesethesethesetheseThese these these those those those these these such such suchsuchsuchsuchSuch such such so so so So So SoSoSoSo So SosososoSOSOSO SO SO SO very very very extremely extremely extremely incredibly extremely exceptionally extremely extremely\n",
            "----------------------------------------\n",
            "\n",
            "Evaluation Metrics on Validation Set:\n",
            "ROUGE-1: 0.0215\n",
            "ROUGE-2: 0.0000\n",
            "ROUGE-L: 0.0207\n",
            "BLEU: 0.0019\n",
            "METEOR: 0.0165\n",
            "BERTScore_F1: 0.7520\n",
            "\n",
            "Epoch 4/5\n",
            "Training Loss: 0.0366\n",
            "Validation Loss: 0.0016\n",
            "\n",
            "Sample Generated Explanations (Validation):\n",
            "GT: the author is pissed at <user> for not getting network in malad.\n",
            "Pred: <<< < < <><><>< < <>:>:>: : : :::: : :':':':\":':'::':':'':':(':':' (':':'':':' \"':':':\":':'\"':':'`:':'-':':''.:':' '\n",
            "----------------------------------------\n",
            "GT: nothing worst than waiting for an hour on the tarmac for a gate to come open in snowy, windy chicago.\n",
            "Pred: thisthisthisThisthisthis this this this This this this these these thesethesethesetheseThesethesethese These these these such such suchsuchsuchsuchSuch such such so so so therefore therefore therefore thus thus thus thereby thereby thereby simultaneously simultaneously simultaneously concurrently concurrently simultaneously simultaneously together together together along along along with with with\n",
            "----------------------------------------\n",
            "GT: nobody likes getting one hour of their life sucked away.\n",
            "Pred: springspringspringSpringspringspring spring spring spring springs springs springs sprung sprung sprung sprang sprang sprang sprung sprung burst burst burst bursting bursting bursting spilling spilling spilling leaking leaking leaking leaked leaked leaked leak leak leak leaks leaks leaks leaking leaking leakage leak leak leaked leaked spilled leaked leaked released leaked leaked leaking leaking disclosing disclosing disclosing releasing leaking\n",
            "----------------------------------------\n",
            "\n",
            "Evaluation Metrics on Validation Set:\n",
            "ROUGE-1: 0.0139\n",
            "ROUGE-2: 0.0002\n",
            "ROUGE-L: 0.0139\n",
            "BLEU: 0.0015\n",
            "METEOR: 0.0118\n",
            "BERTScore_F1: 0.7542\n",
            "\n",
            "Epoch 5/5\n",
            "Training Loss: 0.0204\n",
            "Validation Loss: 0.0014\n",
            "\n",
            "Sample Generated Explanations (Validation):\n",
            "GT: the author is pissed at <user> for not getting network in malad.\n",
            "Pred: <<< < < <>>>>,>,>,>.>.>.>:>:>::\\>:>:*:>:>: :>:>:':>:>:?:>:>: ):>:>: (~>:>:><>:>: \"<>:>:%:>:>:>[>:>:]:>:>:697>:>::(>:>:ilon\n",
            "----------------------------------------\n",
            "GT: nothing worst than waiting for an hour on the tarmac for a gate to come open in snowy, windy chicago.\n",
            "Pred: ititit it it itIt it it its its its their their theirtheirtheirtheiryouryouryourYourYourYour your your your Your Your Your your your own own own personal personal personal private private private Private Private PrivatePrivate Private PrivateprivateprivateprivatePrivatePrivatePrivateprivatePrivateprivateprivate Private Private private private\n",
            "----------------------------------------\n",
            "GT: nobody likes getting one hour of their life sucked away.\n",
            "Pred: ititit it it it It it itIt it it its its its their their theirtheirtheirtheiryouryouryourYourYourYouryouryour your your your own own own personal personal personal private private private Private Private Private private privateprivateprivateprivatePrivate private private privately private private confidential private private intimate intimate intimate\n",
            "----------------------------------------\n",
            "\n",
            "Evaluation Metrics on Validation Set:\n",
            "ROUGE-1: 0.0177\n",
            "ROUGE-2: 0.0002\n",
            "ROUGE-L: 0.0177\n",
            "BLEU: 0.0015\n",
            "METEOR: 0.0112\n",
            "BERTScore_F1: 0.7567\n"
          ]
        }
      ],
      "source": [
        "# ==============================\n",
        "# Main Function\n",
        "# ==============================\n",
        "def main(args):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
        "    # Create training and validation datasets\n",
        "    train_dataset = MultimodalSarcasmDataset(\n",
        "        df_path=os.path.join(args.data_dir, \"train_df.tsv\"),\n",
        "        desc_pickle=os.path.join(args.data_dir, \"D_train.pkl\"),\n",
        "        obj_pickle=os.path.join(args.data_dir, \"O_train.pkl\"),\n",
        "        image_dir=os.path.join(args.data_dir, \"images\"),\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "    val_dataset = MultimodalSarcasmDataset(\n",
        "        df_path=os.path.join(args.data_dir, \"val_df.tsv\"),\n",
        "        desc_pickle=os.path.join(args.data_dir, \"D_val.pkl\"),\n",
        "        obj_pickle=os.path.join(args.data_dir, \"O_val.pkl\"),\n",
        "        image_dir=os.path.join(args.data_dir, \"images\"),\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True,\n",
        "                              collate_fn=lambda batch: collate_fn(batch, tokenizer))\n",
        "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False,\n",
        "                            collate_fn=lambda batch: collate_fn(batch, tokenizer))\n",
        "\n",
        "    model = MultimodalSarcasmExplanationModel()\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=args.learning_rate, weight_decay=1e-4)\n",
        "\n",
        "    best_val_loss = float(\"inf\")\n",
        "    for epoch in range(args.epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{args.epochs}\")\n",
        "        train_loss = train(model, train_loader, optimizer, tokenizer, device)\n",
        "        print(f\"Training Loss: {train_loss:.4f}\")\n",
        "\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                input_ids = batch[\"input_ids\"].to(device)\n",
        "                attention_mask = batch[\"attention_mask\"].to(device)\n",
        "                decoder_input_ids = batch[\"decoder_input_ids\"].to(device)\n",
        "                labels = batch[\"labels\"].to(device)\n",
        "                pixel_values = batch[\"pixel_values\"].to(device)\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask,\n",
        "                                decoder_input_ids=decoder_input_ids, pixel_values=pixel_values, labels=labels)\n",
        "                total_val_loss += outputs.loss.item()\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "        print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        # Generate sample explanations for monitoring\n",
        "        gen_texts, gt_texts = generate_explanations(model, val_loader, tokenizer, device)\n",
        "        print(\"\\nSample Generated Explanations (Validation):\")\n",
        "        for i in range(min(3, len(gen_texts))):\n",
        "            print(f\"GT: {gt_texts[i]}\")\n",
        "            print(f\"Pred: {gen_texts[i]}\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "        metrics = compute_metrics(gen_texts, gt_texts)\n",
        "        print(\"\\nEvaluation Metrics on Validation Set:\")\n",
        "        for k, v in metrics.items():\n",
        "            print(f\"{k}: {v:.4f}\")\n",
        "\n",
        "        # checkpoint_path = os.path.join(args.checkpoint_dir, f\"model_epoch_{epoch+1}.pt\")\n",
        "        # os.makedirs(args.checkpoint_dir, exist_ok=True)\n",
        "        # torch.save(model.state_dict(), checkpoint_path)\n",
        "        # print(f\"Model checkpoint saved to {checkpoint_path}\")\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            # best_model_path = os.path.join(args.checkpoint_dir, \"best_model.pt\")\n",
        "            # torch.save(model.state_dict(), best_model_path)\n",
        "            # print(\"Best model updated.\")\n",
        "\n",
        "    if args.mode == \"test\":\n",
        "        test_df_path = os.path.join(args.data_dir, \"val_df.tsv\")\n",
        "        inference(model, test_df_path,\n",
        "                  desc_pickle=os.path.join(args.data_dir, \"D_val.pkl\"),\n",
        "                  obj_pickle=os.path.join(args.data_dir, \"O_val.pkl\"),\n",
        "                  image_dir=os.path.join(args.data_dir, \"images\"),\n",
        "                  tokenizer=tokenizer,\n",
        "                  device=device,\n",
        "                  output_file=args.output_file)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description=\"Multimodal Sarcasm Explanation (MuSE) Task\")\n",
        "    parser.add_argument(\"--data_dir\", type=str, default=\"MORE-PLUS-DATASET\", help=\"Path to the dataset folder\")\n",
        "    parser.add_argument(\"--checkpoint_dir\", type=str, default=\"checkpoints\", help=\"Directory to save model checkpoints\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=8, help=\"Batch size\")\n",
        "    parser.add_argument(\"--epochs\", type=int, default=5, help=\"Number of training epochs\")\n",
        "    parser.add_argument(\"--learning_rate\", type=float, default=5e-5, help=\"Learning rate\")\n",
        "    parser.add_argument(\"--mode\", type=str, choices=[\"train\", \"test\"], default=\"train\", help=\"Mode: train or test\")\n",
        "    parser.add_argument(\"--output_file\", type=str, default=\"sarcasm_explanations.txt\", help=\"Output file for test predictions\")\n",
        "\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    main(args)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
