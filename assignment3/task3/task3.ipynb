{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "GRbDexIOoYuY"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /home/ritika22408/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pickle\n",
        "import argparse\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration, ViTModel, ViTFeatureExtractor, logging\n",
        "from transformers.modeling_outputs import BaseModelOutput, Seq2SeqLMOutput\n",
        "\n",
        "# For evaluation metrics\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from rouge_score import rouge_scorer\n",
        "from bert_score import score as bert_score\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "random_seed = 408\n",
        "torch.manual_seed(random_seed)\n",
        "np.random.seed(random_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "eBBJCQpxou3d"
      },
      "outputs": [],
      "source": [
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "# Silence transformers warnings\n",
        "logging.set_verbosity_error()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1Tv83vzZoygn"
      },
      "outputs": [],
      "source": [
        "class MultimodalSarcasmDataset(Dataset):\n",
        "    def __init__(self, df_path, desc_pickle, obj_pickle, image_dir, tokenizer, max_length=256, transform=None):\n",
        "        \"\"\"\n",
        "        df_path: path to TSV file with columns [pid, text, explanation, sarcasm_target]\n",
        "        desc_pickle: pickle file containing image descriptions (dictionary: pid -> description string)\n",
        "        obj_pickle: pickle file containing detected objects (dictionary: pid -> object string or list)\n",
        "        image_dir: directory with images named by pid (e.g. pid.jpg or png)\n",
        "        tokenizer: BART tokenizer\n",
        "        max_length: maximum token length for input sequence\n",
        "        transform: torchvision transforms for image preprocessing\n",
        "        \"\"\"\n",
        "        self.df = pd.read_csv(df_path, sep=\"\\t\")\n",
        "        with open(desc_pickle, \"rb\") as f:\n",
        "            self.desc_dict = pickle.load(f)\n",
        "        with open(obj_pickle, \"rb\") as f:\n",
        "            self.obj_dict = pickle.load(f)\n",
        "        self.image_dir = image_dir\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        # if no transform provided, define one for ViT (assumes 224x224 images)\n",
        "        if transform is None:\n",
        "            self.transform = transforms.Compose([\n",
        "                transforms.Resize((224, 224)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "            ])\n",
        "        else:\n",
        "            self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the sample row\n",
        "        row = self.df.iloc[idx]\n",
        "        pid = str(row[\"pid\"])\n",
        "        text = str(row[\"text\"])\n",
        "        explanation = str(row[\"explanation\"])\n",
        "        sarcasm_target = str(row[\"target_of_sarcasm\"])\n",
        "        # Get image description and detected objects from pickles\n",
        "        image_desc = self.desc_dict.get(pid, \"\")\n",
        "        detected_obj = self.obj_dict.get(pid, \"\")\n",
        "        # Ensure all components are strings and concatenate them\n",
        "        input_text = sarcasm_target + \" \" + text + \" \" + str(image_desc) + \" \" + str(detected_obj)\n",
        "\n",
        "        # Load image (assuming image file named <pid>.jpg; adjust extension if needed)\n",
        "        image_path = os.path.join(self.image_dir, pid + \".jpg\")\n",
        "        if not os.path.exists(image_path):\n",
        "            image_path = os.path.join(self.image_dir, pid + \".png\")\n",
        "        try:\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "        except Exception as e:\n",
        "            raise FileNotFoundError(f\"Image for pid {pid} not found: {e}\")\n",
        "        image = self.transform(image)\n",
        "\n",
        "        return {\n",
        "            \"input_text\": input_text,\n",
        "            \"target_text\": explanation,\n",
        "            \"image\": image\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "vMcP_5zroyY2"
      },
      "outputs": [],
      "source": [
        "# ==============================\n",
        "# Custom Collate Function\n",
        "# ==============================\n",
        "def collate_fn(batch, tokenizer, max_length=256, target_max_length=64):\n",
        "    input_texts = [item[\"input_text\"] for item in batch]\n",
        "    target_texts = [item[\"target_text\"] for item in batch]\n",
        "    images = torch.stack([item[\"image\"] for item in batch])\n",
        "\n",
        "    # Tokenize inputs and targets\n",
        "    inputs = tokenizer(input_texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
        "    targets = tokenizer(target_texts, padding=True, truncation=True, max_length=target_max_length, return_tensors=\"pt\")\n",
        "    # Replace pad tokens in targets with -100 for loss computation\n",
        "    targets_input_ids = targets.input_ids.masked_fill(targets.input_ids == tokenizer.pad_token_id, -100)\n",
        "\n",
        "    batch_dict = {\n",
        "        \"input_ids\": inputs.input_ids,\n",
        "        \"attention_mask\": inputs.attention_mask,\n",
        "        \"decoder_input_ids\": targets.input_ids,  # for teacher forcing\n",
        "        \"labels\": targets_input_ids,\n",
        "        \"pixel_values\": images\n",
        "    }\n",
        "    return batch_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0afEJo3qoyRD"
      },
      "outputs": [],
      "source": [
        "# ==============================\n",
        "# Model Definition\n",
        "# ==============================\n",
        "class MultimodalSarcasmExplanationModel(nn.Module):\n",
        "    def __init__(self, bart_model_name=\"facebook/bart-base\", vit_model_name=\"google/vit-base-patch16-224\"):\n",
        "        super(MultimodalSarcasmExplanationModel, self).__init__()\n",
        "        self.bart = BartForConditionalGeneration.from_pretrained(bart_model_name)\n",
        "        self.vit = ViTModel.from_pretrained(vit_model_name)\n",
        "        # The fusion layer projects the ViT output to BART's d_model if needed.\n",
        "        self.fusion_layer = nn.Linear(self.vit.config.hidden_size, self.bart.config.d_model)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, decoder_input_ids, pixel_values, labels=None):\n",
        "        # Encode the text input with BART encoder\n",
        "        encoder_outputs = self.bart.model.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        text_features = encoder_outputs.last_hidden_state  # shape: (batch, seq_len, d_model)\n",
        "\n",
        "        # Extract image features using ViT\n",
        "        vit_outputs = self.vit(pixel_values=pixel_values)\n",
        "        image_feature = vit_outputs.last_hidden_state[:, 0, :]  # global image representation\n",
        "        image_feature_proj = self.fusion_layer(image_feature)   # project to d_model\n",
        "\n",
        "        # Shared Fusion: add the projected image feature to each token embedding of text features.\n",
        "        fused_features = text_features + image_feature_proj.unsqueeze(1)\n",
        "\n",
        "        # Prepare custom encoder output to pass to the decoder\n",
        "        fused_encoder_outputs = BaseModelOutput(last_hidden_state=fused_features)\n",
        "\n",
        "        # Decode using BART decoder with the fused encoder outputs.\n",
        "        outputs = self.bart.model.decoder(\n",
        "            input_ids=decoder_input_ids,\n",
        "            encoder_hidden_states=fused_encoder_outputs.last_hidden_state,\n",
        "            encoder_attention_mask=attention_mask,\n",
        "            return_dict=True\n",
        "        )\n",
        "        sequence_output = outputs.last_hidden_state\n",
        "        # Compute logits using the shared language modeling head of BART\n",
        "        lm_logits = self.bart.lm_head(sequence_output) + self.bart.final_logits_bias\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "            loss = loss_fct(lm_logits.view(-1, self.bart.config.vocab_size), labels.view(-1))\n",
        "\n",
        "        return Seq2SeqLMOutput(loss=loss, logits=lm_logits, encoder_last_hidden_state=fused_encoder_outputs.last_hidden_state)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "w-3eMwJwoyF-"
      },
      "outputs": [],
      "source": [
        "# ==============================\n",
        "# Training Function\n",
        "# ==============================\n",
        "def train(model, dataloader, optimizer, tokenizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        # Move batch to device\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        decoder_input_ids = batch[\"decoder_input_ids\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        pixel_values = batch[\"pixel_values\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask,\n",
        "                        decoder_input_ids=decoder_input_ids, pixel_values=pixel_values, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    return avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "M6FHqaFIpBGF"
      },
      "outputs": [],
      "source": [
        "# ==============================\n",
        "# Validation & Evaluation Functions\n",
        "# ==============================\n",
        "def generate_explanations(model, dataloader, tokenizer, device, max_length=64):\n",
        "    model.eval()\n",
        "    generated_texts = []\n",
        "    ground_truths = []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            pixel_values = batch[\"pixel_values\"].to(device)\n",
        "            # Encode text and fuse image features\n",
        "            encoder_outputs = model.bart.model.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            text_features = encoder_outputs.last_hidden_state\n",
        "\n",
        "            vit_outputs = model.vit(pixel_values=pixel_values)\n",
        "            image_feature = vit_outputs.last_hidden_state[:, 0, :]\n",
        "            image_feature_proj = model.fusion_layer(image_feature)\n",
        "\n",
        "            fused_features = text_features + image_feature_proj.unsqueeze(1)\n",
        "            fused_encoder_outputs = BaseModelOutput(last_hidden_state=fused_features)\n",
        "\n",
        "            generated_ids = model.bart.generate(\n",
        "                encoder_outputs=fused_encoder_outputs,\n",
        "                attention_mask=attention_mask,\n",
        "                max_length=max_length,\n",
        "                num_beams=4,                # using beam search with a beam width of 4\n",
        "                repetition_penalty=2.0,     # increases penalty for repeated tokens\n",
        "                no_repeat_ngram_size=3,     # prevents any 3-gram from repeating\n",
        "                do_sample=True,             # enables sampling (stochastic generation)\n",
        "                top_k=50,                   # limits the next-token choices to the top 50 tokens\n",
        "                top_p=0.9,                  # alternatively, you could use nucleus sampling\n",
        "                temperature=1.2,            # a temperature above 1 can add diversity\n",
        "                early_stopping=True\n",
        "            )\n",
        "\n",
        "            decoded_preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "            generated_texts.extend(decoded_preds)\n",
        "\n",
        "            # Replace -100 with the pad token id before decoding ground truth labels\n",
        "            labels = batch[\"labels\"].to(device).clone()\n",
        "            labels[labels == -100] = tokenizer.pad_token_id\n",
        "            targets = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "            ground_truths.extend(targets)\n",
        "    return generated_texts, ground_truths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "UQ5MPAOgpFSI"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(preds, targets):\n",
        "    rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    rouge1_scores, rouge2_scores, rougeL_scores = [], [], []\n",
        "    bleu_scores, meteor_scores = [], []\n",
        "    for pred, target in zip(preds, targets):\n",
        "        scores = rouge.score(target, pred)\n",
        "        rouge1_scores.append(scores['rouge1'].fmeasure)\n",
        "        rouge2_scores.append(scores['rouge2'].fmeasure)\n",
        "        rougeL_scores.append(scores['rougeL'].fmeasure)\n",
        "\n",
        "        smoothing = SmoothingFunction().method1\n",
        "        bleu = sentence_bleu([target.split()], pred.split(), smoothing_function=smoothing)\n",
        "        bleu_scores.append(bleu)\n",
        "\n",
        "        # Split both prediction and target into tokens for meteor_score\n",
        "        meteor = meteor_score([target.split()], pred.split())\n",
        "        meteor_scores.append(meteor)\n",
        "\n",
        "    P, R, F1 = bert_score(preds, targets, lang=\"en\", verbose=False)\n",
        "\n",
        "    metrics = {\n",
        "        \"ROUGE-1\": np.mean(rouge1_scores),\n",
        "        \"ROUGE-2\": np.mean(rouge2_scores),\n",
        "        \"ROUGE-L\": np.mean(rougeL_scores),\n",
        "        \"BLEU\": np.mean(bleu_scores),\n",
        "        \"METEOR\": np.mean(meteor_scores),\n",
        "        \"BERTScore_F1\": F1.mean().item()\n",
        "    }\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "YzFOaXr5pFIg"
      },
      "outputs": [],
      "source": [
        "# ==============================\n",
        "# Inference Function\n",
        "# ==============================\n",
        "def inference(model, test_df_path, desc_pickle, obj_pickle, image_dir, tokenizer, device, output_file=\"sarcasm_explanations.txt\"):\n",
        "    test_dataset = MultimodalSarcasmDataset(test_df_path, desc_pickle, obj_pickle, image_dir, tokenizer)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False,\n",
        "                             collate_fn=lambda batch: collate_fn(batch, tokenizer))\n",
        "    generated_texts, _ = generate_explanations(model, test_loader, tokenizer, device)\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        for line in generated_texts:\n",
        "            f.write(line + \"\\n\")\n",
        "    print(f\"Generated sarcasm explanations saved to {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cp_kgJC1pE4r"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda:0\n",
            "\n",
            "Epoch 1/5\n",
            "Training Loss: 5.8302\n",
            "Validation Loss: 5.1018\n",
            "\n",
            "Sample Generated Explanations (Validation):\n",
            "GT: the author is pissed at <user> for not getting network in malad.\n",
            "Pred: >aduserthe is network <itmal. the mal awesome bad for in\n",
            "----------------------------------------\n",
            "GT: nothing worst than waiting for an hour on the tarmac for a gate to come open in snowy, windy chicago.\n",
            "Pred: the a. on to in the for's, an of is snow at snowy airport\n",
            "----------------------------------------\n",
            "GT: nobody likes getting one hour of their life sucked away.\n",
            "Pred: the is. about the spring weather's\n",
            "----------------------------------------\n",
            "\n",
            "Evaluation Metrics on Validation Set:\n",
            "ROUGE-1: 0.2939\n",
            "ROUGE-2: 0.0213\n",
            "ROUGE-L: 0.2029\n",
            "BLEU: 0.0155\n",
            "METEOR: 0.1019\n",
            "BERTScore_F1: 0.8343\n",
            "\n",
            "Epoch 2/5\n",
            "Training Loss: 0.7069\n",
            "Validation Loss: 0.1663\n",
            "\n",
            "Sample Generated Explanations (Validation):\n",
            "GT: the author is pissed at <user> for not getting network in malad.\n",
            "Pred: thethethethisthisthis this this this these these thesethesethesetheseThesethesethesetheytheythey they they they them them them him him him he he he him him his his his her her her his his their their their his his your your your own own own personal personal personal private private personal personal\n",
            "----------------------------------------\n",
            "GT: nothing worst than waiting for an hour on the tarmac for a gate to come open in snowy, windy chicago.\n",
            "Pred: thethethethisthisthis this this this these these thesethesetheseThesethesethesethesetheytheytheyTheytheythey they they they them them them him him him he he he him him his his his her her her husband husband husband wife wife wife woman wife wife fiance wife wife bride wife wife widow wife\n",
            "----------------------------------------\n",
            "GT: nobody likes getting one hour of their life sucked away.\n",
            "Pred: springspringspringpringpringpringspringspring spring spring spring Spring spring spring winter winter winter cold cold cold colder colder colder warmer warmer warmer hotter warmer warmer cooler cooler cooler nicer nicer nicer nice nice nice lovely nice nice cute cute cute adorable adorable adorable cute cute little cute cute fun fun fun interesting interesting interesting fascinating interesting interesting\n",
            "----------------------------------------\n",
            "\n",
            "Evaluation Metrics on Validation Set:\n",
            "ROUGE-1: 0.0182\n",
            "ROUGE-2: 0.0003\n",
            "ROUGE-L: 0.0174\n",
            "BLEU: 0.0016\n",
            "METEOR: 0.0200\n",
            "BERTScore_F1: 0.7566\n",
            "\n",
            "Epoch 3/5\n",
            "Training Loss: 0.1245\n",
            "Validation Loss: 0.0139\n",
            "\n",
            "Sample Generated Explanations (Validation):\n",
            "GT: the author is pissed at <user> for not getting network in malad.\n",
            "Pred: UUU U U U u u uuuuauauau au au au o o o O O OOOOoOOOOOOOOoooooooooooooooooooooooooooooooooohoooooooooooooooooooooowwwwwwwww@@@ @ @\n",
            "----------------------------------------\n",
            "GT: nothing worst than waiting for an hour on the tarmac for a gate to come open in snowy, windy chicago.\n",
            "Pred: ititit it it it they they they them them them me me me myself myself myself themselves themselves themselves herself herself herself himself himself himself him him him he he he she she she her her her hair hair hairhairhairhairairairairAirAirAir Air Air Air air air air flights flights planes planes\n",
            "----------------------------------------\n",
            "GT: nobody likes getting one hour of their life sucked away.\n",
            "Pred: springspringspringpringspringspring spring spring spring summer summer summer season season season seasons seasons seasons months months monthsmonthsmonthsmonthsmonthmonthmonthMonthMonthMonth month month month semester semester semester university university university campus campus campus campuses campuses campuses universities universities universities colleges universities universities institutions universities universities Universities universities universities schools universities universities professors\n",
            "----------------------------------------\n",
            "\n",
            "Evaluation Metrics on Validation Set:\n",
            "ROUGE-1: 0.0206\n",
            "ROUGE-2: 0.0000\n",
            "ROUGE-L: 0.0195\n",
            "BLEU: 0.0019\n",
            "METEOR: 0.0164\n",
            "BERTScore_F1: 0.7565\n",
            "\n",
            "Epoch 4/5\n",
            "Training Loss: 0.0321\n",
            "Validation Loss: 0.0064\n",
            "\n",
            "Sample Generated Explanations (Validation):\n",
            "GT: the author is pissed at <user> for not getting network in malad.\n",
            "Pred: <<< < < <><><><>:>:>: : : ::::;;; ; ; ; . . .....: : : @ @ @@ @ @ at at atAt at at At at at AT AT ATATATATatatatatsatsatesates\n",
            "----------------------------------------\n",
            "GT: nothing worst than waiting for an hour on the tarmac for a gate to come open in snowy, windy chicago.\n",
            "Pred: openopenopenOpenopenopen open open open opened opened opened opening opening opening Opening opening opening closing closing closing shutting shutting shutting shut shut shut closed closed closed locked locked locked locking locking lockinglocking locking locking unlocking unlocking unlocking unlock unlock unlock Unlock unlock unlock unlocked unlocked unlocked unlocks unlock unlock unleash unleash unleash unle unle unlock unlock\n",
            "----------------------------------------\n",
            "GT: nobody likes getting one hour of their life sucked away.\n",
            "Pred: springspringspringpringspringspring spring spring spring summer summer summer season season season seasons seasons seasons months seasons seasons Seasons Seasons Seasons Season seasons seasons races races races race races races Races Races Races races races cars cars cars car cars cars automobiles automobiles automobiles cars carscars cars cars vehicles cars cars trucks cars cars drivers cars cars\n",
            "----------------------------------------\n",
            "\n",
            "Evaluation Metrics on Validation Set:\n",
            "ROUGE-1: 0.0204\n",
            "ROUGE-2: 0.0005\n",
            "ROUGE-L: 0.0202\n",
            "BLEU: 0.0022\n",
            "METEOR: 0.0182\n",
            "BERTScore_F1: 0.7565\n",
            "\n",
            "Epoch 5/5\n",
            "Training Loss: 0.0311\n",
            "Validation Loss: 0.0103\n",
            "\n",
            "Sample Generated Explanations (Validation):\n",
            "GT: the author is pissed at <user> for not getting network in malad.\n",
            "Pred: thisthisthisThisthisthis this this this these these thesethesethesetheseThese these these These these these THESE these these those those thosethosethosethoseThosethosethosethesethesethethetheTheTheThe TheTheThe the the the The The TheThe The The the theTheTheTHETheThethe\n",
            "----------------------------------------\n",
            "GT: nothing worst than waiting for an hour on the tarmac for a gate to come open in snowy, windy chicago.\n",
            "Pred: thethetheTheTheThe TheTheThe the the the THE The The TheThe The ThethetheTHETheThethethenotnotnotNotNotNotnotnotntntnttttontontontotototetotottototoTOTOTOOTOTODODODO\n",
            "----------------------------------------\n",
            "GT: nobody likes getting one hour of their life sucked away.\n",
            "Pred: springspringspring spring spring spring summer summer summer winter winter winter season season season seasons seasons seasons Seasons Seasons Seasons seasons seasons months seasons seasons summers seasons seasons days weekends weekends weekends weekend weekends weekends Sundays Sundays Sundays weekends weekends mornings weekends weekends Saturdays Sundays Sundays Mondays Sundays Sundays Sunday Sundays Sundays holidays weekends weekends nights weekends weekends sessions weekends\n",
            "----------------------------------------\n",
            "\n",
            "Evaluation Metrics on Validation Set:\n",
            "ROUGE-1: 0.0332\n",
            "ROUGE-2: 0.0004\n",
            "ROUGE-L: 0.0323\n",
            "BLEU: 0.0033\n",
            "METEOR: 0.0258\n",
            "BERTScore_F1: 0.7551\n"
          ]
        }
      ],
      "source": [
        "# ==============================\n",
        "# Main Function\n",
        "# ==============================\n",
        "def main(args):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
        "    # Create training and validation datasets\n",
        "    train_dataset = MultimodalSarcasmDataset(\n",
        "        df_path=os.path.join(args.data_dir, \"train_df.tsv\"),\n",
        "        desc_pickle=os.path.join(args.data_dir, \"D_train.pkl\"),\n",
        "        obj_pickle=os.path.join(args.data_dir, \"O_train.pkl\"),\n",
        "        image_dir=os.path.join(args.data_dir, \"images\"),\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "    val_dataset = MultimodalSarcasmDataset(\n",
        "        df_path=os.path.join(args.data_dir, \"val_df.tsv\"),\n",
        "        desc_pickle=os.path.join(args.data_dir, \"D_val.pkl\"),\n",
        "        obj_pickle=os.path.join(args.data_dir, \"O_val.pkl\"),\n",
        "        image_dir=os.path.join(args.data_dir, \"images\"),\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True,\n",
        "                              collate_fn=lambda batch: collate_fn(batch, tokenizer))\n",
        "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False,\n",
        "                            collate_fn=lambda batch: collate_fn(batch, tokenizer))\n",
        "\n",
        "    model = MultimodalSarcasmExplanationModel()\n",
        "    model.to(device)\n",
        "\n",
        "    if args.mode == \"test\":\n",
        "        if args.checkpoint_path is None:\n",
        "            raise ValueError(\"Checkpoint path must be provided in test mode using --checkpoint_path\")\n",
        "        print(f\"Loading model checkpoint from {args.checkpoint_path}\")\n",
        "        model.load_state_dict(torch.load(args.checkpoint_path, map_location=device))\n",
        "        # Run inference on the test data\n",
        "        test_df_path = os.path.join(args.data_dir, \"test.tsv\")\n",
        "        inference(model, test_df_path,\n",
        "                  desc_pickle=os.path.join(args.data_dir, \"D_test.pkl\"),\n",
        "                  obj_pickle=os.path.join(args.data_dir, \"O_test.pkl\"),\n",
        "                  image_dir=os.path.join(args.data_dir, \"images\"),\n",
        "                  tokenizer=tokenizer,\n",
        "                  device=device,\n",
        "                  output_file=args.output_file)\n",
        "        \n",
        "    else:\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=args.learning_rate)\n",
        "\n",
        "        best_val_loss = float(\"inf\")\n",
        "        for epoch in range(args.epochs):\n",
        "            print(f\"\\nEpoch {epoch+1}/{args.epochs}\")\n",
        "            train_loss = train(model, train_loader, optimizer, tokenizer, device)\n",
        "            print(f\"Training Loss: {train_loss:.4f}\")\n",
        "\n",
        "            model.eval()\n",
        "            total_val_loss = 0\n",
        "            with torch.no_grad():\n",
        "                for batch in val_loader:\n",
        "                    input_ids = batch[\"input_ids\"].to(device)\n",
        "                    attention_mask = batch[\"attention_mask\"].to(device)\n",
        "                    decoder_input_ids = batch[\"decoder_input_ids\"].to(device)\n",
        "                    labels = batch[\"labels\"].to(device)\n",
        "                    pixel_values = batch[\"pixel_values\"].to(device)\n",
        "                    outputs = model(input_ids=input_ids, attention_mask=attention_mask,\n",
        "                                    decoder_input_ids=decoder_input_ids, pixel_values=pixel_values, labels=labels)\n",
        "                    total_val_loss += outputs.loss.item()\n",
        "            avg_val_loss = total_val_loss / len(val_loader)\n",
        "            print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "            # Generate sample explanations for monitoring\n",
        "            gen_texts, gt_texts = generate_explanations(model, val_loader, tokenizer, device)\n",
        "            print(\"\\nSample Generated Explanations (Validation):\")\n",
        "            for i in range(min(3, len(gen_texts))):\n",
        "                print(f\"GT: {gt_texts[i]}\")\n",
        "                print(f\"Pred: {gen_texts[i]}\")\n",
        "                print(\"-\" * 40)\n",
        "\n",
        "            metrics = compute_metrics(gen_texts, gt_texts)\n",
        "            print(\"\\nEvaluation Metrics on Validation Set:\")\n",
        "            for k, v in metrics.items():\n",
        "                print(f\"{k}: {v:.4f}\")\n",
        "\n",
        "            checkpoint_path = os.path.join(args.checkpoint_dir, f\"model_epoch_{epoch+1}.pt\")\n",
        "            os.makedirs(args.checkpoint_dir, exist_ok=True)\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "            print(f\"Model checkpoint saved to {checkpoint_path}\")\n",
        "\n",
        "            if avg_val_loss < best_val_loss:\n",
        "                best_val_loss = avg_val_loss\n",
        "                best_model_path = os.path.join(args.checkpoint_dir, \"best_model.pt\")\n",
        "                torch.save(model.state_dict(), best_model_path)\n",
        "                print(\"Best model updated.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description=\"Multimodal Sarcasm Explanation (MuSE) Task\")\n",
        "    parser.add_argument(\"--data_dir\", type=str, default=\"MORE-PLUS-DATASET\", help=\"Path to the dataset folder\")\n",
        "    parser.add_argument(\"--checkpoint_dir\", type=str, default=\"checkpoints\", help=\"Directory to save model checkpoints\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=8, help=\"Batch size\")\n",
        "    parser.add_argument(\"--epochs\", type=int, default=5, help=\"Number of training epochs\")\n",
        "    parser.add_argument(\"--learning_rate\", type=float, default=5e-5, help=\"Learning rate\")\n",
        "    parser.add_argument(\"--mode\", type=str, choices=[\"train\", \"test\"], default=\"train\", help=\"Mode: train or test\")\n",
        "    parser.add_argument(\"--output_file\", type=str, default=\"sarcasm_explanations.txt\", help=\"Output file for test predictions\")\n",
        "    parser.add_argument(\"--checkpoint_path\", type=str, default=\"checkpoints/best_model.pt\", help=\"Path to the model checkpoint for testing\")\n",
        "\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    main(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model checkpoint from checkpoints/best_model.pt\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'checkpoints/best_model.pt'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[23], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--checkpoint_path\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoints/best_model.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPath to the model checkpoint for testing\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m args, unknown \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_known_args()\n\u001b[0;32m---> 14\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[22], line 36\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheckpoint path must be provided in test mode using --checkpoint_path\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading model checkpoint from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Run inference on the test data\u001b[39;00m\n\u001b[1;32m     38\u001b[0m test_df_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(args\u001b[38;5;241m.\u001b[39mdata_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:986\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    984\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 986\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    988\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    989\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    990\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    991\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:435\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 435\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:416\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 416\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'checkpoints/best_model.pt'"
          ]
        }
      ],
      "source": [
        "# testing\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description=\"Multimodal Sarcasm Explanation (MuSE) Task\")\n",
        "    parser.add_argument(\"--data_dir\", type=str, default=\"MORE-PLUS-DATASET\", help=\"Path to the dataset folder\")\n",
        "    parser.add_argument(\"--checkpoint_dir\", type=str, default=\"checkpoints\", help=\"Directory to save model checkpoints\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=8, help=\"Batch size\")\n",
        "    parser.add_argument(\"--epochs\", type=int, default=5, help=\"Number of training epochs\")\n",
        "    parser.add_argument(\"--learning_rate\", type=float, default=5e-5, help=\"Learning rate\")\n",
        "    parser.add_argument(\"--mode\", type=str, choices=[\"train\", \"test\"], default=\"test\", help=\"Mode: train or test\")\n",
        "    parser.add_argument(\"--output_file\", type=str, default=\"sarcasm_explanations.txt\", help=\"Output file for test predictions\")\n",
        "    parser.add_argument(\"--checkpoint_path\", type=str, default=\"checkpoints/best_model.pt\", help=\"Path to the model checkpoint for testing\")\n",
        "    \n",
        "    args, unknown = parser.parse_known_args()\n",
        "    main(args)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
