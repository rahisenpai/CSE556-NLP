{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9S74-lxkyRiY",
        "outputId": "a625bb78-a1b8-436d-c244-0bdfb64dced0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: bert-score in /usr/local/lib/python3.11/dist-packages (0.3.13)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.6.0+cu124)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.2.2)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.50.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert-score) (3.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert-score) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.30.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.5.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (3.2.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge_score bert-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRbDexIOoYuY",
        "outputId": "7468ff8e-3f0c-48a8-df50-45cf148a198a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import pickle\n",
        "import argparse\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration, ViTModel, ViTFeatureExtractor, logging\n",
        "from transformers.modeling_outputs import BaseModelOutput, Seq2SeqLMOutput\n",
        "\n",
        "# For evaluation metrics\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from rouge_score import rouge_scorer\n",
        "from bert_score import score as bert_score\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "eBBJCQpxou3d"
      },
      "outputs": [],
      "source": [
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "# Silence transformers warnings\n",
        "logging.set_verbosity_error()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1Tv83vzZoygn"
      },
      "outputs": [],
      "source": [
        "class MultimodalSarcasmDataset(Dataset):\n",
        "    def __init__(self, df_path, desc_pickle, obj_pickle, image_dir, tokenizer, max_length=256, transform=None):\n",
        "        \"\"\"\n",
        "        df_path: path to TSV file with columns [pid, text, explanation, sarcasm_target]\n",
        "        desc_pickle: pickle file containing image descriptions (dictionary: pid -> description string)\n",
        "        obj_pickle: pickle file containing detected objects (dictionary: pid -> object string or list)\n",
        "        image_dir: directory with images named by pid (e.g. pid.jpg or png)\n",
        "        tokenizer: BART tokenizer\n",
        "        max_length: maximum token length for input sequence\n",
        "        transform: torchvision transforms for image preprocessing\n",
        "        \"\"\"\n",
        "        self.df = pd.read_csv(df_path, sep=\"\\t\")\n",
        "        with open(desc_pickle, \"rb\") as f:\n",
        "            self.desc_dict = pickle.load(f)\n",
        "        with open(obj_pickle, \"rb\") as f:\n",
        "            self.obj_dict = pickle.load(f)\n",
        "        self.image_dir = image_dir\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        # if no transform provided, define one for ViT (assumes 224x224 images)\n",
        "        if transform is None:\n",
        "            self.transform = transforms.Compose([\n",
        "                transforms.Resize((224, 224)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "            ])\n",
        "        else:\n",
        "            self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the sample row\n",
        "        row = self.df.iloc[idx]\n",
        "        pid = str(row[\"pid\"])\n",
        "        text = str(row[\"text\"])\n",
        "        explanation = str(row[\"explanation\"])\n",
        "        sarcasm_target = str(row[\"target_of_sarcasm\"])\n",
        "        # Get image description and detected objects from pickles\n",
        "        image_desc = self.desc_dict.get(pid, \"\")\n",
        "        detected_obj = self.obj_dict.get(pid, \"\")\n",
        "        # Ensure all components are strings and concatenate them\n",
        "        input_text = sarcasm_target + \" \" + text + \" \" + str(image_desc) + \" \" + str(detected_obj)\n",
        "\n",
        "        # Load image (assuming image file named <pid>.jpg; adjust extension if needed)\n",
        "        image_path = os.path.join(self.image_dir, pid + \".jpg\")\n",
        "        if not os.path.exists(image_path):\n",
        "            image_path = os.path.join(self.image_dir, pid + \".png\")\n",
        "        try:\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "        except Exception as e:\n",
        "            raise FileNotFoundError(f\"Image for pid {pid} not found: {e}\")\n",
        "        image = self.transform(image)\n",
        "\n",
        "        return {\n",
        "            \"input_text\": input_text,\n",
        "            \"target_text\": explanation,\n",
        "            \"image\": image\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "vMcP_5zroyY2"
      },
      "outputs": [],
      "source": [
        "# ==============================\n",
        "# Custom Collate Function\n",
        "# ==============================\n",
        "def collate_fn(batch, tokenizer, max_length=256, target_max_length=64):\n",
        "    input_texts = [item[\"input_text\"] for item in batch]\n",
        "    target_texts = [item[\"target_text\"] for item in batch]\n",
        "    images = torch.stack([item[\"image\"] for item in batch])\n",
        "\n",
        "    # Tokenize inputs and targets\n",
        "    inputs = tokenizer(input_texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
        "    targets = tokenizer(target_texts, padding=True, truncation=True, max_length=target_max_length, return_tensors=\"pt\")\n",
        "    # Replace pad tokens in targets with -100 for loss computation\n",
        "    targets_input_ids = targets.input_ids.masked_fill(targets.input_ids == tokenizer.pad_token_id, -100)\n",
        "\n",
        "    batch_dict = {\n",
        "        \"input_ids\": inputs.input_ids,\n",
        "        \"attention_mask\": inputs.attention_mask,\n",
        "        \"decoder_input_ids\": targets.input_ids,  # for teacher forcing\n",
        "        \"labels\": targets_input_ids,\n",
        "        \"pixel_values\": images\n",
        "    }\n",
        "    return batch_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0afEJo3qoyRD"
      },
      "outputs": [],
      "source": [
        "# ==============================\n",
        "# Model Definition\n",
        "# ==============================\n",
        "class MultimodalSarcasmExplanationModel(nn.Module):\n",
        "    def __init__(self, bart_model_name=\"facebook/bart-base\", vit_model_name=\"google/vit-base-patch16-224\"):\n",
        "        super(MultimodalSarcasmExplanationModel, self).__init__()\n",
        "        self.bart = BartForConditionalGeneration.from_pretrained(bart_model_name)\n",
        "        self.vit = ViTModel.from_pretrained(vit_model_name)\n",
        "        # The fusion layer projects the ViT output to BART's d_model if needed.\n",
        "        self.fusion_layer = nn.Linear(self.vit.config.hidden_size, self.bart.config.d_model)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, decoder_input_ids, pixel_values, labels=None):\n",
        "        # Encode the text input with BART encoder\n",
        "        encoder_outputs = self.bart.model.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        text_features = encoder_outputs.last_hidden_state  # shape: (batch, seq_len, d_model)\n",
        "\n",
        "        # Extract image features using ViT\n",
        "        vit_outputs = self.vit(pixel_values=pixel_values)\n",
        "        image_feature = vit_outputs.last_hidden_state[:, 0, :]  # global image representation\n",
        "        image_feature_proj = self.fusion_layer(image_feature)   # project to d_model\n",
        "\n",
        "        # Shared Fusion: add the projected image feature to each token embedding of text features.\n",
        "        fused_features = text_features + image_feature_proj.unsqueeze(1)\n",
        "\n",
        "        # Prepare custom encoder output to pass to the decoder\n",
        "        fused_encoder_outputs = BaseModelOutput(last_hidden_state=fused_features)\n",
        "\n",
        "        # Decode using BART decoder with the fused encoder outputs.\n",
        "        outputs = self.bart.model.decoder(\n",
        "            input_ids=decoder_input_ids,\n",
        "            encoder_hidden_states=fused_encoder_outputs.last_hidden_state,\n",
        "            encoder_attention_mask=attention_mask,\n",
        "            return_dict=True\n",
        "        )\n",
        "        sequence_output = outputs.last_hidden_state\n",
        "        # Compute logits using the shared language modeling head of BART\n",
        "        lm_logits = self.bart.lm_head(sequence_output) + self.bart.final_logits_bias\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "            loss = loss_fct(lm_logits.view(-1, self.bart.config.vocab_size), labels.view(-1))\n",
        "\n",
        "        return Seq2SeqLMOutput(loss=loss, logits=lm_logits, encoder_last_hidden_state=fused_encoder_outputs.last_hidden_state)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "w-3eMwJwoyF-"
      },
      "outputs": [],
      "source": [
        "# ==============================\n",
        "# Training Function\n",
        "# ==============================\n",
        "def train(model, dataloader, optimizer, tokenizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        # Move batch to device\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        decoder_input_ids = batch[\"decoder_input_ids\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        pixel_values = batch[\"pixel_values\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask,\n",
        "                        decoder_input_ids=decoder_input_ids, pixel_values=pixel_values, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    return avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "M6FHqaFIpBGF"
      },
      "outputs": [],
      "source": [
        "# ==============================\n",
        "# Validation & Evaluation Functions\n",
        "# ==============================\n",
        "def generate_explanations(model, dataloader, tokenizer, device, max_length=64):\n",
        "    model.eval()\n",
        "    generated_texts = []\n",
        "    ground_truths = []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            pixel_values = batch[\"pixel_values\"].to(device)\n",
        "            # Encode text and fuse image features\n",
        "            encoder_outputs = model.bart.model.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            text_features = encoder_outputs.last_hidden_state\n",
        "\n",
        "            vit_outputs = model.vit(pixel_values=pixel_values)\n",
        "            image_feature = vit_outputs.last_hidden_state[:, 0, :]\n",
        "            image_feature_proj = model.fusion_layer(image_feature)\n",
        "\n",
        "            fused_features = text_features + image_feature_proj.unsqueeze(1)\n",
        "            fused_encoder_outputs = BaseModelOutput(last_hidden_state=fused_features)\n",
        "\n",
        "            generated_ids = model.bart.generate(\n",
        "                encoder_outputs=fused_encoder_outputs,\n",
        "                attention_mask=attention_mask,\n",
        "                max_length=max_length,\n",
        "                num_beams=4,                # using beam search with a beam width of 4\n",
        "                repetition_penalty=2.0,     # increases penalty for repeated tokens\n",
        "                no_repeat_ngram_size=3,     # prevents any 3-gram from repeating\n",
        "                do_sample=True,             # enables sampling (stochastic generation)\n",
        "                top_k=50,                   # limits the next-token choices to the top 50 tokens\n",
        "                top_p=0.9,                  # alternatively, you could use nucleus sampling\n",
        "                temperature=1.2,            # a temperature above 1 can add diversity\n",
        "                early_stopping=True\n",
        "            )\n",
        "\n",
        "            decoded_preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "            generated_texts.extend(decoded_preds)\n",
        "\n",
        "            # Replace -100 with the pad token id before decoding ground truth labels\n",
        "            labels = batch[\"labels\"].to(device).clone()\n",
        "            labels[labels == -100] = tokenizer.pad_token_id\n",
        "            targets = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "            ground_truths.extend(targets)\n",
        "    return generated_texts, ground_truths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "UQ5MPAOgpFSI"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(preds, targets):\n",
        "    rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    rouge1_scores, rouge2_scores, rougeL_scores = [], [], []\n",
        "    bleu_scores, meteor_scores = [], []\n",
        "    for pred, target in zip(preds, targets):\n",
        "        scores = rouge.score(target, pred)\n",
        "        rouge1_scores.append(scores['rouge1'].fmeasure)\n",
        "        rouge2_scores.append(scores['rouge2'].fmeasure)\n",
        "        rougeL_scores.append(scores['rougeL'].fmeasure)\n",
        "\n",
        "        smoothing = SmoothingFunction().method1\n",
        "        bleu = sentence_bleu([target.split()], pred.split(), smoothing_function=smoothing)\n",
        "        bleu_scores.append(bleu)\n",
        "\n",
        "        # Split both prediction and target into tokens for meteor_score\n",
        "        meteor = meteor_score([target.split()], pred.split())\n",
        "        meteor_scores.append(meteor)\n",
        "\n",
        "    P, R, F1 = bert_score(preds, targets, lang=\"en\", verbose=False)\n",
        "\n",
        "    metrics = {\n",
        "        \"ROUGE-1\": np.mean(rouge1_scores),\n",
        "        \"ROUGE-2\": np.mean(rouge2_scores),\n",
        "        \"ROUGE-L\": np.mean(rougeL_scores),\n",
        "        \"BLEU\": np.mean(bleu_scores),\n",
        "        \"METEOR\": np.mean(meteor_scores),\n",
        "        \"BERTScore_F1\": F1.mean().item()\n",
        "    }\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "YzFOaXr5pFIg"
      },
      "outputs": [],
      "source": [
        "# ==============================\n",
        "# Inference Function\n",
        "# ==============================\n",
        "def inference(model, test_df_path, desc_pickle, obj_pickle, image_dir, tokenizer, device, output_file=\"sarcasm_explanations.txt\"):\n",
        "    test_dataset = MultimodalSarcasmDataset(test_df_path, desc_pickle, obj_pickle, image_dir, tokenizer)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False,\n",
        "                             collate_fn=lambda batch: collate_fn(batch, tokenizer))\n",
        "    generated_texts, _ = generate_explanations(model, test_loader, tokenizer, device)\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        for line in generated_texts:\n",
        "            f.write(line + \"\\n\")\n",
        "    print(f\"Generated sarcasm explanations saved to {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cp_kgJC1pE4r",
        "outputId": "c8da3ba7-6950-4da5-d4db-59961808fbfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Epoch 1/5\n",
            "Training Loss: 4.3756\n",
            "Validation Loss: 0.2051\n",
            "\n",
            "Sample Generated Explanations (Validation):\n",
            "GT: the author is pissed at <user> for not getting network in malad.\n",
            "Pred: thethethethisthisthisThisThisThis this this thisThisThis ThisThisThis is is is will will will be be be being being being doing doing doing do do do Do do do does does does has has has have have have need need need needs needs needs needed needed needed needing needing needing need need\n",
            "----------------------------------------\n",
            "GT: nothing worst than waiting for an hour on the tarmac for a gate to come open in snowy, windy chicago.\n",
            "Pred: thethethethisthisthisThisThisThis this this thisThisThis ThisThisThisTHISThisThisthisThisTheseTheseThesethesethesetheseThesethesethosethosethosethatthatthattheretheretherethatthattheythatthatThatthatthat...\".\n",
            "----------------------------------------\n",
            "GT: nobody likes getting one hour of their life sucked away.\n",
            "Pred: springspringspringSpringspringspring spring spring spring winter winter winter weather weather weather climate weather weather storm storm storm storms storms storms thunder thunder thunderThunder thunder thunder Thunder Thunder ThunderThunderThunderThunder Thunder Thunder thunder thunderstormsstormsstormsstormstormsstormsrainrainRainrainrainrain rain rain rain clouds clouds clouds cloud clouds clouds\n",
            "----------------------------------------\n",
            "\n",
            "Evaluation Metrics on Validation Set:\n",
            "ROUGE-1: 0.0198\n",
            "ROUGE-2: 0.0005\n",
            "ROUGE-L: 0.0194\n",
            "BLEU: 0.0025\n",
            "METEOR: 0.0121\n",
            "BERTScore_F1: 0.7514\n",
            "Model checkpoint saved to checkpoints/model_epoch_1.pt\n",
            "Best model updated.\n",
            "\n",
            "Epoch 2/5\n",
            "Training Loss: 0.2677\n",
            "Validation Loss: 6.9242\n",
            "\n",
            "Sample Generated Explanations (Validation):\n",
            "GT: the author is pissed at <user> for not getting network in malad.\n",
            "Pred:  bad. is not has, the's doesn in isn sad and to hates suicides't for just it> him don can up are since mal only going go haves no that didn like' pissed we this lead who had public today on with alone one such trolling at a his working get aren got about road\n",
            "----------------------------------------\n",
            "GT: nothing worst than waiting for an hour on the tarmac for a gate to come open in snowy, windy chicago.\n",
            "Pred:  the is., doesn has to can's on only't bad and for road this don not have since go him are working like it pissed today in a get at just hates roads he sad was had such isn- up lead by suicides with difficult hill average that got first' his public work going alones\n",
            "----------------------------------------\n",
            "GT: nobody likes getting one hour of their life sucked away.\n",
            "Pred: ., is to not the't doesn for and has in's lead bad don since its suicides on this only just can that isn a him go us are going there such have about first hates seem like up hill today' pissed weather r get at- was leads didn we hat work he storm with does\n",
            "----------------------------------------\n",
            "\n",
            "Evaluation Metrics on Validation Set:\n",
            "ROUGE-1: 0.1562\n",
            "ROUGE-2: 0.0021\n",
            "ROUGE-L: 0.0951\n",
            "BLEU: 0.0048\n",
            "METEOR: 0.1123\n",
            "BERTScore_F1: 0.8035\n",
            "Model checkpoint saved to checkpoints/model_epoch_2.pt\n",
            "\n",
            "Epoch 3/5\n",
            "Training Loss: 5.7890\n",
            "Validation Loss: 9.3769\n",
            "\n",
            "Sample Generated Explanations (Validation):\n",
            "GT: the author is pissed at <user> for not getting network in malad.\n",
            "Pred: itthe authoruser<> <this is isn's disappointednob't doesn' pissedtherethese the\n",
            "----------------------------------------\n",
            "GT: nothing worst than waiting for an hour on the tarmac for a gate to come open in snowy, windy chicago.\n",
            "Pred: theit author'stherethis't not doesn is hates just isn like alookingnojust it the\n",
            "----------------------------------------\n",
            "GT: nobody likes getting one hour of their life sucked away.\n",
            "Pred: the authoritthis is's isn doesn't hates sadtherethat tothesenot aeveryonejust the\n",
            "----------------------------------------\n",
            "\n",
            "Evaluation Metrics on Validation Set:\n",
            "ROUGE-1: 0.1425\n",
            "ROUGE-2: 0.0044\n",
            "ROUGE-L: 0.1172\n",
            "BLEU: 0.0091\n",
            "METEOR: 0.0374\n",
            "BERTScore_F1: 0.7999\n",
            "Model checkpoint saved to checkpoints/model_epoch_3.pt\n",
            "\n",
            "Epoch 4/5\n",
            "Training Loss: 4.8187\n",
            "Validation Loss: 7.9446\n",
            "\n",
            "Sample Generated Explanations (Validation):\n",
            "GT: the author is pissed at <user> for not getting network in malad.\n",
            "Pred: the<it is authornobthis pissed's hates <> doesn isnuserthese atsuchsgiven\n",
            "----------------------------------------\n",
            "GT: nothing worst than waiting for an hour on the tarmac for a gate to come open in snowy, windy chicago.\n",
            "Pred: thethe waiting isthethe hates waits athethe authorthe forthe thethe'sthethethe isnthethe waitthetheitthethe hours tothethethis queuesthe pissedthethetherethe waitedthethe\n",
            "----------------------------------------\n",
            "GT: nobody likes getting one hour of their life sucked away.\n",
            "Pred: the spring hatesspringit'sthe isn is authorthis doesnthetheThis'taccording, hatestheter a the tonot\n",
            "----------------------------------------\n",
            "\n",
            "Evaluation Metrics on Validation Set:\n",
            "ROUGE-1: 0.1435\n",
            "ROUGE-2: 0.0050\n",
            "ROUGE-L: 0.1141\n",
            "BLEU: 0.0101\n",
            "METEOR: 0.0466\n",
            "BERTScore_F1: 0.7919\n",
            "Model checkpoint saved to checkpoints/model_epoch_4.pt\n",
            "\n",
            "Epoch 5/5\n",
            "Training Loss: 4.4250\n",
            "Validation Loss: 4.5287\n",
            "\n",
            "Sample Generated Explanations (Validation):\n",
            "GT: the author is pissed at <user> for not getting network in malad.\n",
            "Pred: < author pisseduser < is> for is network in thead this malad.adad,ad aadadadmaladad adadad dadad'tadad badadad'sadad\n",
            "----------------------------------------\n",
            "GT: nothing worst than waiting for an hour on the tarmac for a gate to come open in snowy, windy chicago.\n",
            "Pred: the is waiting an to hour t on for t t long t,. t snow t in a outside snowy t day it wind has t the's and to winter get cold't weather come out. snow from snowy snow snowy. to snowy,y open there airport snowy frozen snowy snowy be storm snow snowy\n",
            "----------------------------------------\n",
            "GT: nobody likes getting one hour of their life sucked away.\n",
            "Pred:  spring weather spring spring spring. spring spring, spring spring't spring spring\n",
            "----------------------------------------\n",
            "\n",
            "Evaluation Metrics on Validation Set:\n",
            "ROUGE-1: 0.3230\n",
            "ROUGE-2: 0.0386\n",
            "ROUGE-L: 0.2308\n",
            "BLEU: 0.0180\n",
            "METEOR: 0.1706\n",
            "BERTScore_F1: 0.8301\n",
            "Model checkpoint saved to checkpoints/model_epoch_5.pt\n"
          ]
        }
      ],
      "source": [
        "# ==============================\n",
        "# Main Function\n",
        "# ==============================\n",
        "def main(args):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
        "    # Create training and validation datasets\n",
        "    train_dataset = MultimodalSarcasmDataset(\n",
        "        df_path=os.path.join(args.data_dir, \"train_df.tsv\"),\n",
        "        desc_pickle=os.path.join(args.data_dir, \"D_train.pkl\"),\n",
        "        obj_pickle=os.path.join(args.data_dir, \"O_train.pkl\"),\n",
        "        image_dir=os.path.join(args.data_dir, \"images\"),\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "    val_dataset = MultimodalSarcasmDataset(\n",
        "        df_path=os.path.join(args.data_dir, \"val_df.tsv\"),\n",
        "        desc_pickle=os.path.join(args.data_dir, \"D_val.pkl\"),\n",
        "        obj_pickle=os.path.join(args.data_dir, \"O_val.pkl\"),\n",
        "        image_dir=os.path.join(args.data_dir, \"images\"),\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True,\n",
        "                              collate_fn=lambda batch: collate_fn(batch, tokenizer))\n",
        "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False,\n",
        "                            collate_fn=lambda batch: collate_fn(batch, tokenizer))\n",
        "\n",
        "    model = MultimodalSarcasmExplanationModel()\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=args.learning_rate)\n",
        "\n",
        "    best_val_loss = float(\"inf\")\n",
        "    for epoch in range(args.epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{args.epochs}\")\n",
        "        train_loss = train(model, train_loader, optimizer, tokenizer, device)\n",
        "        print(f\"Training Loss: {train_loss:.4f}\")\n",
        "\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                input_ids = batch[\"input_ids\"].to(device)\n",
        "                attention_mask = batch[\"attention_mask\"].to(device)\n",
        "                decoder_input_ids = batch[\"decoder_input_ids\"].to(device)\n",
        "                labels = batch[\"labels\"].to(device)\n",
        "                pixel_values = batch[\"pixel_values\"].to(device)\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask,\n",
        "                                decoder_input_ids=decoder_input_ids, pixel_values=pixel_values, labels=labels)\n",
        "                total_val_loss += outputs.loss.item()\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "        print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        # Generate sample explanations for monitoring\n",
        "        gen_texts, gt_texts = generate_explanations(model, val_loader, tokenizer, device)\n",
        "        print(\"\\nSample Generated Explanations (Validation):\")\n",
        "        for i in range(min(3, len(gen_texts))):\n",
        "            print(f\"GT: {gt_texts[i]}\")\n",
        "            print(f\"Pred: {gen_texts[i]}\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "        metrics = compute_metrics(gen_texts, gt_texts)\n",
        "        print(\"\\nEvaluation Metrics on Validation Set:\")\n",
        "        for k, v in metrics.items():\n",
        "            print(f\"{k}: {v:.4f}\")\n",
        "\n",
        "        checkpoint_path = os.path.join(args.checkpoint_dir, f\"model_epoch_{epoch+1}.pt\")\n",
        "        os.makedirs(args.checkpoint_dir, exist_ok=True)\n",
        "        torch.save(model.state_dict(), checkpoint_path)\n",
        "        print(f\"Model checkpoint saved to {checkpoint_path}\")\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            best_model_path = os.path.join(args.checkpoint_dir, \"best_model.pt\")\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            print(\"Best model updated.\")\n",
        "\n",
        "    if args.mode == \"test\":\n",
        "        test_df_path = os.path.join(args.data_dir, \"test.tsv\")\n",
        "        inference(model, test_df_path,\n",
        "                  desc_pickle=os.path.join(args.data_dir, \"D_test.pkl\"),\n",
        "                  obj_pickle=os.path.join(args.data_dir, \"O_test.pkl\"),\n",
        "                  image_dir=os.path.join(args.data_dir, \"images\"),\n",
        "                  tokenizer=tokenizer,\n",
        "                  device=device,\n",
        "                  output_file=args.output_file)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description=\"Multimodal Sarcasm Explanation (MuSE) Task\")\n",
        "    parser.add_argument(\"--data_dir\", type=str, default=\"/content/drive/MyDrive/MORE-PLUS-DATASET\", help=\"Path to the dataset folder\")\n",
        "    parser.add_argument(\"--checkpoint_dir\", type=str, default=\"checkpoints\", help=\"Directory to save model checkpoints\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=8, help=\"Batch size\")\n",
        "    parser.add_argument(\"--epochs\", type=int, default=5, help=\"Number of training epochs\")\n",
        "    parser.add_argument(\"--learning_rate\", type=float, default=5e-5, help=\"Learning rate\")\n",
        "    parser.add_argument(\"--mode\", type=str, choices=[\"train\", \"test\"], default=\"train\", help=\"Mode: train or test\")\n",
        "    parser.add_argument(\"--output_file\", type=str, default=\"sarcasm_explanations.txt\", help=\"Output file for test predictions\")\n",
        "\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    main(args)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
