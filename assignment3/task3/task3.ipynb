{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "GRbDexIOoYuY"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /home/gpu1/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /home/gpu1/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /home/gpu1/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pickle\n",
        "import argparse\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration, ViTModel, ViTFeatureExtractor, logging\n",
        "from transformers.modeling_outputs import BaseModelOutput, Seq2SeqLMOutput\n",
        "\n",
        "# For evaluation metrics\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from rouge_score import rouge_scorer\n",
        "from bert_score import score as bert_score\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "import re\n",
        "\n",
        "random_seed = 408\n",
        "torch.manual_seed(random_seed)\n",
        "np.random.seed(random_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eBBJCQpxou3d"
      },
      "outputs": [],
      "source": [
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "# Silence transformers warnings\n",
        "logging.set_verbosity_error()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1Tv83vzZoygn"
      },
      "outputs": [],
      "source": [
        "class MultimodalSarcasmDataset(Dataset):\n",
        "    def __init__(self, df_path, desc_pickle, obj_pickle, image_dir, tokenizer, max_length=256, transform=None):\n",
        "        \"\"\"\n",
        "        df_path: path to TSV file with columns [pid, text, explanation, sarcasm_target]\n",
        "        desc_pickle: pickle file containing image descriptions (dictionary: pid -> description string)\n",
        "        obj_pickle: pickle file containing detected objects (dictionary: pid -> object string or list)\n",
        "        image_dir: directory with images named by pid (e.g. pid.jpg or png)\n",
        "        tokenizer: BART tokenizer\n",
        "        max_length: maximum token length for input sequence\n",
        "        transform: torchvision transforms for image preprocessing\n",
        "        \"\"\"\n",
        "        self.df = pd.read_csv(df_path, sep=\"\\t\")\n",
        "        with open(desc_pickle, \"rb\") as f:\n",
        "            self.desc_dict = pickle.load(f)\n",
        "        with open(obj_pickle, \"rb\") as f:\n",
        "            self.obj_dict = pickle.load(f)\n",
        "        self.image_dir = image_dir\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        # if no transform provided, define one for ViT (assumes 224x224 images)\n",
        "        if transform is None:\n",
        "            self.transform = transforms.Compose([\n",
        "                transforms.Resize((224, 224)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "            ])\n",
        "        else:\n",
        "            self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the sample row\n",
        "        row = self.df.iloc[idx]\n",
        "        pid = str(row[\"pid\"])\n",
        "        text = str(row[\"text\"])\n",
        "        explanation = str(row[\"explanation\"])\n",
        "        sarcasm_target = str(row[\"target_of_sarcasm\"])\n",
        "        # Get image description and detected objects from pickles\n",
        "        image_desc = self.desc_dict.get(pid, \"\")\n",
        "        detected_obj = self.obj_dict.get(pid, \"\")\n",
        "        # Ensure all components are strings and concatenate them\n",
        "        input_text = sarcasm_target + \" \" + text + \" \" + str(image_desc) + \" \" + str(detected_obj)\n",
        "\n",
        "        # Load image (assuming image file named <pid>.jpg; adjust extension if needed)\n",
        "        image_path = os.path.join(self.image_dir, pid + \".jpg\")\n",
        "        if not os.path.exists(image_path):\n",
        "            image_path = os.path.join(self.image_dir, pid + \".png\")\n",
        "        try:\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "        except Exception as e:\n",
        "            raise FileNotFoundError(f\"Image for pid {pid} not found: {e}\")\n",
        "        image = self.transform(image)\n",
        "\n",
        "        return {\n",
        "            \"input_text\": input_text,\n",
        "            \"target_text\": explanation,\n",
        "            \"image\": image\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vMcP_5zroyY2"
      },
      "outputs": [],
      "source": [
        "# ==============================\n",
        "# Custom Collate Function\n",
        "# ==============================\n",
        "def collate_fn(batch, tokenizer, max_length=256, target_max_length=64):\n",
        "    input_texts = [item[\"input_text\"] for item in batch]\n",
        "    target_texts = [item[\"target_text\"] for item in batch]\n",
        "    images = torch.stack([item[\"image\"] for item in batch])\n",
        "\n",
        "    # Tokenize inputs and targets\n",
        "    inputs = tokenizer(input_texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
        "    targets = tokenizer(target_texts, padding=True, truncation=True, max_length=target_max_length, return_tensors=\"pt\")\n",
        "    # Replace pad tokens in targets with -100 for loss computation\n",
        "    targets_input_ids = targets.input_ids.masked_fill(targets.input_ids == tokenizer.pad_token_id, -100)\n",
        "\n",
        "    batch_dict = {\n",
        "        \"input_ids\": inputs.input_ids,\n",
        "        \"attention_mask\": inputs.attention_mask,\n",
        "        \"decoder_input_ids\": targets.input_ids,  # for teacher forcing\n",
        "        \"labels\": targets_input_ids,\n",
        "        \"pixel_values\": images\n",
        "    }\n",
        "    return batch_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0afEJo3qoyRD"
      },
      "outputs": [],
      "source": [
        "# ==============================\n",
        "# Model Definition\n",
        "# ==============================\n",
        "class MultimodalSarcasmExplanationModel(nn.Module):\n",
        "    def __init__(self, bart_model_name=\"facebook/bart-base\", vit_model_name=\"google/vit-base-patch16-224\"):\n",
        "        super(MultimodalSarcasmExplanationModel, self).__init__()\n",
        "        self.bart = BartForConditionalGeneration.from_pretrained(bart_model_name)\n",
        "        self.vit = ViTModel.from_pretrained(vit_model_name)\n",
        "        # The fusion layer projects the ViT output to BART's d_model if needed.\n",
        "        self.fusion_layer = nn.Linear(self.vit.config.hidden_size, self.bart.config.d_model)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, decoder_input_ids, pixel_values, labels=None):\n",
        "        # Encode the text input with BART encoder\n",
        "        encoder_outputs = self.bart.model.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        text_features = encoder_outputs.last_hidden_state  # shape: (batch, seq_len, d_model)\n",
        "\n",
        "        # Extract image features using ViT\n",
        "        vit_outputs = self.vit(pixel_values=pixel_values)\n",
        "        image_feature = vit_outputs.last_hidden_state[:, 0, :]  # global image representation\n",
        "        image_feature_proj = self.fusion_layer(image_feature)   # project to d_model\n",
        "\n",
        "        # Shared Fusion: add the projected image feature to each token embedding of text features.\n",
        "        fused_features = text_features + image_feature_proj.unsqueeze(1)\n",
        "\n",
        "        # Prepare custom encoder output to pass to the decoder\n",
        "        fused_encoder_outputs = BaseModelOutput(last_hidden_state=fused_features)\n",
        "\n",
        "        # Decode using BART decoder with the fused encoder outputs.\n",
        "        outputs = self.bart.model.decoder(\n",
        "            input_ids=decoder_input_ids,\n",
        "            encoder_hidden_states=fused_encoder_outputs.last_hidden_state,\n",
        "            encoder_attention_mask=attention_mask,\n",
        "            return_dict=True\n",
        "        )\n",
        "        sequence_output = outputs.last_hidden_state\n",
        "        # Compute logits using the shared language modeling head of BART\n",
        "        lm_logits = self.bart.lm_head(sequence_output) + self.bart.final_logits_bias\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "            loss = loss_fct(lm_logits.view(-1, self.bart.config.vocab_size), labels.view(-1))\n",
        "\n",
        "        return Seq2SeqLMOutput(loss=loss, logits=lm_logits, encoder_last_hidden_state=fused_encoder_outputs.last_hidden_state)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "w-3eMwJwoyF-"
      },
      "outputs": [],
      "source": [
        "# ==============================\n",
        "# Training Function\n",
        "# ==============================\n",
        "def train(model, dataloader, optimizer, tokenizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        # Move batch to device\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        decoder_input_ids = batch[\"decoder_input_ids\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        pixel_values = batch[\"pixel_values\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask,\n",
        "                        decoder_input_ids=decoder_input_ids, pixel_values=pixel_values, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    return avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_repeated_sentences(text):\n",
        "    \"\"\"\n",
        "    Enhanced function to remove various types of repetitions in text:\n",
        "    1. Repeated sentences (even with slight variations)\n",
        "    2. Repeated phrases and n-grams\n",
        "    3. Stuttering words and repetitive patterns\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return text\n",
        "        \n",
        "    # First pass: Remove duplicate sentences using sentence similarity\n",
        "    sentences = sent_tokenize(text)\n",
        "    if not sentences:\n",
        "        return text\n",
        "        \n",
        "    # Keep track of sentence similarity to filter near-duplicates\n",
        "    filtered_sentences = [sentences[0]]\n",
        "    for sent in sentences[1:]:\n",
        "        # Simple similarity check - could be improved with more sophisticated metrics\n",
        "        is_duplicate = False\n",
        "        for existing in filtered_sentences:\n",
        "            # Check if either sentence is a subset of the other or very similar\n",
        "            if (sent.lower() in existing.lower() or \n",
        "                existing.lower() in sent.lower() or\n",
        "                len(set(sent.lower().split()) & set(existing.lower().split())) > 0.7 * min(len(sent.split()), len(existing.split()))):\n",
        "                is_duplicate = True\n",
        "                break\n",
        "        if not is_duplicate:\n",
        "            filtered_sentences.append(sent)\n",
        "    \n",
        "    # Rejoin filtered sentences\n",
        "    text = \" \".join(filtered_sentences)\n",
        "    \n",
        "    # Second pass: Handle repeated phrases (3+ words)\n",
        "    words = text.split()\n",
        "    for phrase_len in range(3, min(8, len(words)//2)):  # Check phrases from 3 to 7 words or half text length\n",
        "        i = 0\n",
        "        while i <= len(words) - 2*phrase_len:\n",
        "            phrase1 = \" \".join(words[i:i+phrase_len])\n",
        "            phrase2 = \" \".join(words[i+phrase_len:i+2*phrase_len])\n",
        "            if phrase1.lower() == phrase2.lower():\n",
        "                # Remove the duplicate phrase\n",
        "                words = words[:i+phrase_len] + words[i+2*phrase_len:]\n",
        "            else:\n",
        "                i += 1\n",
        "    \n",
        "    # Third pass: Handle stuttering words (e.g., \"the the\", \"very very very\")\n",
        "    i = 0\n",
        "    while i < len(words) - 1:\n",
        "        if words[i].lower() == words[i+1].lower():\n",
        "            words.pop(i+1)  # Remove the duplicate\n",
        "        else:\n",
        "            i += 1\n",
        "    \n",
        "    # Fourth pass: Handle partial word repetitions (e.g., \"thethethe\")\n",
        "    for i in range(len(words)):\n",
        "        word = words[i]\n",
        "        if len(word) >= 6:  # Only check longer words\n",
        "            # Look for repeating patterns\n",
        "            for pattern_len in range(2, len(word)//2 + 1):\n",
        "                pattern = word[:pattern_len]\n",
        "                if pattern * (len(word) // len(pattern)) == word[:-(len(word) % pattern_len) or None]:\n",
        "                    words[i] = pattern  # Replace with just the pattern\n",
        "                    break\n",
        "    \n",
        "    return \" \".join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "M6FHqaFIpBGF"
      },
      "outputs": [],
      "source": [
        "# ==============================\n",
        "# Validation & Evaluation Functions\n",
        "# ==============================\n",
        "def generate_explanations(model, dataloader, tokenizer, device, max_length=64):\n",
        "    model.eval()\n",
        "    generated_texts = []\n",
        "    ground_truths = []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            pixel_values = batch[\"pixel_values\"].to(device)\n",
        "            # Encode text and fuse image features\n",
        "            encoder_outputs = model.bart.model.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            text_features = encoder_outputs.last_hidden_state\n",
        "\n",
        "            vit_outputs = model.vit(pixel_values=pixel_values)\n",
        "            image_feature = vit_outputs.last_hidden_state[:, 0, :]\n",
        "            image_feature_proj = model.fusion_layer(image_feature)\n",
        "\n",
        "            fused_features = text_features + image_feature_proj.unsqueeze(1)\n",
        "            fused_encoder_outputs = BaseModelOutput(last_hidden_state=fused_features)\n",
        "\n",
        "            generated_ids = model.bart.generate(\n",
        "                encoder_outputs=fused_encoder_outputs,\n",
        "                attention_mask=attention_mask,\n",
        "                max_length=max_length,\n",
        "                num_beams=4,                # using beam search with a beam width of 6\n",
        "                repetition_penalty=2.0,     # increases penalty for repeated tokens\n",
        "                no_repeat_ngram_size=3,     # prevents any 3-gram from repeating\n",
        "                do_sample=True,             # enables sampling (stochastic generation)\n",
        "                top_k=50,                   # limits the next-token choices to the top 75 tokens\n",
        "                temperature=1.5,            # a temperature above 1 can add diversity\n",
        "                early_stopping=True\n",
        "            )\n",
        "\n",
        "            # Decode predictions and post-process to remove repeated sentences\n",
        "            decoded_preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "            decoded_preds = [remove_repeated_sentences(pred) for pred in decoded_preds]\n",
        "            generated_texts.extend(decoded_preds)\n",
        "\n",
        "            # Process ground truth labels\n",
        "            # Replace -100 with the pad token id before decoding ground truth labels\n",
        "            labels = batch[\"labels\"].to(device).clone()\n",
        "            labels[labels == -100] = tokenizer.pad_token_id  # change this line\n",
        "            targets = tokenizer.batch_decode(labels.tolist(), skip_special_tokens=True)\n",
        "            ground_truths.extend(targets)\n",
        "\n",
        "    return generated_texts, ground_truths\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "UQ5MPAOgpFSI"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(preds, targets):\n",
        "    rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    rouge1_scores, rouge2_scores, rougeL_scores = [], [], []\n",
        "    bleu_scores, meteor_scores = [], []\n",
        "    for pred, target in zip(preds, targets):\n",
        "        scores = rouge.score(target, pred)\n",
        "        rouge1_scores.append(scores['rouge1'].fmeasure)\n",
        "        rouge2_scores.append(scores['rouge2'].fmeasure)\n",
        "        rougeL_scores.append(scores['rougeL'].fmeasure)\n",
        "\n",
        "        smoothing = SmoothingFunction().method1\n",
        "        bleu = sentence_bleu([target.split()], pred.split(), smoothing_function=smoothing)\n",
        "        bleu_scores.append(bleu)\n",
        "\n",
        "        # Split both prediction and target into tokens for meteor_score\n",
        "        meteor = meteor_score([target.split()], pred.split())\n",
        "        meteor_scores.append(meteor)\n",
        "\n",
        "    P, R, F1 = bert_score(preds, targets, lang=\"en\", verbose=False)\n",
        "\n",
        "    metrics = {\n",
        "        \"ROUGE-1\": np.mean(rouge1_scores),\n",
        "        \"ROUGE-2\": np.mean(rouge2_scores),\n",
        "        \"ROUGE-L\": np.mean(rougeL_scores),\n",
        "        \"BLEU\": np.mean(bleu_scores),\n",
        "        \"METEOR\": np.mean(meteor_scores),\n",
        "        \"BERTScore_F1\": F1.mean().item()\n",
        "    }\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "YzFOaXr5pFIg"
      },
      "outputs": [],
      "source": [
        "# ==============================\n",
        "# Inference Function\n",
        "# ==============================\n",
        "def inference(model, test_df_path, desc_pickle, obj_pickle, image_dir, tokenizer, device, output_file=\"sarcasm_explanations.txt\"):\n",
        "    test_dataset = MultimodalSarcasmDataset(test_df_path, desc_pickle, obj_pickle, image_dir, tokenizer)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False,\n",
        "                             collate_fn=lambda batch: collate_fn(batch, tokenizer))\n",
        "    generated_texts, gt_texts = generate_explanations(model, test_loader, tokenizer, device)\n",
        "    metrics = compute_metrics(generated_texts, gt_texts)\n",
        "    print(\"\\nEvaluation Metrics on Test Set:\")\n",
        "    for k, v in metrics.items():\n",
        "        print(f\"{k}: {v:.4f}\")\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        for line in generated_texts:\n",
        "            f.write(line + \"\\n\")\n",
        "    print(f\"\\nGenerated sarcasm explanations saved to {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cp_kgJC1pE4r"
      },
      "outputs": [],
      "source": [
        "# ==============================\n",
        "# Main Function\n",
        "# ==============================\n",
        "def main(args):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
        "    # Create training and validation datasets\n",
        "    train_dataset = MultimodalSarcasmDataset(\n",
        "        df_path=os.path.join(args.data_dir, \"train_df.tsv\"),\n",
        "        desc_pickle=os.path.join(args.data_dir, \"D_train.pkl\"),\n",
        "        obj_pickle=os.path.join(args.data_dir, \"O_train.pkl\"),\n",
        "        image_dir=os.path.join(args.data_dir, \"images\"),\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "    val_dataset = MultimodalSarcasmDataset(\n",
        "        df_path=os.path.join(args.data_dir, \"val_df.tsv\"),\n",
        "        desc_pickle=os.path.join(args.data_dir, \"D_val.pkl\"),\n",
        "        obj_pickle=os.path.join(args.data_dir, \"O_val.pkl\"),\n",
        "        image_dir=os.path.join(args.data_dir, \"images\"),\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True,\n",
        "                              collate_fn=lambda batch: collate_fn(batch, tokenizer))\n",
        "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False,\n",
        "                            collate_fn=lambda batch: collate_fn(batch, tokenizer))\n",
        "\n",
        "    model = MultimodalSarcasmExplanationModel()\n",
        "    model.to(device)\n",
        "\n",
        "    if args.mode == \"test\":\n",
        "        if args.checkpoint_path is None:\n",
        "            raise ValueError(\"Checkpoint path must be provided in test mode using --checkpoint_path\")\n",
        "        print(f\"Loading model checkpoint from {args.checkpoint_path}\")\n",
        "        model.load_state_dict(torch.load(args.checkpoint_path, map_location=device))\n",
        "        # Run inference on the test data\n",
        "        test_df_path = os.path.join(args.data_dir, \"val_df.tsv\")\n",
        "        inference(model, test_df_path,\n",
        "                  desc_pickle=os.path.join(args.data_dir, \"D_val.pkl\"),\n",
        "                  obj_pickle=os.path.join(args.data_dir, \"O_val.pkl\"),\n",
        "                  image_dir=os.path.join(args.data_dir, \"images\"),\n",
        "                  tokenizer=tokenizer,\n",
        "                  device=device,\n",
        "                  output_file=args.output_file)\n",
        "        \n",
        "    else:\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=args.learning_rate)\n",
        "\n",
        "        best_bert_score = 0\n",
        "        for epoch in range(args.epochs):\n",
        "            print(f\"\\nEpoch {epoch+1}/{args.epochs}\")\n",
        "            train_loss = train(model, train_loader, optimizer, tokenizer, device)\n",
        "            print(f\"Training Loss: {train_loss:.4f}\")\n",
        "\n",
        "            model.eval()\n",
        "            total_val_loss = 0\n",
        "            with torch.no_grad():\n",
        "                for batch in val_loader:\n",
        "                    input_ids = batch[\"input_ids\"].to(device)\n",
        "                    attention_mask = batch[\"attention_mask\"].to(device)\n",
        "                    decoder_input_ids = batch[\"decoder_input_ids\"].to(device)\n",
        "                    labels = batch[\"labels\"].to(device)\n",
        "                    pixel_values = batch[\"pixel_values\"].to(device)\n",
        "                    outputs = model(input_ids=input_ids, attention_mask=attention_mask,\n",
        "                                    decoder_input_ids=decoder_input_ids, pixel_values=pixel_values, labels=labels)\n",
        "                    total_val_loss += outputs.loss.item()\n",
        "            avg_val_loss = total_val_loss / len(val_loader)\n",
        "            print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "            # Generate sample explanations for monitoring\n",
        "            gen_texts, gt_texts = generate_explanations(model, val_loader, tokenizer, device)\n",
        "            print(\"\\nSample Generated Explanations (Validation):\")\n",
        "            for i in range(min(3, len(gen_texts))):\n",
        "                print(f\"GT: {gt_texts[i]}\")\n",
        "                print(f\"Pred: {gen_texts[i]}\")\n",
        "                print(\"-\" * 40)\n",
        "\n",
        "            metrics = compute_metrics(gen_texts, gt_texts)\n",
        "            print(\"\\nEvaluation Metrics on Validation Set:\")\n",
        "            for k, v in metrics.items():\n",
        "                print(f\"{k}: {v:.4f}\")\n",
        "\n",
        "            checkpoint_path = os.path.join(args.checkpoint_dir, f\"model_epoch_{epoch+1}.pt\")\n",
        "            os.makedirs(args.checkpoint_dir, exist_ok=True)\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "            print(f\"Model checkpoint saved to {checkpoint_path}\")\n",
        "\n",
        "            # save best model based on bert score\n",
        "            if metrics[\"BERTScore_F1\"] > best_bert_score:\n",
        "                best_bert_score = metrics[\"BERTScore_F1\"]\n",
        "                best_model_path = os.path.join(args.checkpoint_dir, \"best_model.pt\")\n",
        "                torch.save(model.state_dict(), best_model_path)\n",
        "                print(f\"Best model saved to {best_model_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda:0\n",
            "\n",
            "Epoch 1/3\n",
            "Training Loss: 3.1209\n",
            "Validation Loss: 0.1868\n",
            "\n",
            "Sample Generated Explanations (Validation):\n",
            "GT: the author is pissed at <user> for not getting network in malad.\n",
            "Pred: the the what which that this this this these these these such so soso so SOSO SO sooooooooooooooooooooooooooooooooooooooo\n",
            "----------------------------------------\n",
            "GT: nothing worst than waiting for an hour on the tarmac for a gate to come open in snowy, windy chicago.\n",
            "Pred: the what what what which that this these these these theseThese these such such such a aaaaaaaaaaahaaaaAAAAAABABABABaBaBa Ba Bailey\n",
            "----------------------------------------\n",
            "GT: nobody likes getting one hour of their life sucked away.\n",
            "Pred: springspringspringpringspringspring spring springs spring sprang sprung spring rise rises rise rose rise rising rise rise rise risen rise up above below above over above above above lower bottom\n",
            "----------------------------------------\n",
            "\n",
            "Evaluation Metrics on Validation Set:\n",
            "ROUGE-1: 0.0665\n",
            "ROUGE-2: 0.0003\n",
            "ROUGE-L: 0.0627\n",
            "BLEU: 0.0078\n",
            "METEOR: 0.0411\n",
            "BERTScore_F1: 0.7759\n",
            "Model checkpoint saved to checkpoints/model_epoch_1.pt\n",
            "Best model saved to checkpoints/best_model.pt\n",
            "\n",
            "Epoch 2/3\n",
            "Training Loss: 0.1065\n",
            "Validation Loss: 0.0101\n",
            "\n",
            "Sample Generated Explanations (Validation):\n",
            "GT: the author is pissed at <user> for not getting network in malad.\n",
            "Pred: thatthatthatThatThatThatthatthat that which what whatwhat what why whywhy why because reason reasons reason cause causes cause causing caused cause\n",
            "----------------------------------------\n",
            "GT: nothing worst than waiting for an hour on the tarmac for a gate to come open in snowy, windy chicago.\n",
            "Pred: thethetheThethethethatthatthatThatThatThatthatthat that which what whatwhatwhatwhatWhatwhatwhatWHATWHATWHATwhatwhatsuchsuchsuch such an ananananaranaranaranarinarinarininininINININ IN\n",
            "----------------------------------------\n",
            "GT: nobody likes getting one hour of their life sucked away.\n",
            "Pred: thethethethatthatthatThatThatThatthatthat that which what whatever whateverwhateverwhateverwhateverwhatwhatwhatWhat what why whywhywhywhyWhyWhyWhy why because Because because since\n",
            "----------------------------------------\n",
            "\n",
            "Evaluation Metrics on Validation Set:\n",
            "ROUGE-1: 0.0379\n",
            "ROUGE-2: 0.0000\n",
            "ROUGE-L: 0.0369\n",
            "BLEU: 0.0046\n",
            "METEOR: 0.0223\n",
            "BERTScore_F1: 0.7685\n",
            "Model checkpoint saved to checkpoints/model_epoch_2.pt\n",
            "\n",
            "Epoch 3/3\n",
            "Training Loss: 0.0321\n",
            "Validation Loss: 0.0029\n",
            "\n",
            "Sample Generated Explanations (Validation):\n",
            "GT: the author is pissed at <user> for not getting network in malad.\n",
            "Pred: thethetheTheTheThe The the what whatwhatwhatwhatWhatwhatwhatthatthatthatThatThatThat That that those thosethosethosethosethesethesetheseThesethesethese these thesethesethesetheytheytheyTheytheythey\n",
            "----------------------------------------\n",
            "GT: nothing worst than waiting for an hour on the tarmac for a gate to come open in snowy, windy chicago.\n",
            "Pred: thethetheTheTheThe TheTheThethethewhatwhatwhatWhatWhatWhatWHATWHATWHAT WHAT which that thatthatthatThatThatThat ThatThatThatthatthatthatthisthatthatitthatthatthosethosethosethesetheseThesethesethesethese\n",
            "----------------------------------------\n",
            "GT: nobody likes getting one hour of their life sucked away.\n",
            "Pred: thethetheTheTheThe The THETHETHETHETheThethethethatthatthatThatThat That that THATThatThatthatThatthatthat that thatThatThatThisThatThatItThatThat:ThatThatThosethosethosethosethesethesethese\n",
            "----------------------------------------\n",
            "\n",
            "Evaluation Metrics on Validation Set:\n",
            "ROUGE-1: 0.0375\n",
            "ROUGE-2: 0.0005\n",
            "ROUGE-L: 0.0359\n",
            "BLEU: 0.0046\n",
            "METEOR: 0.0229\n",
            "BERTScore_F1: 0.7611\n",
            "Model checkpoint saved to checkpoints/model_epoch_3.pt\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description=\"Multimodal Sarcasm Explanation (MuSE) Task\")\n",
        "    parser.add_argument(\"--data_dir\", type=str, default=\"MORE-PLUS-DATASET\", help=\"Path to the dataset folder\")\n",
        "    parser.add_argument(\"--checkpoint_dir\", type=str, default=\"checkpoints\", help=\"Directory to save model checkpoints\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=8, help=\"Batch size\")\n",
        "    parser.add_argument(\"--epochs\", type=int, default=3, help=\"Number of training epochs\")\n",
        "    parser.add_argument(\"--learning_rate\", type=float, default=5e-5, help=\"Learning rate\")\n",
        "    parser.add_argument(\"--mode\", type=str, choices=[\"train\", \"test\"], default=\"train\", help=\"Mode: train or test\")\n",
        "    parser.add_argument(\"--output_file\", type=str, default=\"sarcasm_explanations.txt\", help=\"Output file for test predictions\")\n",
        "    parser.add_argument(\"--checkpoint_path\", type=str, default=\"checkpoints/best_model.pt\", help=\"Path to the model checkpoint for testing\")\n",
        "\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    main(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda:0\n",
            "Loading model checkpoint from checkpoints/best_model.pt\n",
            "\n",
            "Evaluation Metrics on Test Set:\n",
            "ROUGE-1: 0.0593\n",
            "ROUGE-2: 0.0003\n",
            "ROUGE-L: 0.0560\n",
            "BLEU: 0.0069\n",
            "METEOR: 0.0368\n",
            "BERTScore_F1: 0.7732\n",
            "\n",
            "Generated sarcasm explanations saved to sarcasm_explanations.txt\n"
          ]
        }
      ],
      "source": [
        "# testing\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description=\"Multimodal Sarcasm Explanation (MuSE) Task\")\n",
        "    parser.add_argument(\"--data_dir\", type=str, default=\"MORE-PLUS-DATASET\", help=\"Path to the dataset folder\")\n",
        "    parser.add_argument(\"--checkpoint_dir\", type=str, default=\"checkpoints\", help=\"Directory to save model checkpoints\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=8, help=\"Batch size\")\n",
        "    parser.add_argument(\"--epochs\", type=int, default=3, help=\"Number of training epochs\")\n",
        "    parser.add_argument(\"--learning_rate\", type=float, default=5e-5, help=\"Learning rate\")\n",
        "    parser.add_argument(\"--mode\", type=str, choices=[\"train\", \"test\"], default=\"test\", help=\"Mode: train or test\")\n",
        "    parser.add_argument(\"--output_file\", type=str, default=\"sarcasm_explanations.txt\", help=\"Output file for test predictions\")\n",
        "    parser.add_argument(\"--checkpoint_path\", type=str, default=\"checkpoints/best_model.pt\", help=\"Path to the model checkpoint for testing\")\n",
        "\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    main(args)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch2.5",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
