{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "GRbDexIOoYuY"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /home/ritika22408/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import pickle\n",
        "import argparse\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration, ViTModel, ViTFeatureExtractor, logging\n",
        "from transformers.modeling_outputs import BaseModelOutput, Seq2SeqLMOutput\n",
        "\n",
        "# For evaluation metrics\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from rouge_score import rouge_scorer\n",
        "from bert_score import score as bert_score\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# random_seed = 42\n",
        "# torch.manual_seed(random_seed)\n",
        "# np.random.seed(random_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "eBBJCQpxou3d"
      },
      "outputs": [],
      "source": [
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "# Silence transformers warnings\n",
        "logging.set_verbosity_error()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "1Tv83vzZoygn"
      },
      "outputs": [],
      "source": [
        "class MultimodalSarcasmDataset(Dataset):\n",
        "    def __init__(self, df_path, desc_pickle, obj_pickle, image_dir, tokenizer, max_length=256, transform=None):\n",
        "        \"\"\"\n",
        "        df_path: path to TSV file with columns [pid, text, explanation, sarcasm_target]\n",
        "        desc_pickle: pickle file containing image descriptions (dictionary: pid -> description string)\n",
        "        obj_pickle: pickle file containing detected objects (dictionary: pid -> object string or list)\n",
        "        image_dir: directory with images named by pid (e.g. pid.jpg or png)\n",
        "        tokenizer: BART tokenizer\n",
        "        max_length: maximum token length for input sequence\n",
        "        transform: torchvision transforms for image preprocessing\n",
        "        \"\"\"\n",
        "        self.df = pd.read_csv(df_path, sep=\"\\t\")\n",
        "        with open(desc_pickle, \"rb\") as f:\n",
        "            self.desc_dict = pickle.load(f)\n",
        "        with open(obj_pickle, \"rb\") as f:\n",
        "            self.obj_dict = pickle.load(f)\n",
        "        self.image_dir = image_dir\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        # if no transform provided, define one for ViT (assumes 224x224 images)\n",
        "        if transform is None:\n",
        "            self.transform = transforms.Compose([\n",
        "                transforms.Resize((224, 224)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "            ])\n",
        "        else:\n",
        "            self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the sample row\n",
        "        row = self.df.iloc[idx]\n",
        "        pid = str(row[\"pid\"])\n",
        "        text = str(row[\"text\"])\n",
        "        explanation = str(row[\"explanation\"])\n",
        "        sarcasm_target = str(row[\"target_of_sarcasm\"])\n",
        "        # Get image description and detected objects from pickles\n",
        "        image_desc = self.desc_dict.get(pid, \"\")\n",
        "        detected_obj = self.obj_dict.get(pid, \"\")\n",
        "        # Ensure all components are strings and concatenate them\n",
        "        input_text = sarcasm_target + \" \" + text + \" \" + str(image_desc) + \" \" + str(detected_obj)\n",
        "\n",
        "        # Load image (assuming image file named <pid>.jpg; adjust extension if needed)\n",
        "        image_path = os.path.join(self.image_dir, pid + \".jpg\")\n",
        "        if not os.path.exists(image_path):\n",
        "            image_path = os.path.join(self.image_dir, pid + \".png\")\n",
        "        try:\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "        except Exception as e:\n",
        "            raise FileNotFoundError(f\"Image for pid {pid} not found: {e}\")\n",
        "        image = self.transform(image)\n",
        "\n",
        "        return {\n",
        "            \"input_text\": input_text,\n",
        "            \"target_text\": explanation,\n",
        "            \"image\": image\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "vMcP_5zroyY2"
      },
      "outputs": [],
      "source": [
        "# ==============================\n",
        "# Custom Collate Function\n",
        "# ==============================\n",
        "def collate_fn(batch, tokenizer, max_length=256, target_max_length=64):\n",
        "    input_texts = [item[\"input_text\"] for item in batch]\n",
        "    target_texts = [item[\"target_text\"] for item in batch]\n",
        "    images = torch.stack([item[\"image\"] for item in batch])\n",
        "\n",
        "    # Tokenize inputs and targets\n",
        "    inputs = tokenizer(input_texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
        "    targets = tokenizer(target_texts, padding=True, truncation=True, max_length=target_max_length, return_tensors=\"pt\")\n",
        "    # Replace pad tokens in targets with -100 for loss computation\n",
        "    targets_input_ids = targets.input_ids.masked_fill(targets.input_ids == tokenizer.pad_token_id, -100)\n",
        "\n",
        "    batch_dict = {\n",
        "        \"input_ids\": inputs.input_ids,\n",
        "        \"attention_mask\": inputs.attention_mask,\n",
        "        \"decoder_input_ids\": targets.input_ids,  # for teacher forcing\n",
        "        \"labels\": targets_input_ids,\n",
        "        \"pixel_values\": images\n",
        "    }\n",
        "    return batch_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "0afEJo3qoyRD"
      },
      "outputs": [],
      "source": [
        "# ==============================\n",
        "# Model Definition\n",
        "# ==============================\n",
        "class MultimodalSarcasmExplanationModel(nn.Module):\n",
        "    def __init__(self, bart_model_name=\"facebook/bart-base\", vit_model_name=\"google/vit-base-patch16-224\"):\n",
        "        super(MultimodalSarcasmExplanationModel, self).__init__()\n",
        "        self.bart = BartForConditionalGeneration.from_pretrained(bart_model_name)\n",
        "        self.vit = ViTModel.from_pretrained(vit_model_name)\n",
        "        # The fusion layer projects the ViT output to BART's d_model if needed.\n",
        "        self.fusion_layer = nn.Linear(self.vit.config.hidden_size, self.bart.config.d_model)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, decoder_input_ids, pixel_values, labels=None):\n",
        "        # Encode the text input with BART encoder\n",
        "        encoder_outputs = self.bart.model.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        text_features = encoder_outputs.last_hidden_state  # shape: (batch, seq_len, d_model)\n",
        "\n",
        "        # Extract image features using ViT\n",
        "        vit_outputs = self.vit(pixel_values=pixel_values)\n",
        "        image_feature = vit_outputs.last_hidden_state[:, 0, :]  # global image representation\n",
        "        image_feature_proj = self.fusion_layer(image_feature)   # project to d_model\n",
        "\n",
        "        # Shared Fusion: add the projected image feature to each token embedding of text features.\n",
        "        fused_features = text_features + image_feature_proj.unsqueeze(1)\n",
        "\n",
        "        # Prepare custom encoder output to pass to the decoder\n",
        "        fused_encoder_outputs = BaseModelOutput(last_hidden_state=fused_features)\n",
        "\n",
        "        # Decode using BART decoder with the fused encoder outputs.\n",
        "        outputs = self.bart.model.decoder(\n",
        "            input_ids=decoder_input_ids,\n",
        "            encoder_hidden_states=fused_encoder_outputs.last_hidden_state,\n",
        "            encoder_attention_mask=attention_mask,\n",
        "            return_dict=True\n",
        "        )\n",
        "        sequence_output = outputs.last_hidden_state\n",
        "        # Compute logits using the shared language modeling head of BART\n",
        "        lm_logits = self.bart.lm_head(sequence_output) + self.bart.final_logits_bias\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "            loss = loss_fct(lm_logits.view(-1, self.bart.config.vocab_size), labels.view(-1))\n",
        "\n",
        "        return Seq2SeqLMOutput(loss=loss, logits=lm_logits, encoder_last_hidden_state=fused_encoder_outputs.last_hidden_state)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "w-3eMwJwoyF-"
      },
      "outputs": [],
      "source": [
        "# ==============================\n",
        "# Training Function\n",
        "# ==============================\n",
        "def train(model, dataloader, optimizer, tokenizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        # Move batch to device\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        decoder_input_ids = batch[\"decoder_input_ids\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        pixel_values = batch[\"pixel_values\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask,\n",
        "                        decoder_input_ids=decoder_input_ids, pixel_values=pixel_values, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    return avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "M6FHqaFIpBGF"
      },
      "outputs": [],
      "source": [
        "# ==============================\n",
        "# Validation & Evaluation Functions\n",
        "# ==============================\n",
        "def generate_explanations(model, dataloader, tokenizer, device, max_length=64):\n",
        "    model.eval()\n",
        "    generated_texts = []\n",
        "    ground_truths = []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            pixel_values = batch[\"pixel_values\"].to(device)\n",
        "            # Encode text and fuse image features\n",
        "            encoder_outputs = model.bart.model.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            text_features = encoder_outputs.last_hidden_state\n",
        "\n",
        "            vit_outputs = model.vit(pixel_values=pixel_values)\n",
        "            image_feature = vit_outputs.last_hidden_state[:, 0, :]\n",
        "            image_feature_proj = model.fusion_layer(image_feature)\n",
        "\n",
        "            fused_features = text_features + image_feature_proj.unsqueeze(1)\n",
        "            fused_encoder_outputs = BaseModelOutput(last_hidden_state=fused_features)\n",
        "\n",
        "            generated_ids = model.bart.generate(\n",
        "                encoder_outputs=fused_encoder_outputs,\n",
        "                attention_mask=attention_mask,\n",
        "                max_length=max_length,\n",
        "                num_beams=4,                # using beam search with a beam width of 4\n",
        "                repetition_penalty=2.5,     # increases penalty for repeated tokens\n",
        "                no_repeat_ngram_size=3,     # prevents any 3-gram from repeating\n",
        "                do_sample=True,             # enables sampling (stochastic generation)\n",
        "                top_k=50,                   # limits the next-token choices to the top 50 tokens\n",
        "                top_p=0.9,                  # alternatively, you could use nucleus sampling\n",
        "                temperature=1.25,            # a temperature above 1 can add diversity\n",
        "                early_stopping=True\n",
        "            )\n",
        "\n",
        "            decoded_preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "            generated_texts.extend(decoded_preds)\n",
        "\n",
        "            # Replace -100 with the pad token id before decoding ground truth labels\n",
        "            labels = batch[\"labels\"].to(device).clone()\n",
        "            labels[labels == -100] = tokenizer.pad_token_id\n",
        "            targets = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "            ground_truths.extend(targets)\n",
        "    return generated_texts, ground_truths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "UQ5MPAOgpFSI"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(preds, targets):\n",
        "    rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    rouge1_scores, rouge2_scores, rougeL_scores = [], [], []\n",
        "    bleu_scores, meteor_scores = [], []\n",
        "    for pred, target in zip(preds, targets):\n",
        "        scores = rouge.score(target, pred)\n",
        "        rouge1_scores.append(scores['rouge1'].fmeasure)\n",
        "        rouge2_scores.append(scores['rouge2'].fmeasure)\n",
        "        rougeL_scores.append(scores['rougeL'].fmeasure)\n",
        "\n",
        "        smoothing = SmoothingFunction().method1\n",
        "        bleu = sentence_bleu([target.split()], pred.split(), smoothing_function=smoothing)\n",
        "        bleu_scores.append(bleu)\n",
        "\n",
        "        # Split both prediction and target into tokens for meteor_score\n",
        "        meteor = meteor_score([target.split()], pred.split())\n",
        "        meteor_scores.append(meteor)\n",
        "\n",
        "    P, R, F1 = bert_score(preds, targets, lang=\"en\", verbose=False)\n",
        "\n",
        "    metrics = {\n",
        "        \"ROUGE-1\": np.mean(rouge1_scores),\n",
        "        \"ROUGE-2\": np.mean(rouge2_scores),\n",
        "        \"ROUGE-L\": np.mean(rougeL_scores),\n",
        "        \"BLEU\": np.mean(bleu_scores),\n",
        "        \"METEOR\": np.mean(meteor_scores),\n",
        "        \"BERTScore_F1\": F1.mean().item()\n",
        "    }\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "YzFOaXr5pFIg"
      },
      "outputs": [],
      "source": [
        "# ==============================\n",
        "# Inference Function\n",
        "# ==============================\n",
        "def inference(model, test_df_path, desc_pickle, obj_pickle, image_dir, tokenizer, device, output_file=\"sarcasm_explanations.txt\"):\n",
        "    test_dataset = MultimodalSarcasmDataset(test_df_path, desc_pickle, obj_pickle, image_dir, tokenizer)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False,\n",
        "                             collate_fn=lambda batch: collate_fn(batch, tokenizer))\n",
        "    generated_texts, _ = generate_explanations(model, test_loader, tokenizer, device)\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        for line in generated_texts:\n",
        "            f.write(line + \"\\n\")\n",
        "    print(f\"Generated sarcasm explanations saved to {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "cp_kgJC1pE4r"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/5\n",
            "Training Loss: 2.8785\n",
            "Validation Loss: 0.1758\n",
            "\n",
            "Sample Generated Explanations (Validation):\n",
            "GT: the author is pissed at <user> for not getting network in malad.\n",
            "Pred: this this this this these these thesethesethesethese These These ThesethesetheseThesethesethesethethethethemthemthemmmm m m m M M MM M M & & &&&&^^^ ^^^### # # ### \"# \"# \"# # #\n",
            "----------------------------------------\n",
            "GT: nothing worst than waiting for an hour on the tarmac for a gate to come open in snowy, windy chicago.\n",
            "Pred: aaa a a a an an an An An AnAn An AnanananANAN ANANANANANIANIANIanianianiatiatiatiifiifiifiFiFiFifififiFiFi FiFiFiwiFiFiWiFiFiFIFiFi fiFiFiBiFi\n",
            "----------------------------------------\n",
            "GT: nobody likes getting one hour of their life sucked away.\n",
            "Pred: springspringspring spring spring spring springs springs springs sprung sprung sprung sprang sprang sprang popped popped popped pop pop pop Pop Pop PopPop Pop Poppop Pop Pop POP POP POPOPOPOPopopop op op op Opopopopsopopppopoppopopumpopoppoopopup\n",
            "----------------------------------------\n",
            "\n",
            "Evaluation Metrics on Validation Set:\n",
            "ROUGE-1: 0.0289\n",
            "ROUGE-2: 0.0006\n",
            "ROUGE-L: 0.0277\n",
            "BLEU: 0.0022\n",
            "METEOR: 0.0212\n",
            "BERTScore_F1: 0.7548\n",
            "\n",
            "Epoch 2/5\n",
            "Training Loss: 0.4620\n",
            "Validation Loss: 4.2913\n",
            "\n",
            "Sample Generated Explanations (Validation):\n",
            "GT: the author is pissed at <user> for not getting network in malad.\n",
            "Pred: user author author author writer author author the author author is a a a an author author authority author author reviewer author author book author author's author author person author author researcher author author president author author interested author author his author author reader author author man author authorwriter author author character author author photographer author author director author\n",
            "----------------------------------------\n",
            "GT: nothing worst than waiting for an hour on the tarmac for a gate to come open in snowy, windy chicago.\n",
            "Pred: the isn doesn's't't't don't't aren't't is to to to make to to for to to see to to a to to the to to there to to look to to have to to about to to author to to find to to play to to and to to get to to read to\n",
            "----------------------------------------\n",
            "GT: nobody likes getting one hour of their life sucked away.\n",
            "Pred:  author author author isn't't't doesn't't aren't't don't't's't't not't't didn't't won't't to to to be to to for to to see to to is to to find to to make to to look to to have to to the to to a to to\n",
            "----------------------------------------\n",
            "\n",
            "Evaluation Metrics on Validation Set:\n",
            "ROUGE-1: 0.0914\n",
            "ROUGE-2: 0.0159\n",
            "ROUGE-L: 0.0723\n",
            "BLEU: 0.0046\n",
            "METEOR: 0.0706\n",
            "BERTScore_F1: 0.7816\n",
            "\n",
            "Epoch 3/5\n",
            "Training Loss: 0.2820\n",
            "Validation Loss: 0.0161\n",
            "\n",
            "Sample Generated Explanations (Validation):\n",
            "GT: the author is pissed at <user> for not getting network in malad.\n",
            "Pred: thisthisthisThisthisthis this this this these these thesethesethesetheseThese these these such such suchsuchsuchsuchsimilarsimilarsimilar analogous analogous analogous comparable comparable comparable similar similar similar different different distinct distinct distinct contrasting contrasting contrasting contradictory contradictory contradictory conflicting contradictory contradictory oppos oppos oppos contrasts contrasts contrasts differences differences differences difference difference\n",
            "----------------------------------------\n",
            "GT: nothing worst than waiting for an hour on the tarmac for a gate to come open in snowy, windy chicago.\n",
            "Pred: thisthisthisThis this this This this this this these these thesethesethesetheseThese these these such such suchsuchsuchsuchSuch such such Such such such similar similar similarsimilarsimilarSimilarsimilarsimilarsimilarimilarsimilarsimilar similarities similarities similarities distinctions distinctions distinctions differences differences differences difference difference difference between between between different different different\n",
            "----------------------------------------\n",
            "GT: nobody likes getting one hour of their life sucked away.\n",
            "Pred: springspringspringSpringspringspring spring spring spring Spring spring spring summer summer summer winter winter winterwinterwinterWinterwinterwinterwinterseasonseasonseasontimetimetimetimestimestimesTimesTimesTimes TimesTimesTimes times times times days days days mornings mornings mornings weekends weekends weekends Sundays Sundays Sundays weekends weekends weekend weekends weekends Saturdays Sundays\n",
            "----------------------------------------\n",
            "\n",
            "Evaluation Metrics on Validation Set:\n",
            "ROUGE-1: 0.0215\n",
            "ROUGE-2: 0.0007\n",
            "ROUGE-L: 0.0213\n",
            "BLEU: 0.0023\n",
            "METEOR: 0.0165\n",
            "BERTScore_F1: 0.7528\n",
            "\n",
            "Epoch 4/5\n",
            "Training Loss: 0.0301\n",
            "Validation Loss: 0.0070\n",
            "\n",
            "Sample Generated Explanations (Validation):\n",
            "GT: the author is pissed at <user> for not getting network in malad.\n",
            "Pred: thisthisthisThis this this this these these thesethesethesetheseThese these these such such suchsuchsuchsuchSuch such such Such such such so so so SO SO SOSOSOSOsososoSo So So So so so anyway anyway anyway anyways anyways anyways hey hey heyHey hey heyheyheyhey\n",
            "----------------------------------------\n",
            "GT: nothing worst than waiting for an hour on the tarmac for a gate to come open in snowy, windy chicago.\n",
            "Pred: nothingnothingnothing nothing nothing nothing anything anything anything everything everything everything everyone everyone everyone everybody everybody everybody everyone everyone everywhere everywhere everywhere anywhere anywhere anywhere anyone anyone anyone anybody anyone anyone else else else or or or even even even actually actually actually really really really REALLY REALLY REALLY kinda kinda kinda pretty pretty pretty crazy crazy crazycrazycrazy\n",
            "----------------------------------------\n",
            "GT: nobody likes getting one hour of their life sucked away.\n",
            "Pred: springspringspringSpringspringspring spring spring Spring spring spring spring springs springs springs sprung sprung sprung sprang sprang sprang burst burst burst bursting bursting bursting bursts bursts bursts bubbles bubbles bubbles bubble bubbles bubbles bub bub bub Bub Bub Bubbubbubbububububobububhubhubhub hub hub hub Hub Hub hub hub\n",
            "----------------------------------------\n",
            "\n",
            "Evaluation Metrics on Validation Set:\n",
            "ROUGE-1: 0.0175\n",
            "ROUGE-2: 0.0002\n",
            "ROUGE-L: 0.0170\n",
            "BLEU: 0.0018\n",
            "METEOR: 0.0150\n",
            "BERTScore_F1: 0.7561\n",
            "\n",
            "Epoch 5/5\n",
            "Training Loss: 0.0205\n",
            "Validation Loss: 0.0019\n",
            "\n",
            "Sample Generated Explanations (Validation):\n",
            "GT: the author is pissed at <user> for not getting network in malad.\n",
            "Pred: thisthisthis this this this these these thesethese these these such such suchsuchsuchsuch Such such such so so so So So SoSo So So SO SO SOsososoooooooooooooooooooooaaaaaaaaaaaaaaaaaaaaaaaaaaaaaAAAAAAAAAAAAAAAAAAA\n",
            "----------------------------------------\n",
            "GT: nothing worst than waiting for an hour on the tarmac for a gate to come open in snowy, windy chicago.\n",
            "Pred: thisthisthisThis this this this these these thesethesethesetheseThese these these such such suchsuchsuchsuch Such such such so so so SO SOSOSOSO SO SO SOsososoSo So So So so so very very very VERY VERY very very extremely very very quite very very pretty very very\n",
            "----------------------------------------\n",
            "GT: nobody likes getting one hour of their life sucked away.\n",
            "Pred: springspringspringpringspringspring spring spring spring summer summer summer season season season year year year week week week weeks weeks weeks weekly weekly weeklyweeklyweeklyweekly Weekly Weekly Weekly weekly weekly quarterly weekly weekly daily weekly weekly monthly weekly weekly random random randomRandom random randomrandom random random Random random random randomly random random unpredictable random\n",
            "----------------------------------------\n",
            "\n",
            "Evaluation Metrics on Validation Set:\n",
            "ROUGE-1: 0.0153\n",
            "ROUGE-2: 0.0002\n",
            "ROUGE-L: 0.0151\n",
            "BLEU: 0.0017\n",
            "METEOR: 0.0137\n",
            "BERTScore_F1: 0.7529\n"
          ]
        }
      ],
      "source": [
        "# ==============================\n",
        "# Main Function\n",
        "# ==============================\n",
        "def main(args):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
        "    # Create training and validation datasets\n",
        "    train_dataset = MultimodalSarcasmDataset(\n",
        "        df_path=os.path.join(args.data_dir, \"train_df.tsv\"),\n",
        "        desc_pickle=os.path.join(args.data_dir, \"D_train.pkl\"),\n",
        "        obj_pickle=os.path.join(args.data_dir, \"O_train.pkl\"),\n",
        "        image_dir=os.path.join(args.data_dir, \"images\"),\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "    val_dataset = MultimodalSarcasmDataset(\n",
        "        df_path=os.path.join(args.data_dir, \"val_df.tsv\"),\n",
        "        desc_pickle=os.path.join(args.data_dir, \"D_val.pkl\"),\n",
        "        obj_pickle=os.path.join(args.data_dir, \"O_val.pkl\"),\n",
        "        image_dir=os.path.join(args.data_dir, \"images\"),\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True,\n",
        "                              collate_fn=lambda batch: collate_fn(batch, tokenizer))\n",
        "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False,\n",
        "                            collate_fn=lambda batch: collate_fn(batch, tokenizer))\n",
        "\n",
        "    model = MultimodalSarcasmExplanationModel()\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=args.learning_rate, weight_decay=1e-4)\n",
        "\n",
        "    best_val_loss = float(\"inf\")\n",
        "    for epoch in range(args.epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{args.epochs}\")\n",
        "        train_loss = train(model, train_loader, optimizer, tokenizer, device)\n",
        "        print(f\"Training Loss: {train_loss:.4f}\")\n",
        "\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                input_ids = batch[\"input_ids\"].to(device)\n",
        "                attention_mask = batch[\"attention_mask\"].to(device)\n",
        "                decoder_input_ids = batch[\"decoder_input_ids\"].to(device)\n",
        "                labels = batch[\"labels\"].to(device)\n",
        "                pixel_values = batch[\"pixel_values\"].to(device)\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask,\n",
        "                                decoder_input_ids=decoder_input_ids, pixel_values=pixel_values, labels=labels)\n",
        "                total_val_loss += outputs.loss.item()\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "        print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        # Generate sample explanations for monitoring\n",
        "        gen_texts, gt_texts = generate_explanations(model, val_loader, tokenizer, device)\n",
        "        print(\"\\nSample Generated Explanations (Validation):\")\n",
        "        for i in range(min(3, len(gen_texts))):\n",
        "            print(f\"GT: {gt_texts[i]}\")\n",
        "            print(f\"Pred: {gen_texts[i]}\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "        metrics = compute_metrics(gen_texts, gt_texts)\n",
        "        print(\"\\nEvaluation Metrics on Validation Set:\")\n",
        "        for k, v in metrics.items():\n",
        "            print(f\"{k}: {v:.4f}\")\n",
        "\n",
        "        # checkpoint_path = os.path.join(args.checkpoint_dir, f\"model_epoch_{epoch+1}.pt\")\n",
        "        # os.makedirs(args.checkpoint_dir, exist_ok=True)\n",
        "        # torch.save(model.state_dict(), checkpoint_path)\n",
        "        # print(f\"Model checkpoint saved to {checkpoint_path}\")\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            # best_model_path = os.path.join(args.checkpoint_dir, \"best_model.pt\")\n",
        "            # torch.save(model.state_dict(), best_model_path)\n",
        "            # print(\"Best model updated.\")\n",
        "\n",
        "    if args.mode == \"test\":\n",
        "        test_df_path = os.path.join(args.data_dir, \"val_df.tsv\")\n",
        "        inference(model, test_df_path,\n",
        "                  desc_pickle=os.path.join(args.data_dir, \"D_val.pkl\"),\n",
        "                  obj_pickle=os.path.join(args.data_dir, \"O_val.pkl\"),\n",
        "                  image_dir=os.path.join(args.data_dir, \"images\"),\n",
        "                  tokenizer=tokenizer,\n",
        "                  device=device,\n",
        "                  output_file=args.output_file)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description=\"Multimodal Sarcasm Explanation (MuSE) Task\")\n",
        "    parser.add_argument(\"--data_dir\", type=str, default=\"MORE-PLUS-DATASET\", help=\"Path to the dataset folder\")\n",
        "    parser.add_argument(\"--checkpoint_dir\", type=str, default=\"checkpoints\", help=\"Directory to save model checkpoints\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=8, help=\"Batch size\")\n",
        "    parser.add_argument(\"--epochs\", type=int, default=5, help=\"Number of training epochs\")\n",
        "    parser.add_argument(\"--learning_rate\", type=float, default=5e-5, help=\"Learning rate\")\n",
        "    parser.add_argument(\"--mode\", type=str, choices=[\"train\", \"test\"], default=\"train\", help=\"Mode: train or test\")\n",
        "    parser.add_argument(\"--output_file\", type=str, default=\"sarcasm_explanations.txt\", help=\"Output file for test predictions\")\n",
        "\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    main(args)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
